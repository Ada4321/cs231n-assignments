{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161a3aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This mounts your Google Drive to the Colab VM.\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# TODO: Enter the foldername in your Drive where you have saved the unzipped\n",
    "# assignment folder, e.g. 'cs231n/assignments/assignment1/'\n",
    "FOLDERNAME = None\n",
    "assert FOLDERNAME is not None, \"[!] Enter the foldername.\"\n",
    "\n",
    "# Now that we've mounted your Drive, this ensures that\n",
    "# the Python interpreter of the Colab VM can load\n",
    "# python files from within it.\n",
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(FOLDERNAME))\n",
    "\n",
    "# This downloads the CIFAR-10 dataset to your Drive\n",
    "# if it doesn't already exist.\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME/cs231n/datasets/\n",
    "!bash get_datasets.sh\n",
    "%cd /content/drive/My\\ Drive/$FOLDERNAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fdca1c",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "# Softmax exercise\n",
    "\n",
    "*Complete and hand in this completed worksheet (including its outputs and any supporting code outside of the worksheet) with your assignment submission. For more details see the [assignments page](http://vision.stanford.edu/teaching/cs231n/assignments.html) on the course website.*\n",
    "\n",
    "This exercise is analogous to the SVM exercise. You will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f12280f",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from cs231n.data_utils import load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab106c82",
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data shape:  (49000, 3073)\n",
      "Train labels shape:  (49000,)\n",
      "Validation data shape:  (1000, 3073)\n",
      "Validation labels shape:  (1000,)\n",
      "Test data shape:  (1000, 3073)\n",
      "Test labels shape:  (1000,)\n",
      "dev data shape:  (500, 3073)\n",
      "dev labels shape:  (500,)\n"
     ]
    }
   ],
   "source": [
    "def get_CIFAR10_data(num_training=49000, num_validation=1000, num_test=1000, num_dev=500):\n",
    "    \"\"\"\n",
    "    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare\n",
    "    it for the linear classifier. These are the same steps as we used for the\n",
    "    SVM, but condensed to a single function.  \n",
    "    \"\"\"\n",
    "    # Load the raw CIFAR-10 data\n",
    "    cifar10_dir = 'cs231n/datasets/cifar-10-batches-py'\n",
    "    \n",
    "    # Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "    try:\n",
    "       del X_train, y_train\n",
    "       del X_test, y_test\n",
    "       print('Clear previously loaded data.')\n",
    "    except:\n",
    "       pass\n",
    "\n",
    "    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)\n",
    "    \n",
    "    # subsample the data\n",
    "    mask = list(range(num_training, num_training + num_validation))\n",
    "    X_val = X_train[mask]\n",
    "    y_val = y_train[mask]\n",
    "    mask = list(range(num_training))\n",
    "    X_train = X_train[mask]\n",
    "    y_train = y_train[mask]\n",
    "    mask = list(range(num_test))\n",
    "    X_test = X_test[mask]\n",
    "    y_test = y_test[mask]\n",
    "    mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "    X_dev = X_train[mask]\n",
    "    y_dev = y_train[mask]\n",
    "    \n",
    "    # Preprocessing: reshape the image data into rows\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "    X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "    X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "    \n",
    "    # Normalize the data: subtract the mean image\n",
    "    mean_image = np.mean(X_train, axis = 0)\n",
    "    X_train -= mean_image\n",
    "    X_val -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_dev -= mean_image\n",
    "    \n",
    "    # add bias dimension and transform into columns\n",
    "    X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "    X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "    X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "    X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev\n",
    "\n",
    "\n",
    "# Invoke the above function to get our data.\n",
    "X_train, y_train, X_val, y_val, X_test, y_test, X_dev, y_dev = get_CIFAR10_data()\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)\n",
    "print('dev data shape: ', X_dev.shape)\n",
    "print('dev labels shape: ', y_dev.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4721c3cb",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `cs231n/classifiers/softmax.py`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d42765cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1181.554504\n",
      "sanity check: 2.302585\n"
     ]
    }
   ],
   "source": [
    "# First implement the naive softmax loss function with nested loops.\n",
    "# Open the file cs231n/classifiers/softmax.py and implement the\n",
    "# softmax_loss_naive function.\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "#loss, grad = softmax_loss_vectorized(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print('loss: %f' % loss)\n",
    "print('sanity check: %f' % (-np.log(0.1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88764e",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 1**\n",
    "\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.**\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ *Fill this in* \\\n",
    "$Loss = \\frac{1}{N}\\sum_{i}{-\\log{\\frac{e^{f_{y_i}}}{\\sum_{j}{e^{f_j}}}}}$\\\n",
    "当所有$weight$初始化为接近0的随机数，所有$f_j$也接近0，$Loss=\\frac{1}{N}\\times N\\times (-\\log{\\frac{1}{C}})=-\\log{\\frac{1}{C}}$\\\n",
    "本问题中$C=10$，故$Loss=-\\log{\\frac{1}{10}}=-\\log{(0.1)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c2626ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numerical: 0.452098 analytic: 0.452098, relative error: 2.767663e-09\n",
      "numerical: 0.641855 analytic: 0.641855, relative error: 5.968830e-08\n",
      "numerical: 1.006181 analytic: 1.006181, relative error: 2.538211e-08\n",
      "numerical: -2.471089 analytic: -2.471090, relative error: 3.804675e-08\n",
      "numerical: 0.025595 analytic: 0.025595, relative error: 1.508205e-06\n",
      "numerical: 0.396007 analytic: 0.396007, relative error: 5.941774e-08\n",
      "numerical: -3.832726 analytic: -3.832726, relative error: 1.979837e-08\n",
      "numerical: -0.117843 analytic: -0.117843, relative error: 2.554362e-07\n",
      "numerical: -1.080350 analytic: -1.080350, relative error: 9.401523e-10\n",
      "numerical: -3.162225 analytic: -3.162225, relative error: 6.057527e-09\n",
      "numerical: 0.361973 analytic: 0.361973, relative error: 9.014120e-09\n",
      "numerical: 0.849726 analytic: 0.849726, relative error: 9.541227e-09\n",
      "numerical: 2.877891 analytic: 2.877890, relative error: 2.554808e-08\n",
      "numerical: 1.190600 analytic: 1.190600, relative error: 1.860466e-08\n",
      "numerical: 2.221200 analytic: 2.221200, relative error: 2.006336e-08\n",
      "numerical: 1.473738 analytic: 1.473738, relative error: 1.671587e-08\n",
      "numerical: 1.489524 analytic: 1.489524, relative error: 2.460912e-08\n",
      "numerical: 0.658485 analytic: 0.658485, relative error: 3.936272e-08\n",
      "numerical: 1.730965 analytic: 1.730965, relative error: 1.233612e-08\n",
      "numerical: 2.138013 analytic: 2.138013, relative error: 2.096689e-08\n"
     ]
    }
   ],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As we did for the SVM, use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from cs231n.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)\n",
    "\n",
    "# similar to SVM case, do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 5e1)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 5e1)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "434e757f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naive loss: 2.363109e+00 computed in 0.076255s\n",
      "vectorized loss: 2.363109e+00 computed in 0.002733s\n",
      "Loss difference: 0.000000\n",
      "Gradient difference: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Now that we have a naive implementation of the softmax loss function and its gradient,\n",
    "# implement a vectorized version in softmax_loss_vectorized.\n",
    "# The two versions should compute the same results, but the vectorized version should be\n",
    "# much faster.\n",
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('naive loss: %e computed in %fs' % (loss_naive, toc - tic))\n",
    "\n",
    "from cs231n.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.000005)\n",
    "toc = time.time()\n",
    "print('vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic))\n",
    "\n",
    "# As we did for the SVM, we use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print('Loss difference: %f' % np.abs(loss_naive - loss_vectorized))\n",
    "print('Gradient difference: %f' % grad_difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e5e374d",
   "metadata": {
    "tags": [
     "code"
    ],
    "test": "tuning"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 773.932026\n",
      "iteration 100 / 1500: loss 283.783429\n",
      "iteration 200 / 1500: loss 105.039958\n",
      "iteration 300 / 1500: loss 39.888722\n",
      "iteration 400 / 1500: loss 15.824565\n",
      "iteration 500 / 1500: loss 7.180106\n",
      "iteration 600 / 1500: loss 3.954939\n",
      "iteration 700 / 1500: loss 2.774810\n",
      "iteration 800 / 1500: loss 2.260631\n",
      "iteration 900 / 1500: loss 2.121691\n",
      "iteration 1000 / 1500: loss 2.094425\n",
      "iteration 1100 / 1500: loss 2.015820\n",
      "iteration 1200 / 1500: loss 2.148465\n",
      "iteration 1300 / 1500: loss 2.024036\n",
      "iteration 1400 / 1500: loss 2.127012\n",
      "iteration 0 / 1500: loss 802.359100\n",
      "iteration 100 / 1500: loss 282.480565\n",
      "iteration 200 / 1500: loss 100.477679\n",
      "iteration 300 / 1500: loss 36.630263\n",
      "iteration 400 / 1500: loss 14.269631\n",
      "iteration 500 / 1500: loss 6.275264\n",
      "iteration 600 / 1500: loss 3.532757\n",
      "iteration 700 / 1500: loss 2.580896\n",
      "iteration 800 / 1500: loss 2.236764\n",
      "iteration 900 / 1500: loss 2.131744\n",
      "iteration 1000 / 1500: loss 2.035678\n",
      "iteration 1100 / 1500: loss 2.072605\n",
      "iteration 1200 / 1500: loss 2.058391\n",
      "iteration 1300 / 1500: loss 2.035526\n",
      "iteration 1400 / 1500: loss 2.067984\n",
      "iteration 0 / 1500: loss 848.881119\n",
      "iteration 100 / 1500: loss 287.279029\n",
      "iteration 200 / 1500: loss 98.292691\n",
      "iteration 300 / 1500: loss 34.535380\n",
      "iteration 400 / 1500: loss 13.094210\n",
      "iteration 500 / 1500: loss 5.806836\n",
      "iteration 600 / 1500: loss 3.311787\n",
      "iteration 700 / 1500: loss 2.477557\n",
      "iteration 800 / 1500: loss 2.159768\n",
      "iteration 900 / 1500: loss 2.108707\n",
      "iteration 1000 / 1500: loss 2.127902\n",
      "iteration 1100 / 1500: loss 2.076901\n",
      "iteration 1200 / 1500: loss 2.022534\n",
      "iteration 1300 / 1500: loss 2.053291\n",
      "iteration 1400 / 1500: loss 2.032948\n",
      "iteration 0 / 1500: loss 872.205121\n",
      "iteration 100 / 1500: loss 283.733927\n",
      "iteration 200 / 1500: loss 93.420015\n",
      "iteration 300 / 1500: loss 31.653840\n",
      "iteration 400 / 1500: loss 11.682979\n",
      "iteration 500 / 1500: loss 5.121860\n",
      "iteration 600 / 1500: loss 3.061913\n",
      "iteration 700 / 1500: loss 2.382193\n",
      "iteration 800 / 1500: loss 2.221779\n",
      "iteration 900 / 1500: loss 2.141876\n",
      "iteration 1000 / 1500: loss 2.093197\n",
      "iteration 1100 / 1500: loss 2.077154\n",
      "iteration 1200 / 1500: loss 2.031011\n",
      "iteration 1300 / 1500: loss 2.087135\n",
      "iteration 1400 / 1500: loss 2.062322\n",
      "iteration 0 / 1500: loss 896.515325\n",
      "iteration 100 / 1500: loss 280.783858\n",
      "iteration 200 / 1500: loss 88.881836\n",
      "iteration 300 / 1500: loss 29.141006\n",
      "iteration 400 / 1500: loss 10.490651\n",
      "iteration 500 / 1500: loss 4.688335\n",
      "iteration 600 / 1500: loss 2.894580\n",
      "iteration 700 / 1500: loss 2.351346\n",
      "iteration 800 / 1500: loss 2.108906\n",
      "iteration 900 / 1500: loss 2.061945\n",
      "iteration 1000 / 1500: loss 2.069491\n",
      "iteration 1100 / 1500: loss 2.057918\n",
      "iteration 1200 / 1500: loss 2.124060\n",
      "iteration 1300 / 1500: loss 2.088902\n",
      "iteration 1400 / 1500: loss 2.045501\n",
      "iteration 0 / 1500: loss 942.841624\n",
      "iteration 100 / 1500: loss 283.390442\n",
      "iteration 200 / 1500: loss 86.158728\n",
      "iteration 300 / 1500: loss 27.277923\n",
      "iteration 400 / 1500: loss 9.604040\n",
      "iteration 500 / 1500: loss 4.357316\n",
      "iteration 600 / 1500: loss 2.722764\n",
      "iteration 700 / 1500: loss 2.293207\n",
      "iteration 800 / 1500: loss 2.116619\n",
      "iteration 900 / 1500: loss 2.088441\n",
      "iteration 1000 / 1500: loss 2.075356\n",
      "iteration 1100 / 1500: loss 2.097484\n",
      "iteration 1200 / 1500: loss 2.070523\n",
      "iteration 1300 / 1500: loss 2.044738\n",
      "iteration 1400 / 1500: loss 2.111464\n",
      "iteration 0 / 1500: loss 952.408468\n",
      "iteration 100 / 1500: loss 275.091024\n",
      "iteration 200 / 1500: loss 80.493244\n",
      "iteration 300 / 1500: loss 24.583102\n",
      "iteration 400 / 1500: loss 8.524850\n",
      "iteration 500 / 1500: loss 3.954441\n",
      "iteration 600 / 1500: loss 2.600834\n",
      "iteration 700 / 1500: loss 2.201799\n",
      "iteration 800 / 1500: loss 2.100926\n",
      "iteration 900 / 1500: loss 2.035661\n",
      "iteration 1000 / 1500: loss 2.090395\n",
      "iteration 1100 / 1500: loss 2.070868\n",
      "iteration 1200 / 1500: loss 2.068025\n",
      "iteration 1300 / 1500: loss 2.132769\n",
      "iteration 1400 / 1500: loss 2.141117\n",
      "iteration 0 / 1500: loss 994.246505\n",
      "iteration 100 / 1500: loss 275.846180\n",
      "iteration 200 / 1500: loss 77.658900\n",
      "iteration 300 / 1500: loss 22.960963\n",
      "iteration 400 / 1500: loss 7.797536\n",
      "iteration 500 / 1500: loss 3.728875\n",
      "iteration 600 / 1500: loss 2.517161\n",
      "iteration 700 / 1500: loss 2.163626\n",
      "iteration 800 / 1500: loss 2.099774\n",
      "iteration 900 / 1500: loss 2.075741\n",
      "iteration 1000 / 1500: loss 2.045890\n",
      "iteration 1100 / 1500: loss 2.107798\n",
      "iteration 1200 / 1500: loss 2.036201\n",
      "iteration 1300 / 1500: loss 2.043628\n",
      "iteration 1400 / 1500: loss 2.117368\n",
      "iteration 0 / 1500: loss 1030.571727\n",
      "iteration 100 / 1500: loss 274.578325\n",
      "iteration 200 / 1500: loss 74.290610\n",
      "iteration 300 / 1500: loss 21.217108\n",
      "iteration 400 / 1500: loss 7.138883\n",
      "iteration 500 / 1500: loss 3.465096\n",
      "iteration 600 / 1500: loss 2.387366\n",
      "iteration 700 / 1500: loss 2.104497\n",
      "iteration 800 / 1500: loss 2.069991\n",
      "iteration 900 / 1500: loss 2.079610\n",
      "iteration 1000 / 1500: loss 2.059553\n",
      "iteration 1100 / 1500: loss 2.101051\n",
      "iteration 1200 / 1500: loss 2.067792\n",
      "iteration 1300 / 1500: loss 2.059680\n",
      "iteration 1400 / 1500: loss 2.071526\n",
      "iteration 0 / 1500: loss 1058.589057\n",
      "iteration 100 / 1500: loss 270.623447\n",
      "iteration 200 / 1500: loss 70.470382\n",
      "iteration 300 / 1500: loss 19.553579\n",
      "iteration 400 / 1500: loss 6.553733\n",
      "iteration 500 / 1500: loss 3.203609\n",
      "iteration 600 / 1500: loss 2.324939\n",
      "iteration 700 / 1500: loss 2.101883\n",
      "iteration 800 / 1500: loss 2.087661\n",
      "iteration 900 / 1500: loss 2.034020\n",
      "iteration 1000 / 1500: loss 2.078556\n",
      "iteration 1100 / 1500: loss 2.163649\n",
      "iteration 1200 / 1500: loss 2.122776\n",
      "iteration 1300 / 1500: loss 2.039351\n",
      "iteration 1400 / 1500: loss 2.079891\n",
      "iteration 0 / 1500: loss 1070.582455\n",
      "iteration 100 / 1500: loss 263.234220\n",
      "iteration 200 / 1500: loss 65.976965\n",
      "iteration 300 / 1500: loss 17.717694\n",
      "iteration 400 / 1500: loss 5.879643\n",
      "iteration 500 / 1500: loss 3.036749\n",
      "iteration 600 / 1500: loss 2.350536\n",
      "iteration 700 / 1500: loss 2.168114\n",
      "iteration 800 / 1500: loss 2.126520\n",
      "iteration 900 / 1500: loss 2.109412\n",
      "iteration 1000 / 1500: loss 2.148407\n",
      "iteration 1100 / 1500: loss 2.097057\n",
      "iteration 1200 / 1500: loss 2.102948\n",
      "iteration 1300 / 1500: loss 2.130174\n",
      "iteration 1400 / 1500: loss 2.100171\n",
      "iteration 0 / 1500: loss 1103.830641\n",
      "iteration 100 / 1500: loss 260.627644\n",
      "iteration 200 / 1500: loss 62.864268\n",
      "iteration 300 / 1500: loss 16.339542\n",
      "iteration 400 / 1500: loss 5.474892\n",
      "iteration 500 / 1500: loss 2.916249\n",
      "iteration 600 / 1500: loss 2.246492\n",
      "iteration 700 / 1500: loss 2.160826\n",
      "iteration 800 / 1500: loss 2.089140\n",
      "iteration 900 / 1500: loss 2.123618\n",
      "iteration 1000 / 1500: loss 2.140987\n",
      "iteration 1100 / 1500: loss 2.062667\n",
      "iteration 1200 / 1500: loss 2.158206\n",
      "iteration 1300 / 1500: loss 2.116999\n",
      "iteration 1400 / 1500: loss 2.110738\n",
      "iteration 0 / 1500: loss 1154.672154\n",
      "iteration 100 / 1500: loss 261.773932\n",
      "iteration 200 / 1500: loss 60.644269\n",
      "iteration 300 / 1500: loss 15.362754\n",
      "iteration 400 / 1500: loss 5.138046\n",
      "iteration 500 / 1500: loss 2.819219\n",
      "iteration 600 / 1500: loss 2.262967\n",
      "iteration 700 / 1500: loss 2.164636\n",
      "iteration 800 / 1500: loss 2.155499\n",
      "iteration 900 / 1500: loss 2.114193\n",
      "iteration 1000 / 1500: loss 2.134795\n",
      "iteration 1100 / 1500: loss 2.100443\n",
      "iteration 1200 / 1500: loss 2.118263\n",
      "iteration 1300 / 1500: loss 2.111877\n",
      "iteration 1400 / 1500: loss 2.075353\n",
      "iteration 0 / 1500: loss 1182.837289\n",
      "iteration 100 / 1500: loss 257.660789\n",
      "iteration 200 / 1500: loss 57.534885\n",
      "iteration 300 / 1500: loss 14.112947\n",
      "iteration 400 / 1500: loss 4.695527\n",
      "iteration 500 / 1500: loss 2.668573\n",
      "iteration 600 / 1500: loss 2.204434\n",
      "iteration 700 / 1500: loss 2.154065\n",
      "iteration 800 / 1500: loss 2.076450\n",
      "iteration 900 / 1500: loss 2.144009\n",
      "iteration 1000 / 1500: loss 2.079013\n",
      "iteration 1100 / 1500: loss 2.152482\n",
      "iteration 1200 / 1500: loss 2.050849\n",
      "iteration 1300 / 1500: loss 2.071124\n",
      "iteration 1400 / 1500: loss 2.121090\n",
      "iteration 0 / 1500: loss 1216.945804\n",
      "iteration 100 / 1500: loss 254.967102\n",
      "iteration 200 / 1500: loss 54.799976\n",
      "iteration 300 / 1500: loss 13.116948\n",
      "iteration 400 / 1500: loss 4.356420\n",
      "iteration 500 / 1500: loss 2.559457\n",
      "iteration 600 / 1500: loss 2.154387\n",
      "iteration 700 / 1500: loss 2.116378\n",
      "iteration 800 / 1500: loss 2.049692\n",
      "iteration 900 / 1500: loss 2.114160\n",
      "iteration 1000 / 1500: loss 2.060529\n",
      "iteration 1100 / 1500: loss 2.123580\n",
      "iteration 1200 / 1500: loss 2.147484\n",
      "iteration 1300 / 1500: loss 2.108755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.055202\n",
      "iteration 0 / 1500: loss 1240.899381\n",
      "iteration 100 / 1500: loss 249.497734\n",
      "iteration 200 / 1500: loss 51.675873\n",
      "iteration 300 / 1500: loss 12.085394\n",
      "iteration 400 / 1500: loss 4.074098\n",
      "iteration 500 / 1500: loss 2.489598\n",
      "iteration 600 / 1500: loss 2.146255\n",
      "iteration 700 / 1500: loss 2.144112\n",
      "iteration 800 / 1500: loss 2.085700\n",
      "iteration 900 / 1500: loss 2.096398\n",
      "iteration 1000 / 1500: loss 2.120957\n",
      "iteration 1100 / 1500: loss 2.085791\n",
      "iteration 1200 / 1500: loss 2.051981\n",
      "iteration 1300 / 1500: loss 2.099291\n",
      "iteration 1400 / 1500: loss 2.110010\n",
      "iteration 0 / 1500: loss 1240.111121\n",
      "iteration 100 / 1500: loss 239.880707\n",
      "iteration 200 / 1500: loss 47.686572\n",
      "iteration 300 / 1500: loss 10.832025\n",
      "iteration 400 / 1500: loss 3.827140\n",
      "iteration 500 / 1500: loss 2.442492\n",
      "iteration 600 / 1500: loss 2.202715\n",
      "iteration 700 / 1500: loss 2.105927\n",
      "iteration 800 / 1500: loss 2.071205\n",
      "iteration 900 / 1500: loss 2.102193\n",
      "iteration 1000 / 1500: loss 2.074636\n",
      "iteration 1100 / 1500: loss 2.052896\n",
      "iteration 1200 / 1500: loss 2.080428\n",
      "iteration 1300 / 1500: loss 2.139247\n",
      "iteration 1400 / 1500: loss 2.099409\n",
      "iteration 0 / 1500: loss 1289.901380\n",
      "iteration 100 / 1500: loss 239.548531\n",
      "iteration 200 / 1500: loss 45.975350\n",
      "iteration 300 / 1500: loss 10.148189\n",
      "iteration 400 / 1500: loss 3.593602\n",
      "iteration 500 / 1500: loss 2.373810\n",
      "iteration 600 / 1500: loss 2.162752\n",
      "iteration 700 / 1500: loss 2.110530\n",
      "iteration 800 / 1500: loss 2.161312\n",
      "iteration 900 / 1500: loss 2.109890\n",
      "iteration 1000 / 1500: loss 2.115628\n",
      "iteration 1100 / 1500: loss 2.095107\n",
      "iteration 1200 / 1500: loss 2.062123\n",
      "iteration 1300 / 1500: loss 2.149769\n",
      "iteration 1400 / 1500: loss 2.067701\n",
      "iteration 0 / 1500: loss 1343.325988\n",
      "iteration 100 / 1500: loss 239.721559\n",
      "iteration 200 / 1500: loss 44.157025\n",
      "iteration 300 / 1500: loss 9.564055\n",
      "iteration 400 / 1500: loss 3.399091\n",
      "iteration 500 / 1500: loss 2.326286\n",
      "iteration 600 / 1500: loss 2.146800\n",
      "iteration 700 / 1500: loss 2.160707\n",
      "iteration 800 / 1500: loss 2.140633\n",
      "iteration 900 / 1500: loss 2.105555\n",
      "iteration 1000 / 1500: loss 2.025555\n",
      "iteration 1100 / 1500: loss 2.096932\n",
      "iteration 1200 / 1500: loss 2.109192\n",
      "iteration 1300 / 1500: loss 2.102153\n",
      "iteration 1400 / 1500: loss 2.115018\n",
      "iteration 0 / 1500: loss 1379.879353\n",
      "iteration 100 / 1500: loss 236.382288\n",
      "iteration 200 / 1500: loss 41.982154\n",
      "iteration 300 / 1500: loss 8.893760\n",
      "iteration 400 / 1500: loss 3.279073\n",
      "iteration 500 / 1500: loss 2.294109\n",
      "iteration 600 / 1500: loss 2.154081\n",
      "iteration 700 / 1500: loss 2.072138\n",
      "iteration 800 / 1500: loss 2.123905\n",
      "iteration 900 / 1500: loss 2.090237\n",
      "iteration 1000 / 1500: loss 2.103118\n",
      "iteration 1100 / 1500: loss 2.034850\n",
      "iteration 1200 / 1500: loss 2.060369\n",
      "iteration 1300 / 1500: loss 2.075173\n",
      "iteration 1400 / 1500: loss 2.085487\n",
      "iteration 0 / 1500: loss 1385.238303\n",
      "iteration 100 / 1500: loss 228.016007\n",
      "iteration 200 / 1500: loss 39.052255\n",
      "iteration 300 / 1500: loss 8.139206\n",
      "iteration 400 / 1500: loss 3.093775\n",
      "iteration 500 / 1500: loss 2.300422\n",
      "iteration 600 / 1500: loss 2.089197\n",
      "iteration 700 / 1500: loss 2.059856\n",
      "iteration 800 / 1500: loss 2.147843\n",
      "iteration 900 / 1500: loss 2.107581\n",
      "iteration 1000 / 1500: loss 2.090174\n",
      "iteration 1100 / 1500: loss 2.098144\n",
      "iteration 1200 / 1500: loss 2.120532\n",
      "iteration 1300 / 1500: loss 2.125817\n",
      "iteration 1400 / 1500: loss 2.066586\n",
      "iteration 0 / 1500: loss 1415.762341\n",
      "iteration 100 / 1500: loss 223.965322\n",
      "iteration 200 / 1500: loss 36.938131\n",
      "iteration 300 / 1500: loss 7.569667\n",
      "iteration 400 / 1500: loss 2.999543\n",
      "iteration 500 / 1500: loss 2.235867\n",
      "iteration 600 / 1500: loss 2.211342\n",
      "iteration 700 / 1500: loss 2.191201\n",
      "iteration 800 / 1500: loss 2.111294\n",
      "iteration 900 / 1500: loss 2.110552\n",
      "iteration 1000 / 1500: loss 2.062371\n",
      "iteration 1100 / 1500: loss 2.118096\n",
      "iteration 1200 / 1500: loss 2.102514\n",
      "iteration 1300 / 1500: loss 2.111190\n",
      "iteration 1400 / 1500: loss 2.095761\n",
      "iteration 0 / 1500: loss 1470.168389\n",
      "iteration 100 / 1500: loss 223.201436\n",
      "iteration 200 / 1500: loss 35.444843\n",
      "iteration 300 / 1500: loss 7.106838\n",
      "iteration 400 / 1500: loss 2.855935\n",
      "iteration 500 / 1500: loss 2.234246\n",
      "iteration 600 / 1500: loss 2.167375\n",
      "iteration 700 / 1500: loss 2.091924\n",
      "iteration 800 / 1500: loss 2.064005\n",
      "iteration 900 / 1500: loss 2.115012\n",
      "iteration 1000 / 1500: loss 2.140360\n",
      "iteration 1100 / 1500: loss 2.132648\n",
      "iteration 1200 / 1500: loss 2.106035\n",
      "iteration 1300 / 1500: loss 2.146243\n",
      "iteration 1400 / 1500: loss 2.147352\n",
      "iteration 0 / 1500: loss 1476.528406\n",
      "iteration 100 / 1500: loss 215.475154\n",
      "iteration 200 / 1500: loss 32.992734\n",
      "iteration 300 / 1500: loss 6.552922\n",
      "iteration 400 / 1500: loss 2.759225\n",
      "iteration 500 / 1500: loss 2.276027\n",
      "iteration 600 / 1500: loss 2.111766\n",
      "iteration 700 / 1500: loss 2.133941\n",
      "iteration 800 / 1500: loss 2.127058\n",
      "iteration 900 / 1500: loss 2.105065\n",
      "iteration 1000 / 1500: loss 2.083026\n",
      "iteration 1100 / 1500: loss 2.104354\n",
      "iteration 1200 / 1500: loss 2.131041\n",
      "iteration 1300 / 1500: loss 2.113902\n",
      "iteration 1400 / 1500: loss 2.154064\n",
      "iteration 0 / 1500: loss 1496.758075\n",
      "iteration 100 / 1500: loss 209.830475\n",
      "iteration 200 / 1500: loss 31.018813\n",
      "iteration 300 / 1500: loss 6.137395\n",
      "iteration 400 / 1500: loss 2.685586\n",
      "iteration 500 / 1500: loss 2.193003\n",
      "iteration 600 / 1500: loss 2.097324\n",
      "iteration 700 / 1500: loss 2.105670\n",
      "iteration 800 / 1500: loss 2.120154\n",
      "iteration 900 / 1500: loss 2.073490\n",
      "iteration 1000 / 1500: loss 2.101571\n",
      "iteration 1100 / 1500: loss 2.106288\n",
      "iteration 1200 / 1500: loss 2.064892\n",
      "iteration 1300 / 1500: loss 2.111373\n",
      "iteration 1400 / 1500: loss 2.112873\n",
      "iteration 0 / 1500: loss 1542.184451\n",
      "iteration 100 / 1500: loss 207.349971\n",
      "iteration 200 / 1500: loss 29.516966\n",
      "iteration 300 / 1500: loss 5.794622\n",
      "iteration 400 / 1500: loss 2.581951\n",
      "iteration 500 / 1500: loss 2.173383\n",
      "iteration 600 / 1500: loss 2.143481\n",
      "iteration 700 / 1500: loss 2.110551\n",
      "iteration 800 / 1500: loss 2.106528\n",
      "iteration 900 / 1500: loss 2.087987\n",
      "iteration 1000 / 1500: loss 2.172687\n",
      "iteration 1100 / 1500: loss 2.147710\n",
      "iteration 1200 / 1500: loss 2.111818\n",
      "iteration 1300 / 1500: loss 2.065829\n",
      "iteration 1400 / 1500: loss 2.169385\n",
      "iteration 0 / 1500: loss 776.434485\n",
      "iteration 100 / 1500: loss 220.269071\n",
      "iteration 200 / 1500: loss 63.460426\n",
      "iteration 300 / 1500: loss 19.356370\n",
      "iteration 400 / 1500: loss 6.961126\n",
      "iteration 500 / 1500: loss 3.438795\n",
      "iteration 600 / 1500: loss 2.420264\n",
      "iteration 700 / 1500: loss 2.161595\n",
      "iteration 800 / 1500: loss 2.057418\n",
      "iteration 900 / 1500: loss 2.056692\n",
      "iteration 1000 / 1500: loss 1.989517\n",
      "iteration 1100 / 1500: loss 2.018346\n",
      "iteration 1200 / 1500: loss 2.047797\n",
      "iteration 1300 / 1500: loss 2.016744\n",
      "iteration 1400 / 1500: loss 2.114957\n",
      "iteration 0 / 1500: loss 806.281654\n",
      "iteration 100 / 1500: loss 217.188729\n",
      "iteration 200 / 1500: loss 59.644828\n",
      "iteration 300 / 1500: loss 17.499438\n",
      "iteration 400 / 1500: loss 6.197222\n",
      "iteration 500 / 1500: loss 3.177920\n",
      "iteration 600 / 1500: loss 2.406792\n",
      "iteration 700 / 1500: loss 2.104180\n",
      "iteration 800 / 1500: loss 2.082344\n",
      "iteration 900 / 1500: loss 2.036622\n",
      "iteration 1000 / 1500: loss 2.073693\n",
      "iteration 1100 / 1500: loss 2.020163\n",
      "iteration 1200 / 1500: loss 2.055668\n",
      "iteration 1300 / 1500: loss 2.046773\n",
      "iteration 1400 / 1500: loss 2.056180\n",
      "iteration 0 / 1500: loss 834.535451\n",
      "iteration 100 / 1500: loss 214.179488\n",
      "iteration 200 / 1500: loss 56.100865\n",
      "iteration 300 / 1500: loss 15.834133\n",
      "iteration 400 / 1500: loss 5.563380\n",
      "iteration 500 / 1500: loss 2.965585\n",
      "iteration 600 / 1500: loss 2.254331\n",
      "iteration 700 / 1500: loss 2.142087\n",
      "iteration 800 / 1500: loss 2.094247\n",
      "iteration 900 / 1500: loss 2.040934\n",
      "iteration 1000 / 1500: loss 2.072676\n",
      "iteration 1100 / 1500: loss 2.044611\n",
      "iteration 1200 / 1500: loss 2.062616\n",
      "iteration 1300 / 1500: loss 2.091687\n",
      "iteration 1400 / 1500: loss 2.047910\n",
      "iteration 0 / 1500: loss 861.436729\n",
      "iteration 100 / 1500: loss 209.900798\n",
      "iteration 200 / 1500: loss 52.240180\n",
      "iteration 300 / 1500: loss 14.272051\n",
      "iteration 400 / 1500: loss 5.002686\n",
      "iteration 500 / 1500: loss 2.802681\n",
      "iteration 600 / 1500: loss 2.292901\n",
      "iteration 700 / 1500: loss 2.056139\n",
      "iteration 800 / 1500: loss 2.095720\n",
      "iteration 900 / 1500: loss 2.085664\n",
      "iteration 1000 / 1500: loss 2.048626\n",
      "iteration 1100 / 1500: loss 2.045612\n",
      "iteration 1200 / 1500: loss 2.063450\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300 / 1500: loss 2.014096\n",
      "iteration 1400 / 1500: loss 2.089498\n",
      "iteration 0 / 1500: loss 893.737778\n",
      "iteration 100 / 1500: loss 206.852836\n",
      "iteration 200 / 1500: loss 49.172971\n",
      "iteration 300 / 1500: loss 12.922490\n",
      "iteration 400 / 1500: loss 4.566226\n",
      "iteration 500 / 1500: loss 2.686772\n",
      "iteration 600 / 1500: loss 2.234303\n",
      "iteration 700 / 1500: loss 2.076304\n",
      "iteration 800 / 1500: loss 2.035895\n",
      "iteration 900 / 1500: loss 2.027217\n",
      "iteration 1000 / 1500: loss 2.117586\n",
      "iteration 1100 / 1500: loss 2.035314\n",
      "iteration 1200 / 1500: loss 2.087116\n",
      "iteration 1300 / 1500: loss 2.031253\n",
      "iteration 1400 / 1500: loss 2.023349\n",
      "iteration 0 / 1500: loss 932.440131\n",
      "iteration 100 / 1500: loss 204.876107\n",
      "iteration 200 / 1500: loss 46.441902\n",
      "iteration 300 / 1500: loss 11.773448\n",
      "iteration 400 / 1500: loss 4.213814\n",
      "iteration 500 / 1500: loss 2.600481\n",
      "iteration 600 / 1500: loss 2.129561\n",
      "iteration 700 / 1500: loss 2.117670\n",
      "iteration 800 / 1500: loss 2.111425\n",
      "iteration 900 / 1500: loss 2.074266\n",
      "iteration 1000 / 1500: loss 2.037128\n",
      "iteration 1100 / 1500: loss 2.060804\n",
      "iteration 1200 / 1500: loss 2.105939\n",
      "iteration 1300 / 1500: loss 2.087209\n",
      "iteration 1400 / 1500: loss 2.099710\n",
      "iteration 0 / 1500: loss 962.714993\n",
      "iteration 100 / 1500: loss 201.304741\n",
      "iteration 200 / 1500: loss 43.518758\n",
      "iteration 300 / 1500: loss 10.653345\n",
      "iteration 400 / 1500: loss 3.827127\n",
      "iteration 500 / 1500: loss 2.448292\n",
      "iteration 600 / 1500: loss 2.142848\n",
      "iteration 700 / 1500: loss 2.051527\n",
      "iteration 800 / 1500: loss 2.025088\n",
      "iteration 900 / 1500: loss 2.114657\n",
      "iteration 1000 / 1500: loss 2.036080\n",
      "iteration 1100 / 1500: loss 2.090973\n",
      "iteration 1200 / 1500: loss 2.061328\n",
      "iteration 1300 / 1500: loss 2.035229\n",
      "iteration 1400 / 1500: loss 2.060720\n",
      "iteration 0 / 1500: loss 992.233887\n",
      "iteration 100 / 1500: loss 197.433345\n",
      "iteration 200 / 1500: loss 40.771757\n",
      "iteration 300 / 1500: loss 9.717502\n",
      "iteration 400 / 1500: loss 3.597410\n",
      "iteration 500 / 1500: loss 2.384520\n",
      "iteration 600 / 1500: loss 2.214157\n",
      "iteration 700 / 1500: loss 2.120966\n",
      "iteration 800 / 1500: loss 2.065763\n",
      "iteration 900 / 1500: loss 2.081542\n",
      "iteration 1000 / 1500: loss 2.111814\n",
      "iteration 1100 / 1500: loss 2.094959\n",
      "iteration 1200 / 1500: loss 2.086169\n",
      "iteration 1300 / 1500: loss 2.067037\n",
      "iteration 1400 / 1500: loss 2.091365\n",
      "iteration 0 / 1500: loss 1014.141274\n",
      "iteration 100 / 1500: loss 191.895833\n",
      "iteration 200 / 1500: loss 37.673038\n",
      "iteration 300 / 1500: loss 8.777590\n",
      "iteration 400 / 1500: loss 3.316677\n",
      "iteration 500 / 1500: loss 2.318129\n",
      "iteration 600 / 1500: loss 2.147429\n",
      "iteration 700 / 1500: loss 2.117800\n",
      "iteration 800 / 1500: loss 2.102354\n",
      "iteration 900 / 1500: loss 2.121261\n",
      "iteration 1000 / 1500: loss 2.056282\n",
      "iteration 1100 / 1500: loss 2.085613\n",
      "iteration 1200 / 1500: loss 2.087705\n",
      "iteration 1300 / 1500: loss 2.097326\n",
      "iteration 1400 / 1500: loss 2.082050\n",
      "iteration 0 / 1500: loss 1057.513535\n",
      "iteration 100 / 1500: loss 190.421607\n",
      "iteration 200 / 1500: loss 35.669508\n",
      "iteration 300 / 1500: loss 8.026288\n",
      "iteration 400 / 1500: loss 3.189678\n",
      "iteration 500 / 1500: loss 2.285796\n",
      "iteration 600 / 1500: loss 2.074937\n",
      "iteration 700 / 1500: loss 2.103724\n",
      "iteration 800 / 1500: loss 2.112164\n",
      "iteration 900 / 1500: loss 2.086494\n",
      "iteration 1000 / 1500: loss 2.111767\n",
      "iteration 1100 / 1500: loss 2.094524\n",
      "iteration 1200 / 1500: loss 2.100622\n",
      "iteration 1300 / 1500: loss 2.081702\n",
      "iteration 1400 / 1500: loss 2.106320\n",
      "iteration 0 / 1500: loss 1089.938362\n",
      "iteration 100 / 1500: loss 186.147583\n",
      "iteration 200 / 1500: loss 33.291334\n",
      "iteration 300 / 1500: loss 7.392629\n",
      "iteration 400 / 1500: loss 2.988859\n",
      "iteration 500 / 1500: loss 2.243178\n",
      "iteration 600 / 1500: loss 2.089135\n",
      "iteration 700 / 1500: loss 2.114376\n",
      "iteration 800 / 1500: loss 2.118820\n",
      "iteration 900 / 1500: loss 2.098496\n",
      "iteration 1000 / 1500: loss 2.140733\n",
      "iteration 1100 / 1500: loss 2.079138\n",
      "iteration 1200 / 1500: loss 2.107938\n",
      "iteration 1300 / 1500: loss 2.060219\n",
      "iteration 1400 / 1500: loss 2.076199\n",
      "iteration 0 / 1500: loss 1116.648547\n",
      "iteration 100 / 1500: loss 181.604154\n",
      "iteration 200 / 1500: loss 30.972909\n",
      "iteration 300 / 1500: loss 6.784548\n",
      "iteration 400 / 1500: loss 2.849527\n",
      "iteration 500 / 1500: loss 2.222479\n",
      "iteration 600 / 1500: loss 2.113409\n",
      "iteration 700 / 1500: loss 2.127241\n",
      "iteration 800 / 1500: loss 2.004171\n",
      "iteration 900 / 1500: loss 2.076524\n",
      "iteration 1000 / 1500: loss 2.114723\n",
      "iteration 1100 / 1500: loss 2.044076\n",
      "iteration 1200 / 1500: loss 2.076250\n",
      "iteration 1300 / 1500: loss 2.077740\n",
      "iteration 1400 / 1500: loss 2.096179\n",
      "iteration 0 / 1500: loss 1146.092591\n",
      "iteration 100 / 1500: loss 177.061868\n",
      "iteration 200 / 1500: loss 28.979098\n",
      "iteration 300 / 1500: loss 6.209390\n",
      "iteration 400 / 1500: loss 2.713270\n",
      "iteration 500 / 1500: loss 2.169190\n",
      "iteration 600 / 1500: loss 2.102051\n",
      "iteration 700 / 1500: loss 2.060052\n",
      "iteration 800 / 1500: loss 2.108702\n",
      "iteration 900 / 1500: loss 2.106640\n",
      "iteration 1000 / 1500: loss 2.082415\n",
      "iteration 1100 / 1500: loss 2.042745\n",
      "iteration 1200 / 1500: loss 2.138513\n",
      "iteration 1300 / 1500: loss 2.121684\n",
      "iteration 1400 / 1500: loss 2.041497\n",
      "iteration 0 / 1500: loss 1171.946986\n",
      "iteration 100 / 1500: loss 172.413049\n",
      "iteration 200 / 1500: loss 26.958730\n",
      "iteration 300 / 1500: loss 5.677708\n",
      "iteration 400 / 1500: loss 2.666042\n",
      "iteration 500 / 1500: loss 2.184674\n",
      "iteration 600 / 1500: loss 2.139579\n",
      "iteration 700 / 1500: loss 2.084032\n",
      "iteration 800 / 1500: loss 2.069739\n",
      "iteration 900 / 1500: loss 2.039754\n",
      "iteration 1000 / 1500: loss 2.070374\n",
      "iteration 1100 / 1500: loss 2.062389\n",
      "iteration 1200 / 1500: loss 2.115836\n",
      "iteration 1300 / 1500: loss 2.045761\n",
      "iteration 1400 / 1500: loss 2.093871\n",
      "iteration 0 / 1500: loss 1208.088119\n",
      "iteration 100 / 1500: loss 168.758067\n",
      "iteration 200 / 1500: loss 25.161371\n",
      "iteration 300 / 1500: loss 5.274859\n",
      "iteration 400 / 1500: loss 2.550372\n",
      "iteration 500 / 1500: loss 2.197484\n",
      "iteration 600 / 1500: loss 2.103959\n",
      "iteration 700 / 1500: loss 2.105641\n",
      "iteration 800 / 1500: loss 2.105188\n",
      "iteration 900 / 1500: loss 2.130130\n",
      "iteration 1000 / 1500: loss 2.091397\n",
      "iteration 1100 / 1500: loss 2.145689\n",
      "iteration 1200 / 1500: loss 2.097137\n",
      "iteration 1300 / 1500: loss 2.072099\n",
      "iteration 1400 / 1500: loss 2.087927\n",
      "iteration 0 / 1500: loss 1229.888465\n",
      "iteration 100 / 1500: loss 163.581392\n",
      "iteration 200 / 1500: loss 23.331495\n",
      "iteration 300 / 1500: loss 4.919796\n",
      "iteration 400 / 1500: loss 2.436505\n",
      "iteration 500 / 1500: loss 2.146629\n",
      "iteration 600 / 1500: loss 2.077673\n",
      "iteration 700 / 1500: loss 2.121152\n",
      "iteration 800 / 1500: loss 2.068808\n",
      "iteration 900 / 1500: loss 2.072265\n",
      "iteration 1000 / 1500: loss 2.109892\n",
      "iteration 1100 / 1500: loss 2.114628\n",
      "iteration 1200 / 1500: loss 2.069060\n",
      "iteration 1300 / 1500: loss 2.154634\n",
      "iteration 1400 / 1500: loss 2.097810\n",
      "iteration 0 / 1500: loss 1253.855038\n",
      "iteration 100 / 1500: loss 158.576666\n",
      "iteration 200 / 1500: loss 21.667880\n",
      "iteration 300 / 1500: loss 4.557400\n",
      "iteration 400 / 1500: loss 2.399390\n",
      "iteration 500 / 1500: loss 2.163888\n",
      "iteration 600 / 1500: loss 2.111230\n",
      "iteration 700 / 1500: loss 2.104413\n",
      "iteration 800 / 1500: loss 2.116092\n",
      "iteration 900 / 1500: loss 2.112517\n",
      "iteration 1000 / 1500: loss 2.087805\n",
      "iteration 1100 / 1500: loss 2.106112\n",
      "iteration 1200 / 1500: loss 2.117620\n",
      "iteration 1300 / 1500: loss 2.098890\n",
      "iteration 1400 / 1500: loss 2.124640\n",
      "iteration 0 / 1500: loss 1286.985559\n",
      "iteration 100 / 1500: loss 154.654056\n",
      "iteration 200 / 1500: loss 20.194329\n",
      "iteration 300 / 1500: loss 4.264394\n",
      "iteration 400 / 1500: loss 2.314713\n",
      "iteration 500 / 1500: loss 2.125383\n",
      "iteration 600 / 1500: loss 2.106183\n",
      "iteration 700 / 1500: loss 2.061356\n",
      "iteration 800 / 1500: loss 2.098878\n",
      "iteration 900 / 1500: loss 2.078450\n",
      "iteration 1000 / 1500: loss 2.100094\n",
      "iteration 1100 / 1500: loss 2.077741\n",
      "iteration 1200 / 1500: loss 2.097451\n",
      "iteration 1300 / 1500: loss 2.117358\n",
      "iteration 1400 / 1500: loss 2.052546\n",
      "iteration 0 / 1500: loss 1337.038510\n",
      "iteration 100 / 1500: loss 152.686262\n",
      "iteration 200 / 1500: loss 19.097769\n",
      "iteration 300 / 1500: loss 4.072158\n",
      "iteration 400 / 1500: loss 2.364449\n",
      "iteration 500 / 1500: loss 2.171364\n",
      "iteration 600 / 1500: loss 2.126159\n",
      "iteration 700 / 1500: loss 2.121245\n",
      "iteration 800 / 1500: loss 2.111442\n",
      "iteration 900 / 1500: loss 2.141143\n",
      "iteration 1000 / 1500: loss 2.143788\n",
      "iteration 1100 / 1500: loss 2.123708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 2.134863\n",
      "iteration 1300 / 1500: loss 2.088724\n",
      "iteration 1400 / 1500: loss 2.109484\n",
      "iteration 0 / 1500: loss 1363.038472\n",
      "iteration 100 / 1500: loss 147.989130\n",
      "iteration 200 / 1500: loss 17.745202\n",
      "iteration 300 / 1500: loss 3.759497\n",
      "iteration 400 / 1500: loss 2.283184\n",
      "iteration 500 / 1500: loss 2.101129\n",
      "iteration 600 / 1500: loss 2.111804\n",
      "iteration 700 / 1500: loss 2.118845\n",
      "iteration 800 / 1500: loss 2.113233\n",
      "iteration 900 / 1500: loss 2.112078\n",
      "iteration 1000 / 1500: loss 2.133303\n",
      "iteration 1100 / 1500: loss 2.095312\n",
      "iteration 1200 / 1500: loss 2.186576\n",
      "iteration 1300 / 1500: loss 2.102127\n",
      "iteration 1400 / 1500: loss 2.087270\n",
      "iteration 0 / 1500: loss 1385.057226\n",
      "iteration 100 / 1500: loss 142.814575\n",
      "iteration 200 / 1500: loss 16.404606\n",
      "iteration 300 / 1500: loss 3.580574\n",
      "iteration 400 / 1500: loss 2.188867\n",
      "iteration 500 / 1500: loss 2.120570\n",
      "iteration 600 / 1500: loss 2.135973\n",
      "iteration 700 / 1500: loss 2.092017\n",
      "iteration 800 / 1500: loss 2.124922\n",
      "iteration 900 / 1500: loss 2.093754\n",
      "iteration 1000 / 1500: loss 2.117285\n",
      "iteration 1100 / 1500: loss 2.154894\n",
      "iteration 1200 / 1500: loss 2.134890\n",
      "iteration 1300 / 1500: loss 2.073458\n",
      "iteration 1400 / 1500: loss 2.086858\n",
      "iteration 0 / 1500: loss 1411.522248\n",
      "iteration 100 / 1500: loss 138.690269\n",
      "iteration 200 / 1500: loss 15.353392\n",
      "iteration 300 / 1500: loss 3.387244\n",
      "iteration 400 / 1500: loss 2.147078\n",
      "iteration 500 / 1500: loss 2.117573\n",
      "iteration 600 / 1500: loss 2.163738\n",
      "iteration 700 / 1500: loss 2.117218\n",
      "iteration 800 / 1500: loss 2.096707\n",
      "iteration 900 / 1500: loss 2.118908\n",
      "iteration 1000 / 1500: loss 2.122222\n",
      "iteration 1100 / 1500: loss 2.102748\n",
      "iteration 1200 / 1500: loss 2.096968\n",
      "iteration 1300 / 1500: loss 2.138776\n",
      "iteration 1400 / 1500: loss 2.083760\n",
      "iteration 0 / 1500: loss 1454.728579\n",
      "iteration 100 / 1500: loss 135.757367\n",
      "iteration 200 / 1500: loss 14.462742\n",
      "iteration 300 / 1500: loss 3.241073\n",
      "iteration 400 / 1500: loss 2.221148\n",
      "iteration 500 / 1500: loss 2.128866\n",
      "iteration 600 / 1500: loss 2.136512\n",
      "iteration 700 / 1500: loss 2.098603\n",
      "iteration 800 / 1500: loss 2.112764\n",
      "iteration 900 / 1500: loss 2.096565\n",
      "iteration 1000 / 1500: loss 2.101215\n",
      "iteration 1100 / 1500: loss 2.119011\n",
      "iteration 1200 / 1500: loss 2.061356\n",
      "iteration 1300 / 1500: loss 2.131679\n",
      "iteration 1400 / 1500: loss 2.086397\n",
      "iteration 0 / 1500: loss 1469.129999\n",
      "iteration 100 / 1500: loss 130.487491\n",
      "iteration 200 / 1500: loss 13.385710\n",
      "iteration 300 / 1500: loss 3.083703\n",
      "iteration 400 / 1500: loss 2.204516\n",
      "iteration 500 / 1500: loss 2.060689\n",
      "iteration 600 / 1500: loss 2.097585\n",
      "iteration 700 / 1500: loss 2.088306\n",
      "iteration 800 / 1500: loss 2.127546\n",
      "iteration 900 / 1500: loss 2.092818\n",
      "iteration 1000 / 1500: loss 2.146120\n",
      "iteration 1100 / 1500: loss 2.158808\n",
      "iteration 1200 / 1500: loss 2.137446\n",
      "iteration 1300 / 1500: loss 2.129423\n",
      "iteration 1400 / 1500: loss 2.124250\n",
      "iteration 0 / 1500: loss 1521.861865\n",
      "iteration 100 / 1500: loss 128.321405\n",
      "iteration 200 / 1500: loss 12.595979\n",
      "iteration 300 / 1500: loss 3.014424\n",
      "iteration 400 / 1500: loss 2.169111\n",
      "iteration 500 / 1500: loss 2.096456\n",
      "iteration 600 / 1500: loss 2.133404\n",
      "iteration 700 / 1500: loss 2.119729\n",
      "iteration 800 / 1500: loss 2.098009\n",
      "iteration 900 / 1500: loss 2.121603\n",
      "iteration 1000 / 1500: loss 2.118794\n",
      "iteration 1100 / 1500: loss 2.117032\n",
      "iteration 1200 / 1500: loss 2.146112\n",
      "iteration 1300 / 1500: loss 2.126108\n",
      "iteration 1400 / 1500: loss 2.096631\n",
      "iteration 0 / 1500: loss 1555.007061\n",
      "iteration 100 / 1500: loss 124.699229\n",
      "iteration 200 / 1500: loss 11.825074\n",
      "iteration 300 / 1500: loss 2.875917\n",
      "iteration 400 / 1500: loss 2.175228\n",
      "iteration 500 / 1500: loss 2.112302\n",
      "iteration 600 / 1500: loss 2.104008\n",
      "iteration 700 / 1500: loss 2.156035\n",
      "iteration 800 / 1500: loss 2.109287\n",
      "iteration 900 / 1500: loss 2.099751\n",
      "iteration 1000 / 1500: loss 2.111754\n",
      "iteration 1100 / 1500: loss 2.118569\n",
      "iteration 1200 / 1500: loss 2.095416\n",
      "iteration 1300 / 1500: loss 2.112156\n",
      "iteration 1400 / 1500: loss 2.128818\n",
      "iteration 0 / 1500: loss 774.536366\n",
      "iteration 100 / 1500: loss 158.364168\n",
      "iteration 200 / 1500: loss 33.745275\n",
      "iteration 300 / 1500: loss 8.519295\n",
      "iteration 400 / 1500: loss 3.381936\n",
      "iteration 500 / 1500: loss 2.295455\n",
      "iteration 600 / 1500: loss 2.102289\n",
      "iteration 700 / 1500: loss 2.061624\n",
      "iteration 800 / 1500: loss 2.089175\n",
      "iteration 900 / 1500: loss 2.127647\n",
      "iteration 1000 / 1500: loss 2.064850\n",
      "iteration 1100 / 1500: loss 2.035118\n",
      "iteration 1200 / 1500: loss 2.056364\n",
      "iteration 1300 / 1500: loss 1.989826\n",
      "iteration 1400 / 1500: loss 2.048500\n",
      "iteration 0 / 1500: loss 798.155754\n",
      "iteration 100 / 1500: loss 153.172586\n",
      "iteration 200 / 1500: loss 30.803444\n",
      "iteration 300 / 1500: loss 7.515749\n",
      "iteration 400 / 1500: loss 3.093278\n",
      "iteration 500 / 1500: loss 2.252330\n",
      "iteration 600 / 1500: loss 2.105414\n",
      "iteration 700 / 1500: loss 2.044074\n",
      "iteration 800 / 1500: loss 2.090491\n",
      "iteration 900 / 1500: loss 2.082770\n",
      "iteration 1000 / 1500: loss 2.049453\n",
      "iteration 1100 / 1500: loss 2.049692\n",
      "iteration 1200 / 1500: loss 2.018828\n",
      "iteration 1300 / 1500: loss 2.035667\n",
      "iteration 1400 / 1500: loss 2.034065\n",
      "iteration 0 / 1500: loss 825.799518\n",
      "iteration 100 / 1500: loss 148.965372\n",
      "iteration 200 / 1500: loss 28.280833\n",
      "iteration 300 / 1500: loss 6.798741\n",
      "iteration 400 / 1500: loss 2.987058\n",
      "iteration 500 / 1500: loss 2.203004\n",
      "iteration 600 / 1500: loss 2.075887\n",
      "iteration 700 / 1500: loss 2.044898\n",
      "iteration 800 / 1500: loss 2.117466\n",
      "iteration 900 / 1500: loss 2.076042\n",
      "iteration 1000 / 1500: loss 2.043810\n",
      "iteration 1100 / 1500: loss 2.086728\n",
      "iteration 1200 / 1500: loss 2.064010\n",
      "iteration 1300 / 1500: loss 2.079828\n",
      "iteration 1400 / 1500: loss 2.010754\n",
      "iteration 0 / 1500: loss 853.013304\n",
      "iteration 100 / 1500: loss 144.213268\n",
      "iteration 200 / 1500: loss 25.859085\n",
      "iteration 300 / 1500: loss 6.059214\n",
      "iteration 400 / 1500: loss 2.757978\n",
      "iteration 500 / 1500: loss 2.176228\n",
      "iteration 600 / 1500: loss 2.078003\n",
      "iteration 700 / 1500: loss 2.081308\n",
      "iteration 800 / 1500: loss 2.080569\n",
      "iteration 900 / 1500: loss 2.036364\n",
      "iteration 1000 / 1500: loss 2.104253\n",
      "iteration 1100 / 1500: loss 2.061448\n",
      "iteration 1200 / 1500: loss 2.045590\n",
      "iteration 1300 / 1500: loss 2.060194\n",
      "iteration 1400 / 1500: loss 2.082258\n",
      "iteration 0 / 1500: loss 887.229704\n",
      "iteration 100 / 1500: loss 140.848987\n",
      "iteration 200 / 1500: loss 23.856312\n",
      "iteration 300 / 1500: loss 5.498507\n",
      "iteration 400 / 1500: loss 2.628275\n",
      "iteration 500 / 1500: loss 2.159864\n",
      "iteration 600 / 1500: loss 2.187713\n",
      "iteration 700 / 1500: loss 2.068216\n",
      "iteration 800 / 1500: loss 2.101054\n",
      "iteration 900 / 1500: loss 2.053575\n",
      "iteration 1000 / 1500: loss 2.008410\n",
      "iteration 1100 / 1500: loss 2.048564\n",
      "iteration 1200 / 1500: loss 2.084610\n",
      "iteration 1300 / 1500: loss 2.099718\n",
      "iteration 1400 / 1500: loss 2.067533\n",
      "iteration 0 / 1500: loss 913.298296\n",
      "iteration 100 / 1500: loss 136.006673\n",
      "iteration 200 / 1500: loss 21.843313\n",
      "iteration 300 / 1500: loss 4.993564\n",
      "iteration 400 / 1500: loss 2.503772\n",
      "iteration 500 / 1500: loss 2.162358\n",
      "iteration 600 / 1500: loss 2.064531\n",
      "iteration 700 / 1500: loss 2.069788\n",
      "iteration 800 / 1500: loss 2.143170\n",
      "iteration 900 / 1500: loss 2.071160\n",
      "iteration 1000 / 1500: loss 2.070037\n",
      "iteration 1100 / 1500: loss 2.094289\n",
      "iteration 1200 / 1500: loss 2.048974\n",
      "iteration 1300 / 1500: loss 2.117004\n",
      "iteration 1400 / 1500: loss 2.044265\n",
      "iteration 0 / 1500: loss 940.311189\n",
      "iteration 100 / 1500: loss 131.727074\n",
      "iteration 200 / 1500: loss 19.956255\n",
      "iteration 300 / 1500: loss 4.520698\n",
      "iteration 400 / 1500: loss 2.418812\n",
      "iteration 500 / 1500: loss 2.104303\n",
      "iteration 600 / 1500: loss 2.100663\n",
      "iteration 700 / 1500: loss 2.132078\n",
      "iteration 800 / 1500: loss 2.044210\n",
      "iteration 900 / 1500: loss 2.045235\n",
      "iteration 1000 / 1500: loss 2.073085\n",
      "iteration 1100 / 1500: loss 2.123427\n",
      "iteration 1200 / 1500: loss 2.124042\n",
      "iteration 1300 / 1500: loss 2.100409\n",
      "iteration 1400 / 1500: loss 2.069743\n",
      "iteration 0 / 1500: loss 992.297104\n",
      "iteration 100 / 1500: loss 130.216319\n",
      "iteration 200 / 1500: loss 18.722399\n",
      "iteration 300 / 1500: loss 4.197577\n",
      "iteration 400 / 1500: loss 2.339907\n",
      "iteration 500 / 1500: loss 2.101678\n",
      "iteration 600 / 1500: loss 2.095632\n",
      "iteration 700 / 1500: loss 2.116702\n",
      "iteration 800 / 1500: loss 2.057664\n",
      "iteration 900 / 1500: loss 2.048015\n",
      "iteration 1000 / 1500: loss 2.080027\n",
      "iteration 1100 / 1500: loss 2.058318\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 2.061367\n",
      "iteration 1300 / 1500: loss 2.118804\n",
      "iteration 1400 / 1500: loss 2.072250\n",
      "iteration 0 / 1500: loss 1034.143649\n",
      "iteration 100 / 1500: loss 127.288453\n",
      "iteration 200 / 1500: loss 17.294207\n",
      "iteration 300 / 1500: loss 3.933909\n",
      "iteration 400 / 1500: loss 2.303796\n",
      "iteration 500 / 1500: loss 2.093344\n",
      "iteration 600 / 1500: loss 2.095937\n",
      "iteration 700 / 1500: loss 2.108245\n",
      "iteration 800 / 1500: loss 2.083524\n",
      "iteration 900 / 1500: loss 2.048358\n",
      "iteration 1000 / 1500: loss 2.103784\n",
      "iteration 1100 / 1500: loss 2.098484\n",
      "iteration 1200 / 1500: loss 2.078239\n",
      "iteration 1300 / 1500: loss 2.094063\n",
      "iteration 1400 / 1500: loss 2.101782\n",
      "iteration 0 / 1500: loss 1047.640049\n",
      "iteration 100 / 1500: loss 120.849764\n",
      "iteration 200 / 1500: loss 15.613026\n",
      "iteration 300 / 1500: loss 3.563722\n",
      "iteration 400 / 1500: loss 2.314993\n",
      "iteration 500 / 1500: loss 2.096474\n",
      "iteration 600 / 1500: loss 2.091694\n",
      "iteration 700 / 1500: loss 2.110187\n",
      "iteration 800 / 1500: loss 2.062782\n",
      "iteration 900 / 1500: loss 2.122946\n",
      "iteration 1000 / 1500: loss 2.084054\n",
      "iteration 1100 / 1500: loss 2.080156\n",
      "iteration 1200 / 1500: loss 2.140929\n",
      "iteration 1300 / 1500: loss 2.131916\n",
      "iteration 1400 / 1500: loss 2.088236\n",
      "iteration 0 / 1500: loss 1069.475840\n",
      "iteration 100 / 1500: loss 115.872526\n",
      "iteration 200 / 1500: loss 14.288393\n",
      "iteration 300 / 1500: loss 3.382417\n",
      "iteration 400 / 1500: loss 2.222382\n",
      "iteration 500 / 1500: loss 2.109844\n",
      "iteration 600 / 1500: loss 2.054667\n",
      "iteration 700 / 1500: loss 2.050680\n",
      "iteration 800 / 1500: loss 2.067326\n",
      "iteration 900 / 1500: loss 2.075182\n",
      "iteration 1000 / 1500: loss 2.120007\n",
      "iteration 1100 / 1500: loss 2.081738\n",
      "iteration 1200 / 1500: loss 2.046532\n",
      "iteration 1300 / 1500: loss 2.087238\n",
      "iteration 1400 / 1500: loss 2.095451\n",
      "iteration 0 / 1500: loss 1110.512056\n",
      "iteration 100 / 1500: loss 113.131350\n",
      "iteration 200 / 1500: loss 13.237022\n",
      "iteration 300 / 1500: loss 3.249809\n",
      "iteration 400 / 1500: loss 2.226515\n",
      "iteration 500 / 1500: loss 2.076879\n",
      "iteration 600 / 1500: loss 2.064085\n",
      "iteration 700 / 1500: loss 2.109173\n",
      "iteration 800 / 1500: loss 2.081125\n",
      "iteration 900 / 1500: loss 2.055473\n",
      "iteration 1000 / 1500: loss 2.077556\n",
      "iteration 1100 / 1500: loss 2.092907\n",
      "iteration 1200 / 1500: loss 2.114515\n",
      "iteration 1300 / 1500: loss 2.151813\n",
      "iteration 1400 / 1500: loss 2.094426\n",
      "iteration 0 / 1500: loss 1145.314542\n",
      "iteration 100 / 1500: loss 109.542105\n",
      "iteration 200 / 1500: loss 12.235104\n",
      "iteration 300 / 1500: loss 3.023145\n",
      "iteration 400 / 1500: loss 2.170160\n",
      "iteration 500 / 1500: loss 2.098188\n",
      "iteration 600 / 1500: loss 2.113875\n",
      "iteration 700 / 1500: loss 2.086285\n",
      "iteration 800 / 1500: loss 2.102584\n",
      "iteration 900 / 1500: loss 2.099046\n",
      "iteration 1000 / 1500: loss 2.075657\n",
      "iteration 1100 / 1500: loss 2.123108\n",
      "iteration 1200 / 1500: loss 2.139642\n",
      "iteration 1300 / 1500: loss 2.060480\n",
      "iteration 1400 / 1500: loss 2.112442\n",
      "iteration 0 / 1500: loss 1175.888025\n",
      "iteration 100 / 1500: loss 105.434559\n",
      "iteration 200 / 1500: loss 11.258371\n",
      "iteration 300 / 1500: loss 2.906508\n",
      "iteration 400 / 1500: loss 2.169022\n",
      "iteration 500 / 1500: loss 2.109952\n",
      "iteration 600 / 1500: loss 2.028956\n",
      "iteration 700 / 1500: loss 2.067280\n",
      "iteration 800 / 1500: loss 2.071535\n",
      "iteration 900 / 1500: loss 2.085414\n",
      "iteration 1000 / 1500: loss 2.098308\n",
      "iteration 1100 / 1500: loss 2.136545\n",
      "iteration 1200 / 1500: loss 2.147938\n",
      "iteration 1300 / 1500: loss 2.138724\n",
      "iteration 1400 / 1500: loss 2.099793\n",
      "iteration 0 / 1500: loss 1203.231035\n",
      "iteration 100 / 1500: loss 101.314602\n",
      "iteration 200 / 1500: loss 10.270270\n",
      "iteration 300 / 1500: loss 2.787024\n",
      "iteration 400 / 1500: loss 2.141734\n",
      "iteration 500 / 1500: loss 2.030863\n",
      "iteration 600 / 1500: loss 2.080360\n",
      "iteration 700 / 1500: loss 2.100761\n",
      "iteration 800 / 1500: loss 2.113413\n",
      "iteration 900 / 1500: loss 2.117242\n",
      "iteration 1000 / 1500: loss 2.059070\n",
      "iteration 1100 / 1500: loss 2.107871\n",
      "iteration 1200 / 1500: loss 2.077655\n",
      "iteration 1300 / 1500: loss 2.119046\n",
      "iteration 1400 / 1500: loss 2.095821\n",
      "iteration 0 / 1500: loss 1224.975851\n",
      "iteration 100 / 1500: loss 96.881477\n",
      "iteration 200 / 1500: loss 9.458027\n",
      "iteration 300 / 1500: loss 2.687077\n",
      "iteration 400 / 1500: loss 2.138283\n",
      "iteration 500 / 1500: loss 2.114578\n",
      "iteration 600 / 1500: loss 2.096253\n",
      "iteration 700 / 1500: loss 2.126746\n",
      "iteration 800 / 1500: loss 2.169687\n",
      "iteration 900 / 1500: loss 2.062695\n",
      "iteration 1000 / 1500: loss 2.100637\n",
      "iteration 1100 / 1500: loss 2.127116\n",
      "iteration 1200 / 1500: loss 2.104590\n",
      "iteration 1300 / 1500: loss 2.102123\n",
      "iteration 1400 / 1500: loss 2.030084\n",
      "iteration 0 / 1500: loss 1255.658193\n",
      "iteration 100 / 1500: loss 93.285579\n",
      "iteration 200 / 1500: loss 8.748868\n",
      "iteration 300 / 1500: loss 2.596517\n",
      "iteration 400 / 1500: loss 2.143674\n",
      "iteration 500 / 1500: loss 2.211413\n",
      "iteration 600 / 1500: loss 2.135586\n",
      "iteration 700 / 1500: loss 2.062485\n",
      "iteration 800 / 1500: loss 2.122285\n",
      "iteration 900 / 1500: loss 2.049316\n",
      "iteration 1000 / 1500: loss 2.145822\n",
      "iteration 1100 / 1500: loss 2.108475\n",
      "iteration 1200 / 1500: loss 2.114905\n",
      "iteration 1300 / 1500: loss 2.098641\n",
      "iteration 1400 / 1500: loss 2.096460\n",
      "iteration 0 / 1500: loss 1288.556484\n",
      "iteration 100 / 1500: loss 89.661041\n",
      "iteration 200 / 1500: loss 8.113731\n",
      "iteration 300 / 1500: loss 2.483494\n",
      "iteration 400 / 1500: loss 2.128976\n",
      "iteration 500 / 1500: loss 2.052265\n",
      "iteration 600 / 1500: loss 2.122731\n",
      "iteration 700 / 1500: loss 2.075756\n",
      "iteration 800 / 1500: loss 2.106202\n",
      "iteration 900 / 1500: loss 2.065826\n",
      "iteration 1000 / 1500: loss 2.070282\n",
      "iteration 1100 / 1500: loss 2.112951\n",
      "iteration 1200 / 1500: loss 2.120332\n",
      "iteration 1300 / 1500: loss 2.169537\n",
      "iteration 1400 / 1500: loss 2.118182\n",
      "iteration 0 / 1500: loss 1338.956915\n",
      "iteration 100 / 1500: loss 87.573608\n",
      "iteration 200 / 1500: loss 7.548390\n",
      "iteration 300 / 1500: loss 2.437179\n",
      "iteration 400 / 1500: loss 2.167342\n",
      "iteration 500 / 1500: loss 2.108194\n",
      "iteration 600 / 1500: loss 2.148751\n",
      "iteration 700 / 1500: loss 2.119115\n",
      "iteration 800 / 1500: loss 2.050264\n",
      "iteration 900 / 1500: loss 2.110664\n",
      "iteration 1000 / 1500: loss 2.044995\n",
      "iteration 1100 / 1500: loss 2.063254\n",
      "iteration 1200 / 1500: loss 2.155724\n",
      "iteration 1300 / 1500: loss 2.065691\n",
      "iteration 1400 / 1500: loss 2.062082\n",
      "iteration 0 / 1500: loss 1343.070142\n",
      "iteration 100 / 1500: loss 82.386837\n",
      "iteration 200 / 1500: loss 6.907500\n",
      "iteration 300 / 1500: loss 2.417679\n",
      "iteration 400 / 1500: loss 2.150139\n",
      "iteration 500 / 1500: loss 2.143396\n",
      "iteration 600 / 1500: loss 2.142152\n",
      "iteration 700 / 1500: loss 2.101631\n",
      "iteration 800 / 1500: loss 2.138479\n",
      "iteration 900 / 1500: loss 2.128441\n",
      "iteration 1000 / 1500: loss 2.136828\n",
      "iteration 1100 / 1500: loss 2.123930\n",
      "iteration 1200 / 1500: loss 2.099491\n",
      "iteration 1300 / 1500: loss 2.173384\n",
      "iteration 1400 / 1500: loss 2.065488\n",
      "iteration 0 / 1500: loss 1373.899899\n",
      "iteration 100 / 1500: loss 79.119938\n",
      "iteration 200 / 1500: loss 6.478543\n",
      "iteration 300 / 1500: loss 2.327496\n",
      "iteration 400 / 1500: loss 2.154507\n",
      "iteration 500 / 1500: loss 2.072502\n",
      "iteration 600 / 1500: loss 2.159407\n",
      "iteration 700 / 1500: loss 2.122498\n",
      "iteration 800 / 1500: loss 2.108279\n",
      "iteration 900 / 1500: loss 2.081593\n",
      "iteration 1000 / 1500: loss 2.128759\n",
      "iteration 1100 / 1500: loss 2.105142\n",
      "iteration 1200 / 1500: loss 2.151756\n",
      "iteration 1300 / 1500: loss 2.143052\n",
      "iteration 1400 / 1500: loss 2.097984\n",
      "iteration 0 / 1500: loss 1422.869510\n",
      "iteration 100 / 1500: loss 77.071086\n",
      "iteration 200 / 1500: loss 6.056930\n",
      "iteration 300 / 1500: loss 2.353369\n",
      "iteration 400 / 1500: loss 2.120930\n",
      "iteration 500 / 1500: loss 2.123455\n",
      "iteration 600 / 1500: loss 2.098210\n",
      "iteration 700 / 1500: loss 2.186204\n",
      "iteration 800 / 1500: loss 2.119640\n",
      "iteration 900 / 1500: loss 2.089407\n",
      "iteration 1000 / 1500: loss 2.096513\n",
      "iteration 1100 / 1500: loss 2.110379\n",
      "iteration 1200 / 1500: loss 2.118504\n",
      "iteration 1300 / 1500: loss 2.140116\n",
      "iteration 1400 / 1500: loss 2.103974\n",
      "iteration 0 / 1500: loss 1446.666270\n",
      "iteration 100 / 1500: loss 73.500277\n",
      "iteration 200 / 1500: loss 5.668329\n",
      "iteration 300 / 1500: loss 2.295879\n",
      "iteration 400 / 1500: loss 2.090045\n",
      "iteration 500 / 1500: loss 2.086735\n",
      "iteration 600 / 1500: loss 2.153668\n",
      "iteration 700 / 1500: loss 2.122289\n",
      "iteration 800 / 1500: loss 2.134061\n",
      "iteration 900 / 1500: loss 2.143199\n",
      "iteration 1000 / 1500: loss 2.125656\n",
      "iteration 1100 / 1500: loss 2.152121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 2.113918\n",
      "iteration 1300 / 1500: loss 2.097751\n",
      "iteration 1400 / 1500: loss 2.099707\n",
      "iteration 0 / 1500: loss 1473.054461\n",
      "iteration 100 / 1500: loss 70.196693\n",
      "iteration 200 / 1500: loss 5.278413\n",
      "iteration 300 / 1500: loss 2.275905\n",
      "iteration 400 / 1500: loss 2.055986\n",
      "iteration 500 / 1500: loss 2.110094\n",
      "iteration 600 / 1500: loss 2.122658\n",
      "iteration 700 / 1500: loss 2.115814\n",
      "iteration 800 / 1500: loss 2.091694\n",
      "iteration 900 / 1500: loss 2.187651\n",
      "iteration 1000 / 1500: loss 2.093402\n",
      "iteration 1100 / 1500: loss 2.108998\n",
      "iteration 1200 / 1500: loss 2.106809\n",
      "iteration 1300 / 1500: loss 2.158533\n",
      "iteration 1400 / 1500: loss 2.091120\n",
      "iteration 0 / 1500: loss 1480.342027\n",
      "iteration 100 / 1500: loss 66.442707\n",
      "iteration 200 / 1500: loss 4.856823\n",
      "iteration 300 / 1500: loss 2.255604\n",
      "iteration 400 / 1500: loss 2.169463\n",
      "iteration 500 / 1500: loss 2.113872\n",
      "iteration 600 / 1500: loss 2.128601\n",
      "iteration 700 / 1500: loss 2.101393\n",
      "iteration 800 / 1500: loss 2.126263\n",
      "iteration 900 / 1500: loss 2.101397\n",
      "iteration 1000 / 1500: loss 2.121664\n",
      "iteration 1100 / 1500: loss 2.125229\n",
      "iteration 1200 / 1500: loss 2.138349\n",
      "iteration 1300 / 1500: loss 2.094798\n",
      "iteration 1400 / 1500: loss 2.128538\n",
      "iteration 0 / 1500: loss 1547.650779\n",
      "iteration 100 / 1500: loss 65.017946\n",
      "iteration 200 / 1500: loss 4.693836\n",
      "iteration 300 / 1500: loss 2.219053\n",
      "iteration 400 / 1500: loss 2.096142\n",
      "iteration 500 / 1500: loss 2.122579\n",
      "iteration 600 / 1500: loss 2.124568\n",
      "iteration 700 / 1500: loss 2.154154\n",
      "iteration 800 / 1500: loss 2.132243\n",
      "iteration 900 / 1500: loss 2.133551\n",
      "iteration 1000 / 1500: loss 2.184350\n",
      "iteration 1100 / 1500: loss 2.148387\n",
      "iteration 1200 / 1500: loss 2.094689\n",
      "iteration 1300 / 1500: loss 2.080653\n",
      "iteration 1400 / 1500: loss 2.092640\n",
      "iteration 0 / 1500: loss 780.000973\n",
      "iteration 100 / 1500: loss 106.000458\n",
      "iteration 200 / 1500: loss 15.924902\n",
      "iteration 300 / 1500: loss 3.947885\n",
      "iteration 400 / 1500: loss 2.275644\n",
      "iteration 500 / 1500: loss 2.060263\n",
      "iteration 600 / 1500: loss 2.038209\n",
      "iteration 700 / 1500: loss 2.107981\n",
      "iteration 800 / 1500: loss 2.068738\n",
      "iteration 900 / 1500: loss 2.072658\n",
      "iteration 1000 / 1500: loss 2.031294\n",
      "iteration 1100 / 1500: loss 2.067706\n",
      "iteration 1200 / 1500: loss 2.086843\n",
      "iteration 1300 / 1500: loss 1.959957\n",
      "iteration 1400 / 1500: loss 2.028478\n",
      "iteration 0 / 1500: loss 803.532282\n",
      "iteration 100 / 1500: loss 100.905540\n",
      "iteration 200 / 1500: loss 14.237044\n",
      "iteration 300 / 1500: loss 3.626854\n",
      "iteration 400 / 1500: loss 2.260820\n",
      "iteration 500 / 1500: loss 2.028496\n",
      "iteration 600 / 1500: loss 2.026131\n",
      "iteration 700 / 1500: loss 2.043161\n",
      "iteration 800 / 1500: loss 2.067422\n",
      "iteration 900 / 1500: loss 2.037466\n",
      "iteration 1000 / 1500: loss 2.054430\n",
      "iteration 1100 / 1500: loss 2.125034\n",
      "iteration 1200 / 1500: loss 2.060956\n",
      "iteration 1300 / 1500: loss 2.047354\n",
      "iteration 1400 / 1500: loss 2.058783\n",
      "iteration 0 / 1500: loss 834.693706\n",
      "iteration 100 / 1500: loss 96.660745\n",
      "iteration 200 / 1500: loss 12.729894\n",
      "iteration 300 / 1500: loss 3.304322\n",
      "iteration 400 / 1500: loss 2.277663\n",
      "iteration 500 / 1500: loss 2.087681\n",
      "iteration 600 / 1500: loss 2.104420\n",
      "iteration 700 / 1500: loss 2.038168\n",
      "iteration 800 / 1500: loss 2.062483\n",
      "iteration 900 / 1500: loss 2.094322\n",
      "iteration 1000 / 1500: loss 2.013750\n",
      "iteration 1100 / 1500: loss 2.067471\n",
      "iteration 1200 / 1500: loss 2.050920\n",
      "iteration 1300 / 1500: loss 2.056933\n",
      "iteration 1400 / 1500: loss 2.057230\n",
      "iteration 0 / 1500: loss 874.321528\n",
      "iteration 100 / 1500: loss 93.548218\n",
      "iteration 200 / 1500: loss 11.741660\n",
      "iteration 300 / 1500: loss 3.037294\n",
      "iteration 400 / 1500: loss 2.139430\n",
      "iteration 500 / 1500: loss 2.042992\n",
      "iteration 600 / 1500: loss 2.121876\n",
      "iteration 700 / 1500: loss 2.090882\n",
      "iteration 800 / 1500: loss 2.071273\n",
      "iteration 900 / 1500: loss 2.051030\n",
      "iteration 1000 / 1500: loss 2.014932\n",
      "iteration 1100 / 1500: loss 2.041560\n",
      "iteration 1200 / 1500: loss 2.052212\n",
      "iteration 1300 / 1500: loss 2.040566\n",
      "iteration 1400 / 1500: loss 2.071861\n",
      "iteration 0 / 1500: loss 881.412812\n",
      "iteration 100 / 1500: loss 87.031106\n",
      "iteration 200 / 1500: loss 10.366298\n",
      "iteration 300 / 1500: loss 2.917946\n",
      "iteration 400 / 1500: loss 2.132754\n",
      "iteration 500 / 1500: loss 1.993933\n",
      "iteration 600 / 1500: loss 1.985791\n",
      "iteration 700 / 1500: loss 2.040249\n",
      "iteration 800 / 1500: loss 2.020497\n",
      "iteration 900 / 1500: loss 2.050014\n",
      "iteration 1000 / 1500: loss 2.065404\n",
      "iteration 1100 / 1500: loss 2.052429\n",
      "iteration 1200 / 1500: loss 2.102867\n",
      "iteration 1300 / 1500: loss 2.042073\n",
      "iteration 1400 / 1500: loss 2.055418\n",
      "iteration 0 / 1500: loss 935.613578\n",
      "iteration 100 / 1500: loss 85.501120\n",
      "iteration 200 / 1500: loss 9.538781\n",
      "iteration 300 / 1500: loss 2.669834\n",
      "iteration 400 / 1500: loss 2.131008\n",
      "iteration 500 / 1500: loss 2.043857\n",
      "iteration 600 / 1500: loss 2.065194\n",
      "iteration 700 / 1500: loss 2.085057\n",
      "iteration 800 / 1500: loss 2.080676\n",
      "iteration 900 / 1500: loss 2.043017\n",
      "iteration 1000 / 1500: loss 2.031686\n",
      "iteration 1100 / 1500: loss 2.084737\n",
      "iteration 1200 / 1500: loss 2.049189\n",
      "iteration 1300 / 1500: loss 2.119975\n",
      "iteration 1400 / 1500: loss 2.041665\n",
      "iteration 0 / 1500: loss 949.637312\n",
      "iteration 100 / 1500: loss 79.981860\n",
      "iteration 200 / 1500: loss 8.497507\n",
      "iteration 300 / 1500: loss 2.616940\n",
      "iteration 400 / 1500: loss 2.126834\n",
      "iteration 500 / 1500: loss 2.056883\n",
      "iteration 600 / 1500: loss 2.072342\n",
      "iteration 700 / 1500: loss 2.101162\n",
      "iteration 800 / 1500: loss 2.111986\n",
      "iteration 900 / 1500: loss 2.095493\n",
      "iteration 1000 / 1500: loss 2.172833\n",
      "iteration 1100 / 1500: loss 2.068189\n",
      "iteration 1200 / 1500: loss 2.127171\n",
      "iteration 1300 / 1500: loss 2.038612\n",
      "iteration 1400 / 1500: loss 2.080213\n",
      "iteration 0 / 1500: loss 979.256546\n",
      "iteration 100 / 1500: loss 76.355251\n",
      "iteration 200 / 1500: loss 7.748321\n",
      "iteration 300 / 1500: loss 2.549446\n",
      "iteration 400 / 1500: loss 2.123860\n",
      "iteration 500 / 1500: loss 2.167424\n",
      "iteration 600 / 1500: loss 2.091868\n",
      "iteration 700 / 1500: loss 2.140360\n",
      "iteration 800 / 1500: loss 2.087885\n",
      "iteration 900 / 1500: loss 2.115342\n",
      "iteration 1000 / 1500: loss 2.024471\n",
      "iteration 1100 / 1500: loss 2.080231\n",
      "iteration 1200 / 1500: loss 2.062392\n",
      "iteration 1300 / 1500: loss 2.047429\n",
      "iteration 1400 / 1500: loss 2.146932\n",
      "iteration 0 / 1500: loss 1013.714299\n",
      "iteration 100 / 1500: loss 72.941240\n",
      "iteration 200 / 1500: loss 7.106185\n",
      "iteration 300 / 1500: loss 2.443587\n",
      "iteration 400 / 1500: loss 2.034801\n",
      "iteration 500 / 1500: loss 2.033593\n",
      "iteration 600 / 1500: loss 2.054849\n",
      "iteration 700 / 1500: loss 2.180030\n",
      "iteration 800 / 1500: loss 2.130083\n",
      "iteration 900 / 1500: loss 2.085930\n",
      "iteration 1000 / 1500: loss 2.082563\n",
      "iteration 1100 / 1500: loss 2.098228\n",
      "iteration 1200 / 1500: loss 2.162490\n",
      "iteration 1300 / 1500: loss 2.084132\n",
      "iteration 1400 / 1500: loss 2.061950\n",
      "iteration 0 / 1500: loss 1060.671901\n",
      "iteration 100 / 1500: loss 70.494187\n",
      "iteration 200 / 1500: loss 6.531174\n",
      "iteration 300 / 1500: loss 2.404764\n",
      "iteration 400 / 1500: loss 2.153038\n",
      "iteration 500 / 1500: loss 2.073716\n",
      "iteration 600 / 1500: loss 2.136101\n",
      "iteration 700 / 1500: loss 2.063293\n",
      "iteration 800 / 1500: loss 2.094222\n",
      "iteration 900 / 1500: loss 2.110847\n",
      "iteration 1000 / 1500: loss 2.106689\n",
      "iteration 1100 / 1500: loss 2.037344\n",
      "iteration 1200 / 1500: loss 2.046770\n",
      "iteration 1300 / 1500: loss 2.061873\n",
      "iteration 1400 / 1500: loss 2.114314\n",
      "iteration 0 / 1500: loss 1087.074633\n",
      "iteration 100 / 1500: loss 66.796196\n",
      "iteration 200 / 1500: loss 5.909881\n",
      "iteration 300 / 1500: loss 2.296903\n",
      "iteration 400 / 1500: loss 2.046012\n",
      "iteration 500 / 1500: loss 2.075885\n",
      "iteration 600 / 1500: loss 2.075956\n",
      "iteration 700 / 1500: loss 2.095110\n",
      "iteration 800 / 1500: loss 2.170681\n",
      "iteration 900 / 1500: loss 2.090558\n",
      "iteration 1000 / 1500: loss 2.089226\n",
      "iteration 1100 / 1500: loss 2.012890\n",
      "iteration 1200 / 1500: loss 2.029364\n",
      "iteration 1300 / 1500: loss 2.118448\n",
      "iteration 1400 / 1500: loss 2.110715\n",
      "iteration 0 / 1500: loss 1102.639506\n",
      "iteration 100 / 1500: loss 62.554995\n",
      "iteration 200 / 1500: loss 5.384484\n",
      "iteration 300 / 1500: loss 2.267462\n",
      "iteration 400 / 1500: loss 2.085543\n",
      "iteration 500 / 1500: loss 2.121478\n",
      "iteration 600 / 1500: loss 2.069188\n",
      "iteration 700 / 1500: loss 2.104380\n",
      "iteration 800 / 1500: loss 2.132104\n",
      "iteration 900 / 1500: loss 2.022265\n",
      "iteration 1000 / 1500: loss 2.117983\n",
      "iteration 1100 / 1500: loss 2.069411\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 2.143410\n",
      "iteration 1300 / 1500: loss 2.142757\n",
      "iteration 1400 / 1500: loss 2.117336\n",
      "iteration 0 / 1500: loss 1138.844384\n",
      "iteration 100 / 1500: loss 59.713241\n",
      "iteration 200 / 1500: loss 4.987963\n",
      "iteration 300 / 1500: loss 2.300650\n",
      "iteration 400 / 1500: loss 2.031546\n",
      "iteration 500 / 1500: loss 2.046091\n",
      "iteration 600 / 1500: loss 2.107737\n",
      "iteration 700 / 1500: loss 2.074255\n",
      "iteration 800 / 1500: loss 2.090344\n",
      "iteration 900 / 1500: loss 2.117522\n",
      "iteration 1000 / 1500: loss 2.047903\n",
      "iteration 1100 / 1500: loss 2.047610\n",
      "iteration 1200 / 1500: loss 2.145318\n",
      "iteration 1300 / 1500: loss 2.116537\n",
      "iteration 1400 / 1500: loss 2.060647\n",
      "iteration 0 / 1500: loss 1168.891532\n",
      "iteration 100 / 1500: loss 56.688434\n",
      "iteration 200 / 1500: loss 4.649164\n",
      "iteration 300 / 1500: loss 2.257439\n",
      "iteration 400 / 1500: loss 2.094756\n",
      "iteration 500 / 1500: loss 2.119132\n",
      "iteration 600 / 1500: loss 2.113996\n",
      "iteration 700 / 1500: loss 2.104826\n",
      "iteration 800 / 1500: loss 2.087083\n",
      "iteration 900 / 1500: loss 2.080699\n",
      "iteration 1000 / 1500: loss 2.086076\n",
      "iteration 1100 / 1500: loss 2.101434\n",
      "iteration 1200 / 1500: loss 2.099253\n",
      "iteration 1300 / 1500: loss 2.058956\n",
      "iteration 1400 / 1500: loss 2.102570\n",
      "iteration 0 / 1500: loss 1194.244369\n",
      "iteration 100 / 1500: loss 53.639694\n",
      "iteration 200 / 1500: loss 4.363955\n",
      "iteration 300 / 1500: loss 2.172319\n",
      "iteration 400 / 1500: loss 2.090118\n",
      "iteration 500 / 1500: loss 2.065764\n",
      "iteration 600 / 1500: loss 2.147401\n",
      "iteration 700 / 1500: loss 2.070964\n",
      "iteration 800 / 1500: loss 2.097772\n",
      "iteration 900 / 1500: loss 2.137820\n",
      "iteration 1000 / 1500: loss 2.123711\n",
      "iteration 1100 / 1500: loss 2.077580\n",
      "iteration 1200 / 1500: loss 2.087756\n",
      "iteration 1300 / 1500: loss 2.095666\n",
      "iteration 1400 / 1500: loss 2.060478\n",
      "iteration 0 / 1500: loss 1231.960647\n",
      "iteration 100 / 1500: loss 51.030432\n",
      "iteration 200 / 1500: loss 4.032706\n",
      "iteration 300 / 1500: loss 2.185572\n",
      "iteration 400 / 1500: loss 2.103981\n",
      "iteration 500 / 1500: loss 2.113737\n",
      "iteration 600 / 1500: loss 2.108624\n",
      "iteration 700 / 1500: loss 2.126887\n",
      "iteration 800 / 1500: loss 2.135748\n",
      "iteration 900 / 1500: loss 2.102708\n",
      "iteration 1000 / 1500: loss 2.026104\n",
      "iteration 1100 / 1500: loss 2.126588\n",
      "iteration 1200 / 1500: loss 2.118354\n",
      "iteration 1300 / 1500: loss 2.152160\n",
      "iteration 1400 / 1500: loss 2.108735\n",
      "iteration 0 / 1500: loss 1258.296501\n",
      "iteration 100 / 1500: loss 48.167638\n",
      "iteration 200 / 1500: loss 3.761866\n",
      "iteration 300 / 1500: loss 2.118902\n",
      "iteration 400 / 1500: loss 2.089823\n",
      "iteration 500 / 1500: loss 2.158134\n",
      "iteration 600 / 1500: loss 2.148601\n",
      "iteration 700 / 1500: loss 2.088073\n",
      "iteration 800 / 1500: loss 2.109936\n",
      "iteration 900 / 1500: loss 2.100992\n",
      "iteration 1000 / 1500: loss 2.077024\n",
      "iteration 1100 / 1500: loss 2.090600\n",
      "iteration 1200 / 1500: loss 2.130590\n",
      "iteration 1300 / 1500: loss 2.089622\n",
      "iteration 1400 / 1500: loss 2.080419\n",
      "iteration 0 / 1500: loss 1290.582004\n",
      "iteration 100 / 1500: loss 45.648812\n",
      "iteration 200 / 1500: loss 3.587838\n",
      "iteration 300 / 1500: loss 2.146984\n",
      "iteration 400 / 1500: loss 2.123778\n",
      "iteration 500 / 1500: loss 2.079081\n",
      "iteration 600 / 1500: loss 2.080874\n",
      "iteration 700 / 1500: loss 2.051230\n",
      "iteration 800 / 1500: loss 2.055084\n",
      "iteration 900 / 1500: loss 2.080037\n",
      "iteration 1000 / 1500: loss 2.030713\n",
      "iteration 1100 / 1500: loss 2.135861\n",
      "iteration 1200 / 1500: loss 2.120007\n",
      "iteration 1300 / 1500: loss 2.083558\n",
      "iteration 1400 / 1500: loss 2.135932\n",
      "iteration 0 / 1500: loss 1304.625742\n",
      "iteration 100 / 1500: loss 42.783027\n",
      "iteration 200 / 1500: loss 3.377694\n",
      "iteration 300 / 1500: loss 2.182840\n",
      "iteration 400 / 1500: loss 2.058971\n",
      "iteration 500 / 1500: loss 2.113648\n",
      "iteration 600 / 1500: loss 2.150657\n",
      "iteration 700 / 1500: loss 2.157069\n",
      "iteration 800 / 1500: loss 2.094945\n",
      "iteration 900 / 1500: loss 2.162784\n",
      "iteration 1000 / 1500: loss 2.110515\n",
      "iteration 1100 / 1500: loss 2.114729\n",
      "iteration 1200 / 1500: loss 2.102238\n",
      "iteration 1300 / 1500: loss 2.128763\n",
      "iteration 1400 / 1500: loss 2.123625\n",
      "iteration 0 / 1500: loss 1362.939656\n",
      "iteration 100 / 1500: loss 41.193854\n",
      "iteration 200 / 1500: loss 3.270830\n",
      "iteration 300 / 1500: loss 2.156017\n",
      "iteration 400 / 1500: loss 2.138897\n",
      "iteration 500 / 1500: loss 2.036075\n",
      "iteration 600 / 1500: loss 2.130156\n",
      "iteration 700 / 1500: loss 2.124877\n",
      "iteration 800 / 1500: loss 2.081920\n",
      "iteration 900 / 1500: loss 2.086657\n",
      "iteration 1000 / 1500: loss 2.105603\n",
      "iteration 1100 / 1500: loss 2.058428\n",
      "iteration 1200 / 1500: loss 2.091727\n",
      "iteration 1300 / 1500: loss 2.101448\n",
      "iteration 1400 / 1500: loss 2.109182\n",
      "iteration 0 / 1500: loss 1387.455933\n",
      "iteration 100 / 1500: loss 38.815406\n",
      "iteration 200 / 1500: loss 3.123959\n",
      "iteration 300 / 1500: loss 2.133486\n",
      "iteration 400 / 1500: loss 2.138362\n",
      "iteration 500 / 1500: loss 2.113388\n",
      "iteration 600 / 1500: loss 2.085195\n",
      "iteration 700 / 1500: loss 2.089153\n",
      "iteration 800 / 1500: loss 2.129656\n",
      "iteration 900 / 1500: loss 2.143239\n",
      "iteration 1000 / 1500: loss 2.134886\n",
      "iteration 1100 / 1500: loss 2.093351\n",
      "iteration 1200 / 1500: loss 2.066497\n",
      "iteration 1300 / 1500: loss 2.141962\n",
      "iteration 1400 / 1500: loss 2.122463\n",
      "iteration 0 / 1500: loss 1422.730762\n",
      "iteration 100 / 1500: loss 36.746378\n",
      "iteration 200 / 1500: loss 2.968483\n",
      "iteration 300 / 1500: loss 2.171664\n",
      "iteration 400 / 1500: loss 2.153608\n",
      "iteration 500 / 1500: loss 2.160338\n",
      "iteration 600 / 1500: loss 2.074232\n",
      "iteration 700 / 1500: loss 2.128857\n",
      "iteration 800 / 1500: loss 2.161704\n",
      "iteration 900 / 1500: loss 2.072660\n",
      "iteration 1000 / 1500: loss 2.104638\n",
      "iteration 1100 / 1500: loss 2.073939\n",
      "iteration 1200 / 1500: loss 2.099652\n",
      "iteration 1300 / 1500: loss 2.098170\n",
      "iteration 1400 / 1500: loss 2.147746\n",
      "iteration 0 / 1500: loss 1441.616144\n",
      "iteration 100 / 1500: loss 34.452595\n",
      "iteration 200 / 1500: loss 2.870824\n",
      "iteration 300 / 1500: loss 2.095915\n",
      "iteration 400 / 1500: loss 2.093455\n",
      "iteration 500 / 1500: loss 2.146744\n",
      "iteration 600 / 1500: loss 2.125548\n",
      "iteration 700 / 1500: loss 2.137933\n",
      "iteration 800 / 1500: loss 2.097947\n",
      "iteration 900 / 1500: loss 2.155607\n",
      "iteration 1000 / 1500: loss 2.140254\n",
      "iteration 1100 / 1500: loss 2.107969\n",
      "iteration 1200 / 1500: loss 2.086415\n",
      "iteration 1300 / 1500: loss 2.125674\n",
      "iteration 1400 / 1500: loss 2.106705\n",
      "iteration 0 / 1500: loss 1490.774945\n",
      "iteration 100 / 1500: loss 33.011492\n",
      "iteration 200 / 1500: loss 2.794709\n",
      "iteration 300 / 1500: loss 2.136582\n",
      "iteration 400 / 1500: loss 2.140078\n",
      "iteration 500 / 1500: loss 2.073235\n",
      "iteration 600 / 1500: loss 2.123140\n",
      "iteration 700 / 1500: loss 2.123002\n",
      "iteration 800 / 1500: loss 2.113691\n",
      "iteration 900 / 1500: loss 2.131042\n",
      "iteration 1000 / 1500: loss 2.107878\n",
      "iteration 1100 / 1500: loss 2.108011\n",
      "iteration 1200 / 1500: loss 2.090018\n",
      "iteration 1300 / 1500: loss 2.128364\n",
      "iteration 1400 / 1500: loss 2.151084\n",
      "iteration 0 / 1500: loss 1521.014693\n",
      "iteration 100 / 1500: loss 31.194371\n",
      "iteration 200 / 1500: loss 2.692710\n",
      "iteration 300 / 1500: loss 2.144273\n",
      "iteration 400 / 1500: loss 2.119295\n",
      "iteration 500 / 1500: loss 2.108369\n",
      "iteration 600 / 1500: loss 2.128784\n",
      "iteration 700 / 1500: loss 2.074080\n",
      "iteration 800 / 1500: loss 2.105724\n",
      "iteration 900 / 1500: loss 2.099734\n",
      "iteration 1000 / 1500: loss 2.107574\n",
      "iteration 1100 / 1500: loss 2.160231\n",
      "iteration 1200 / 1500: loss 2.121424\n",
      "iteration 1300 / 1500: loss 2.112515\n",
      "iteration 1400 / 1500: loss 2.143971\n",
      "iteration 0 / 1500: loss 1552.867147\n",
      "iteration 100 / 1500: loss 29.466061\n",
      "iteration 200 / 1500: loss 2.563716\n",
      "iteration 300 / 1500: loss 2.120244\n",
      "iteration 400 / 1500: loss 2.101963\n",
      "iteration 500 / 1500: loss 2.145359\n",
      "iteration 600 / 1500: loss 2.099295\n",
      "iteration 700 / 1500: loss 2.199449\n",
      "iteration 800 / 1500: loss 2.099514\n",
      "iteration 900 / 1500: loss 2.080355\n",
      "iteration 1000 / 1500: loss 2.143416\n",
      "iteration 1100 / 1500: loss 2.120760\n",
      "iteration 1200 / 1500: loss 2.111121\n",
      "iteration 1300 / 1500: loss 2.095844\n",
      "iteration 1400 / 1500: loss 2.056685\n",
      "iteration 0 / 1500: loss 782.264670\n",
      "iteration 100 / 1500: loss 63.760046\n",
      "iteration 200 / 1500: loss 6.985903\n",
      "iteration 300 / 1500: loss 2.473530\n",
      "iteration 400 / 1500: loss 2.082357\n",
      "iteration 500 / 1500: loss 2.030744\n",
      "iteration 600 / 1500: loss 2.056539\n",
      "iteration 700 / 1500: loss 2.083560\n",
      "iteration 800 / 1500: loss 2.035832\n",
      "iteration 900 / 1500: loss 2.090162\n",
      "iteration 1000 / 1500: loss 2.143278\n",
      "iteration 1100 / 1500: loss 1.997961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 2.036578\n",
      "iteration 1300 / 1500: loss 2.036731\n",
      "iteration 1400 / 1500: loss 2.151133\n",
      "iteration 0 / 1500: loss 805.903054\n",
      "iteration 100 / 1500: loss 59.468471\n",
      "iteration 200 / 1500: loss 6.191677\n",
      "iteration 300 / 1500: loss 2.386118\n",
      "iteration 400 / 1500: loss 2.096513\n",
      "iteration 500 / 1500: loss 2.089794\n",
      "iteration 600 / 1500: loss 2.109536\n",
      "iteration 700 / 1500: loss 2.068816\n",
      "iteration 800 / 1500: loss 2.089666\n",
      "iteration 900 / 1500: loss 2.131082\n",
      "iteration 1000 / 1500: loss 2.034620\n",
      "iteration 1100 / 1500: loss 2.064547\n",
      "iteration 1200 / 1500: loss 2.079916\n",
      "iteration 1300 / 1500: loss 2.031521\n",
      "iteration 1400 / 1500: loss 2.010606\n",
      "iteration 0 / 1500: loss 822.739895\n",
      "iteration 100 / 1500: loss 55.086596\n",
      "iteration 200 / 1500: loss 5.484954\n",
      "iteration 300 / 1500: loss 2.281119\n",
      "iteration 400 / 1500: loss 2.053783\n",
      "iteration 500 / 1500: loss 2.087239\n",
      "iteration 600 / 1500: loss 2.024291\n",
      "iteration 700 / 1500: loss 2.016821\n",
      "iteration 800 / 1500: loss 2.060021\n",
      "iteration 900 / 1500: loss 2.032433\n",
      "iteration 1000 / 1500: loss 2.072187\n",
      "iteration 1100 / 1500: loss 2.073939\n",
      "iteration 1200 / 1500: loss 2.041971\n",
      "iteration 1300 / 1500: loss 2.042656\n",
      "iteration 1400 / 1500: loss 2.066321\n",
      "iteration 0 / 1500: loss 866.042175\n",
      "iteration 100 / 1500: loss 52.434088\n",
      "iteration 200 / 1500: loss 4.963032\n",
      "iteration 300 / 1500: loss 2.251578\n",
      "iteration 400 / 1500: loss 2.070097\n",
      "iteration 500 / 1500: loss 2.146605\n",
      "iteration 600 / 1500: loss 2.115892\n",
      "iteration 700 / 1500: loss 2.143936\n",
      "iteration 800 / 1500: loss 2.110559\n",
      "iteration 900 / 1500: loss 2.071881\n",
      "iteration 1000 / 1500: loss 2.005097\n",
      "iteration 1100 / 1500: loss 2.046002\n",
      "iteration 1200 / 1500: loss 2.016135\n",
      "iteration 1300 / 1500: loss 2.045054\n",
      "iteration 1400 / 1500: loss 2.070339\n",
      "iteration 0 / 1500: loss 883.295161\n",
      "iteration 100 / 1500: loss 48.377506\n",
      "iteration 200 / 1500: loss 4.484851\n",
      "iteration 300 / 1500: loss 2.181577\n",
      "iteration 400 / 1500: loss 2.043667\n",
      "iteration 500 / 1500: loss 2.100593\n",
      "iteration 600 / 1500: loss 2.102122\n",
      "iteration 700 / 1500: loss 2.157608\n",
      "iteration 800 / 1500: loss 2.112603\n",
      "iteration 900 / 1500: loss 2.069753\n",
      "iteration 1000 / 1500: loss 2.089293\n",
      "iteration 1100 / 1500: loss 2.110203\n",
      "iteration 1200 / 1500: loss 2.098460\n",
      "iteration 1300 / 1500: loss 2.029182\n",
      "iteration 1400 / 1500: loss 2.094331\n",
      "iteration 0 / 1500: loss 927.755868\n",
      "iteration 100 / 1500: loss 46.128359\n",
      "iteration 200 / 1500: loss 4.185896\n",
      "iteration 300 / 1500: loss 2.232891\n",
      "iteration 400 / 1500: loss 2.136625\n",
      "iteration 500 / 1500: loss 2.068809\n",
      "iteration 600 / 1500: loss 2.150629\n",
      "iteration 700 / 1500: loss 2.029566\n",
      "iteration 800 / 1500: loss 2.037411\n",
      "iteration 900 / 1500: loss 2.110828\n",
      "iteration 1000 / 1500: loss 2.109794\n",
      "iteration 1100 / 1500: loss 2.096858\n",
      "iteration 1200 / 1500: loss 2.114301\n",
      "iteration 1300 / 1500: loss 2.067098\n",
      "iteration 1400 / 1500: loss 2.077146\n",
      "iteration 0 / 1500: loss 965.895427\n",
      "iteration 100 / 1500: loss 43.542793\n",
      "iteration 200 / 1500: loss 3.889098\n",
      "iteration 300 / 1500: loss 2.143391\n",
      "iteration 400 / 1500: loss 2.078021\n",
      "iteration 500 / 1500: loss 2.105851\n",
      "iteration 600 / 1500: loss 2.049000\n",
      "iteration 700 / 1500: loss 2.053357\n",
      "iteration 800 / 1500: loss 2.066699\n",
      "iteration 900 / 1500: loss 2.095970\n",
      "iteration 1000 / 1500: loss 2.057141\n",
      "iteration 1100 / 1500: loss 2.088131\n",
      "iteration 1200 / 1500: loss 2.004122\n",
      "iteration 1300 / 1500: loss 2.006279\n",
      "iteration 1400 / 1500: loss 2.029988\n",
      "iteration 0 / 1500: loss 986.929370\n",
      "iteration 100 / 1500: loss 40.250845\n",
      "iteration 200 / 1500: loss 3.550177\n",
      "iteration 300 / 1500: loss 2.158132\n",
      "iteration 400 / 1500: loss 2.061177\n",
      "iteration 500 / 1500: loss 2.091146\n",
      "iteration 600 / 1500: loss 2.078803\n",
      "iteration 700 / 1500: loss 2.043514\n",
      "iteration 800 / 1500: loss 2.134605\n",
      "iteration 900 / 1500: loss 2.053630\n",
      "iteration 1000 / 1500: loss 2.104494\n",
      "iteration 1100 / 1500: loss 2.141500\n",
      "iteration 1200 / 1500: loss 2.087255\n",
      "iteration 1300 / 1500: loss 2.089922\n",
      "iteration 1400 / 1500: loss 2.133716\n",
      "iteration 0 / 1500: loss 1026.942201\n",
      "iteration 100 / 1500: loss 37.954278\n",
      "iteration 200 / 1500: loss 3.340397\n",
      "iteration 300 / 1500: loss 2.077945\n",
      "iteration 400 / 1500: loss 2.080727\n",
      "iteration 500 / 1500: loss 2.027971\n",
      "iteration 600 / 1500: loss 2.077326\n",
      "iteration 700 / 1500: loss 2.124459\n",
      "iteration 800 / 1500: loss 2.113671\n",
      "iteration 900 / 1500: loss 2.080489\n",
      "iteration 1000 / 1500: loss 2.100073\n",
      "iteration 1100 / 1500: loss 2.042241\n",
      "iteration 1200 / 1500: loss 2.008505\n",
      "iteration 1300 / 1500: loss 2.067935\n",
      "iteration 1400 / 1500: loss 2.086252\n",
      "iteration 0 / 1500: loss 1055.396124\n",
      "iteration 100 / 1500: loss 35.402192\n",
      "iteration 200 / 1500: loss 3.123896\n",
      "iteration 300 / 1500: loss 2.170062\n",
      "iteration 400 / 1500: loss 2.130290\n",
      "iteration 500 / 1500: loss 2.086292\n",
      "iteration 600 / 1500: loss 2.119089\n",
      "iteration 700 / 1500: loss 2.067072\n",
      "iteration 800 / 1500: loss 2.102245\n",
      "iteration 900 / 1500: loss 2.056709\n",
      "iteration 1000 / 1500: loss 2.104166\n",
      "iteration 1100 / 1500: loss 2.103084\n",
      "iteration 1200 / 1500: loss 2.109727\n",
      "iteration 1300 / 1500: loss 2.108989\n",
      "iteration 1400 / 1500: loss 2.058724\n",
      "iteration 0 / 1500: loss 1076.539569\n",
      "iteration 100 / 1500: loss 32.752751\n",
      "iteration 200 / 1500: loss 3.019138\n",
      "iteration 300 / 1500: loss 2.133784\n",
      "iteration 400 / 1500: loss 2.043429\n",
      "iteration 500 / 1500: loss 2.168599\n",
      "iteration 600 / 1500: loss 2.127899\n",
      "iteration 700 / 1500: loss 2.117937\n",
      "iteration 800 / 1500: loss 2.107953\n",
      "iteration 900 / 1500: loss 2.117918\n",
      "iteration 1000 / 1500: loss 2.075600\n",
      "iteration 1100 / 1500: loss 2.070561\n",
      "iteration 1200 / 1500: loss 2.085309\n",
      "iteration 1300 / 1500: loss 2.113270\n",
      "iteration 1400 / 1500: loss 2.087271\n",
      "iteration 0 / 1500: loss 1118.866162\n",
      "iteration 100 / 1500: loss 30.814414\n",
      "iteration 200 / 1500: loss 2.828974\n",
      "iteration 300 / 1500: loss 2.088552\n",
      "iteration 400 / 1500: loss 2.033814\n",
      "iteration 500 / 1500: loss 2.052799\n",
      "iteration 600 / 1500: loss 2.148708\n",
      "iteration 700 / 1500: loss 2.119030\n",
      "iteration 800 / 1500: loss 2.099755\n",
      "iteration 900 / 1500: loss 2.116176\n",
      "iteration 1000 / 1500: loss 2.088227\n",
      "iteration 1100 / 1500: loss 2.058285\n",
      "iteration 1200 / 1500: loss 2.074904\n",
      "iteration 1300 / 1500: loss 2.090196\n",
      "iteration 1400 / 1500: loss 2.120113\n",
      "iteration 0 / 1500: loss 1142.720591\n",
      "iteration 100 / 1500: loss 28.598324\n",
      "iteration 200 / 1500: loss 2.714422\n",
      "iteration 300 / 1500: loss 2.123532\n",
      "iteration 400 / 1500: loss 2.110212\n",
      "iteration 500 / 1500: loss 2.082202\n",
      "iteration 600 / 1500: loss 2.081843\n",
      "iteration 700 / 1500: loss 2.069252\n",
      "iteration 800 / 1500: loss 2.098470\n",
      "iteration 900 / 1500: loss 2.180809\n",
      "iteration 1000 / 1500: loss 2.079717\n",
      "iteration 1100 / 1500: loss 2.093624\n",
      "iteration 1200 / 1500: loss 2.104688\n",
      "iteration 1300 / 1500: loss 2.116189\n",
      "iteration 1400 / 1500: loss 2.064995\n",
      "iteration 0 / 1500: loss 1162.189998\n",
      "iteration 100 / 1500: loss 26.411085\n",
      "iteration 200 / 1500: loss 2.592286\n",
      "iteration 300 / 1500: loss 2.078903\n",
      "iteration 400 / 1500: loss 2.084158\n",
      "iteration 500 / 1500: loss 2.138269\n",
      "iteration 600 / 1500: loss 2.102271\n",
      "iteration 700 / 1500: loss 2.114013\n",
      "iteration 800 / 1500: loss 2.112916\n",
      "iteration 900 / 1500: loss 2.058549\n",
      "iteration 1000 / 1500: loss 2.089794\n",
      "iteration 1100 / 1500: loss 2.087807\n",
      "iteration 1200 / 1500: loss 2.064188\n",
      "iteration 1300 / 1500: loss 2.023982\n",
      "iteration 1400 / 1500: loss 2.053951\n",
      "iteration 0 / 1500: loss 1192.614661\n",
      "iteration 100 / 1500: loss 24.627703\n",
      "iteration 200 / 1500: loss 2.587509\n",
      "iteration 300 / 1500: loss 2.048826\n",
      "iteration 400 / 1500: loss 2.152817\n",
      "iteration 500 / 1500: loss 2.079744\n",
      "iteration 600 / 1500: loss 2.133376\n",
      "iteration 700 / 1500: loss 2.083317\n",
      "iteration 800 / 1500: loss 2.090033\n",
      "iteration 900 / 1500: loss 2.119352\n",
      "iteration 1000 / 1500: loss 2.097282\n",
      "iteration 1100 / 1500: loss 2.121388\n",
      "iteration 1200 / 1500: loss 2.104545\n",
      "iteration 1300 / 1500: loss 2.071350\n",
      "iteration 1400 / 1500: loss 2.089120\n",
      "iteration 0 / 1500: loss 1247.123950\n",
      "iteration 100 / 1500: loss 23.393577\n",
      "iteration 200 / 1500: loss 2.513081\n",
      "iteration 300 / 1500: loss 2.073990\n",
      "iteration 400 / 1500: loss 2.097661\n",
      "iteration 500 / 1500: loss 2.098701\n",
      "iteration 600 / 1500: loss 2.084894\n",
      "iteration 700 / 1500: loss 2.025750\n",
      "iteration 800 / 1500: loss 2.086730\n",
      "iteration 900 / 1500: loss 2.143098\n",
      "iteration 1000 / 1500: loss 2.110879\n",
      "iteration 1100 / 1500: loss 2.127991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 2.065163\n",
      "iteration 1300 / 1500: loss 2.120802\n",
      "iteration 1400 / 1500: loss 2.111242\n",
      "iteration 0 / 1500: loss 1269.926567\n",
      "iteration 100 / 1500: loss 21.712069\n",
      "iteration 200 / 1500: loss 2.371778\n",
      "iteration 300 / 1500: loss 2.136528\n",
      "iteration 400 / 1500: loss 2.077904\n",
      "iteration 500 / 1500: loss 2.146452\n",
      "iteration 600 / 1500: loss 2.119676\n",
      "iteration 700 / 1500: loss 2.076128\n",
      "iteration 800 / 1500: loss 2.116601\n",
      "iteration 900 / 1500: loss 2.075926\n",
      "iteration 1000 / 1500: loss 2.141441\n",
      "iteration 1100 / 1500: loss 2.081298\n",
      "iteration 1200 / 1500: loss 2.107845\n",
      "iteration 1300 / 1500: loss 2.098203\n",
      "iteration 1400 / 1500: loss 2.146820\n",
      "iteration 0 / 1500: loss 1311.092849\n",
      "iteration 100 / 1500: loss 20.356823\n",
      "iteration 200 / 1500: loss 2.377660\n",
      "iteration 300 / 1500: loss 2.069841\n",
      "iteration 400 / 1500: loss 2.079651\n",
      "iteration 500 / 1500: loss 2.114103\n",
      "iteration 600 / 1500: loss 2.064829\n",
      "iteration 700 / 1500: loss 2.117375\n",
      "iteration 800 / 1500: loss 2.069793\n",
      "iteration 900 / 1500: loss 2.135817\n",
      "iteration 1000 / 1500: loss 2.118198\n",
      "iteration 1100 / 1500: loss 2.091728\n",
      "iteration 1200 / 1500: loss 2.077900\n",
      "iteration 1300 / 1500: loss 2.151241\n",
      "iteration 1400 / 1500: loss 2.086107\n",
      "iteration 0 / 1500: loss 1312.263833\n",
      "iteration 100 / 1500: loss 18.541350\n",
      "iteration 200 / 1500: loss 2.325382\n",
      "iteration 300 / 1500: loss 2.101813\n",
      "iteration 400 / 1500: loss 2.099215\n",
      "iteration 500 / 1500: loss 2.137040\n",
      "iteration 600 / 1500: loss 2.147132\n",
      "iteration 700 / 1500: loss 2.153274\n",
      "iteration 800 / 1500: loss 2.083516\n",
      "iteration 900 / 1500: loss 2.101122\n",
      "iteration 1000 / 1500: loss 2.152162\n",
      "iteration 1100 / 1500: loss 2.067961\n",
      "iteration 1200 / 1500: loss 2.109095\n",
      "iteration 1300 / 1500: loss 2.103315\n",
      "iteration 1400 / 1500: loss 2.089154\n",
      "iteration 0 / 1500: loss 1363.483623\n",
      "iteration 100 / 1500: loss 17.539468\n",
      "iteration 200 / 1500: loss 2.277664\n",
      "iteration 300 / 1500: loss 2.099018\n",
      "iteration 400 / 1500: loss 2.126301\n",
      "iteration 500 / 1500: loss 2.090897\n",
      "iteration 600 / 1500: loss 2.107106\n",
      "iteration 700 / 1500: loss 2.106814\n",
      "iteration 800 / 1500: loss 2.146364\n",
      "iteration 900 / 1500: loss 2.155683\n",
      "iteration 1000 / 1500: loss 2.107232\n",
      "iteration 1100 / 1500: loss 2.080673\n",
      "iteration 1200 / 1500: loss 2.077898\n",
      "iteration 1300 / 1500: loss 2.121239\n",
      "iteration 1400 / 1500: loss 2.106896\n",
      "iteration 0 / 1500: loss 1372.923688\n",
      "iteration 100 / 1500: loss 16.147060\n",
      "iteration 200 / 1500: loss 2.221797\n",
      "iteration 300 / 1500: loss 2.143201\n",
      "iteration 400 / 1500: loss 2.102119\n",
      "iteration 500 / 1500: loss 2.134067\n",
      "iteration 600 / 1500: loss 2.103788\n",
      "iteration 700 / 1500: loss 2.139494\n",
      "iteration 800 / 1500: loss 2.108114\n",
      "iteration 900 / 1500: loss 2.090020\n",
      "iteration 1000 / 1500: loss 2.113075\n",
      "iteration 1100 / 1500: loss 2.129621\n",
      "iteration 1200 / 1500: loss 2.131648\n",
      "iteration 1300 / 1500: loss 2.166972\n",
      "iteration 1400 / 1500: loss 2.132692\n",
      "iteration 0 / 1500: loss 1420.039124\n",
      "iteration 100 / 1500: loss 15.238661\n",
      "iteration 200 / 1500: loss 2.259928\n",
      "iteration 300 / 1500: loss 2.084002\n",
      "iteration 400 / 1500: loss 2.135151\n",
      "iteration 500 / 1500: loss 2.127124\n",
      "iteration 600 / 1500: loss 2.072584\n",
      "iteration 700 / 1500: loss 2.134543\n",
      "iteration 800 / 1500: loss 2.123042\n",
      "iteration 900 / 1500: loss 2.166332\n",
      "iteration 1000 / 1500: loss 2.084026\n",
      "iteration 1100 / 1500: loss 2.144604\n",
      "iteration 1200 / 1500: loss 2.149489\n",
      "iteration 1300 / 1500: loss 2.138611\n",
      "iteration 1400 / 1500: loss 2.070365\n",
      "iteration 0 / 1500: loss 1469.161319\n",
      "iteration 100 / 1500: loss 14.390997\n",
      "iteration 200 / 1500: loss 2.161245\n",
      "iteration 300 / 1500: loss 2.111519\n",
      "iteration 400 / 1500: loss 2.105421\n",
      "iteration 500 / 1500: loss 2.135888\n",
      "iteration 600 / 1500: loss 2.121120\n",
      "iteration 700 / 1500: loss 2.165369\n",
      "iteration 800 / 1500: loss 2.160992\n",
      "iteration 900 / 1500: loss 2.090494\n",
      "iteration 1000 / 1500: loss 2.130570\n",
      "iteration 1100 / 1500: loss 2.101199\n",
      "iteration 1200 / 1500: loss 2.145780\n",
      "iteration 1300 / 1500: loss 2.080268\n",
      "iteration 1400 / 1500: loss 2.098861\n",
      "iteration 0 / 1500: loss 1467.739430\n",
      "iteration 100 / 1500: loss 13.129275\n",
      "iteration 200 / 1500: loss 2.170421\n",
      "iteration 300 / 1500: loss 2.067784\n",
      "iteration 400 / 1500: loss 2.103220\n",
      "iteration 500 / 1500: loss 2.108075\n",
      "iteration 600 / 1500: loss 2.102438\n",
      "iteration 700 / 1500: loss 2.115180\n",
      "iteration 800 / 1500: loss 2.170779\n",
      "iteration 900 / 1500: loss 2.072695\n",
      "iteration 1000 / 1500: loss 2.110302\n",
      "iteration 1100 / 1500: loss 2.110741\n",
      "iteration 1200 / 1500: loss 2.125521\n",
      "iteration 1300 / 1500: loss 2.098040\n",
      "iteration 1400 / 1500: loss 2.117590\n",
      "iteration 0 / 1500: loss 1512.707920\n",
      "iteration 100 / 1500: loss 12.372891\n",
      "iteration 200 / 1500: loss 2.180148\n",
      "iteration 300 / 1500: loss 2.146097\n",
      "iteration 400 / 1500: loss 2.135409\n",
      "iteration 500 / 1500: loss 2.119249\n",
      "iteration 600 / 1500: loss 2.134322\n",
      "iteration 700 / 1500: loss 2.112595\n",
      "iteration 800 / 1500: loss 2.096758\n",
      "iteration 900 / 1500: loss 2.114499\n",
      "iteration 1000 / 1500: loss 2.126623\n",
      "iteration 1100 / 1500: loss 2.088002\n",
      "iteration 1200 / 1500: loss 2.091363\n",
      "iteration 1300 / 1500: loss 2.075811\n",
      "iteration 1400 / 1500: loss 2.114545\n",
      "iteration 0 / 1500: loss 1543.834603\n",
      "iteration 100 / 1500: loss 11.582893\n",
      "iteration 200 / 1500: loss 2.151911\n",
      "iteration 300 / 1500: loss 2.144391\n",
      "iteration 400 / 1500: loss 2.074769\n",
      "iteration 500 / 1500: loss 2.097583\n",
      "iteration 600 / 1500: loss 2.131126\n",
      "iteration 700 / 1500: loss 2.140388\n",
      "iteration 800 / 1500: loss 2.146776\n",
      "iteration 900 / 1500: loss 2.116270\n",
      "iteration 1000 / 1500: loss 2.141645\n",
      "iteration 1100 / 1500: loss 2.079576\n",
      "iteration 1200 / 1500: loss 2.100020\n",
      "iteration 1300 / 1500: loss 2.087362\n",
      "iteration 1400 / 1500: loss 2.136306\n",
      "iteration 0 / 1500: loss 768.970039\n",
      "iteration 100 / 1500: loss 33.341937\n",
      "iteration 200 / 1500: loss 3.373586\n",
      "iteration 300 / 1500: loss 2.127886\n",
      "iteration 400 / 1500: loss 2.059440\n",
      "iteration 500 / 1500: loss 2.031107\n",
      "iteration 600 / 1500: loss 2.000894\n",
      "iteration 700 / 1500: loss 2.080930\n",
      "iteration 800 / 1500: loss 2.102273\n",
      "iteration 900 / 1500: loss 2.056710\n",
      "iteration 1000 / 1500: loss 2.028536\n",
      "iteration 1100 / 1500: loss 2.024159\n",
      "iteration 1200 / 1500: loss 2.040772\n",
      "iteration 1300 / 1500: loss 1.991723\n",
      "iteration 1400 / 1500: loss 2.070923\n",
      "iteration 0 / 1500: loss 806.482752\n",
      "iteration 100 / 1500: loss 30.984667\n",
      "iteration 200 / 1500: loss 3.087514\n",
      "iteration 300 / 1500: loss 2.158398\n",
      "iteration 400 / 1500: loss 1.997829\n",
      "iteration 500 / 1500: loss 2.063603\n",
      "iteration 600 / 1500: loss 2.046761\n",
      "iteration 700 / 1500: loss 1.978532\n",
      "iteration 800 / 1500: loss 2.064314\n",
      "iteration 900 / 1500: loss 2.081468\n",
      "iteration 1000 / 1500: loss 2.067602\n",
      "iteration 1100 / 1500: loss 2.016762\n",
      "iteration 1200 / 1500: loss 2.073963\n",
      "iteration 1300 / 1500: loss 2.030117\n",
      "iteration 1400 / 1500: loss 2.102163\n",
      "iteration 0 / 1500: loss 838.835233\n",
      "iteration 100 / 1500: loss 28.505516\n",
      "iteration 200 / 1500: loss 2.858346\n",
      "iteration 300 / 1500: loss 2.064460\n",
      "iteration 400 / 1500: loss 2.043618\n",
      "iteration 500 / 1500: loss 2.081339\n",
      "iteration 600 / 1500: loss 2.101826\n",
      "iteration 700 / 1500: loss 2.081053\n",
      "iteration 800 / 1500: loss 2.057448\n",
      "iteration 900 / 1500: loss 2.025537\n",
      "iteration 1000 / 1500: loss 2.002006\n",
      "iteration 1100 / 1500: loss 2.013872\n",
      "iteration 1200 / 1500: loss 2.048560\n",
      "iteration 1300 / 1500: loss 2.040428\n",
      "iteration 1400 / 1500: loss 2.057136\n",
      "iteration 0 / 1500: loss 868.836003\n",
      "iteration 100 / 1500: loss 26.204377\n",
      "iteration 200 / 1500: loss 2.686914\n",
      "iteration 300 / 1500: loss 2.067545\n",
      "iteration 400 / 1500: loss 2.099347\n",
      "iteration 500 / 1500: loss 2.047558\n",
      "iteration 600 / 1500: loss 2.097419\n",
      "iteration 700 / 1500: loss 2.041465\n",
      "iteration 800 / 1500: loss 2.093281\n",
      "iteration 900 / 1500: loss 2.106923\n",
      "iteration 1000 / 1500: loss 2.005517\n",
      "iteration 1100 / 1500: loss 2.042186\n",
      "iteration 1200 / 1500: loss 2.049349\n",
      "iteration 1300 / 1500: loss 2.013925\n",
      "iteration 1400 / 1500: loss 2.052413\n",
      "iteration 0 / 1500: loss 899.879884\n",
      "iteration 100 / 1500: loss 23.996115\n",
      "iteration 200 / 1500: loss 2.505324\n",
      "iteration 300 / 1500: loss 2.079212\n",
      "iteration 400 / 1500: loss 2.093492\n",
      "iteration 500 / 1500: loss 2.091806\n",
      "iteration 600 / 1500: loss 2.102696\n",
      "iteration 700 / 1500: loss 2.083042\n",
      "iteration 800 / 1500: loss 2.063574\n",
      "iteration 900 / 1500: loss 2.048484\n",
      "iteration 1000 / 1500: loss 2.091070\n",
      "iteration 1100 / 1500: loss 2.108406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 2.060010\n",
      "iteration 1300 / 1500: loss 2.108879\n",
      "iteration 1400 / 1500: loss 2.094943\n",
      "iteration 0 / 1500: loss 935.642213\n",
      "iteration 100 / 1500: loss 22.069621\n",
      "iteration 200 / 1500: loss 2.515589\n",
      "iteration 300 / 1500: loss 2.079520\n",
      "iteration 400 / 1500: loss 2.056485\n",
      "iteration 500 / 1500: loss 2.075432\n",
      "iteration 600 / 1500: loss 2.115445\n",
      "iteration 700 / 1500: loss 2.069826\n",
      "iteration 800 / 1500: loss 2.070362\n",
      "iteration 900 / 1500: loss 2.094803\n",
      "iteration 1000 / 1500: loss 2.164627\n",
      "iteration 1100 / 1500: loss 2.079404\n",
      "iteration 1200 / 1500: loss 2.093250\n",
      "iteration 1300 / 1500: loss 2.038665\n",
      "iteration 1400 / 1500: loss 2.010446\n",
      "iteration 0 / 1500: loss 963.723385\n",
      "iteration 100 / 1500: loss 20.283423\n",
      "iteration 200 / 1500: loss 2.403081\n",
      "iteration 300 / 1500: loss 2.022199\n",
      "iteration 400 / 1500: loss 2.091684\n",
      "iteration 500 / 1500: loss 2.061156\n",
      "iteration 600 / 1500: loss 2.014244\n",
      "iteration 700 / 1500: loss 2.096999\n",
      "iteration 800 / 1500: loss 2.082036\n",
      "iteration 900 / 1500: loss 2.092964\n",
      "iteration 1000 / 1500: loss 2.025120\n",
      "iteration 1100 / 1500: loss 2.128729\n",
      "iteration 1200 / 1500: loss 2.097762\n",
      "iteration 1300 / 1500: loss 2.106667\n",
      "iteration 1400 / 1500: loss 2.088909\n",
      "iteration 0 / 1500: loss 979.154887\n",
      "iteration 100 / 1500: loss 18.296479\n",
      "iteration 200 / 1500: loss 2.283318\n",
      "iteration 300 / 1500: loss 2.078779\n",
      "iteration 400 / 1500: loss 2.081248\n",
      "iteration 500 / 1500: loss 2.073242\n",
      "iteration 600 / 1500: loss 2.136367\n",
      "iteration 700 / 1500: loss 2.081860\n",
      "iteration 800 / 1500: loss 2.114819\n",
      "iteration 900 / 1500: loss 2.129207\n",
      "iteration 1000 / 1500: loss 2.040605\n",
      "iteration 1100 / 1500: loss 2.091633\n",
      "iteration 1200 / 1500: loss 2.099100\n",
      "iteration 1300 / 1500: loss 2.147181\n",
      "iteration 1400 / 1500: loss 2.113412\n",
      "iteration 0 / 1500: loss 1017.786809\n",
      "iteration 100 / 1500: loss 16.850223\n",
      "iteration 200 / 1500: loss 2.228917\n",
      "iteration 300 / 1500: loss 2.048767\n",
      "iteration 400 / 1500: loss 2.087621\n",
      "iteration 500 / 1500: loss 2.093495\n",
      "iteration 600 / 1500: loss 2.110535\n",
      "iteration 700 / 1500: loss 2.059412\n",
      "iteration 800 / 1500: loss 2.110640\n",
      "iteration 900 / 1500: loss 2.081001\n",
      "iteration 1000 / 1500: loss 2.050649\n",
      "iteration 1100 / 1500: loss 2.104598\n",
      "iteration 1200 / 1500: loss 2.061889\n",
      "iteration 1300 / 1500: loss 2.142462\n",
      "iteration 1400 / 1500: loss 2.086325\n",
      "iteration 0 / 1500: loss 1040.537165\n",
      "iteration 100 / 1500: loss 15.396693\n",
      "iteration 200 / 1500: loss 2.297293\n",
      "iteration 300 / 1500: loss 2.017814\n",
      "iteration 400 / 1500: loss 2.060790\n",
      "iteration 500 / 1500: loss 2.073128\n",
      "iteration 600 / 1500: loss 2.104008\n",
      "iteration 700 / 1500: loss 2.088834\n",
      "iteration 800 / 1500: loss 2.061476\n",
      "iteration 900 / 1500: loss 2.045712\n",
      "iteration 1000 / 1500: loss 2.086920\n",
      "iteration 1100 / 1500: loss 2.087212\n",
      "iteration 1200 / 1500: loss 2.087265\n",
      "iteration 1300 / 1500: loss 2.053574\n",
      "iteration 1400 / 1500: loss 2.046775\n",
      "iteration 0 / 1500: loss 1078.739304\n",
      "iteration 100 / 1500: loss 14.236108\n",
      "iteration 200 / 1500: loss 2.212834\n",
      "iteration 300 / 1500: loss 2.140566\n",
      "iteration 400 / 1500: loss 2.101730\n",
      "iteration 500 / 1500: loss 2.132761\n",
      "iteration 600 / 1500: loss 2.063474\n",
      "iteration 700 / 1500: loss 2.043027\n",
      "iteration 800 / 1500: loss 2.065665\n",
      "iteration 900 / 1500: loss 2.114752\n",
      "iteration 1000 / 1500: loss 2.119149\n",
      "iteration 1100 / 1500: loss 2.090548\n",
      "iteration 1200 / 1500: loss 2.088140\n",
      "iteration 1300 / 1500: loss 2.073994\n",
      "iteration 1400 / 1500: loss 2.105543\n",
      "iteration 0 / 1500: loss 1111.994884\n",
      "iteration 100 / 1500: loss 13.025963\n",
      "iteration 200 / 1500: loss 2.228341\n",
      "iteration 300 / 1500: loss 2.105622\n",
      "iteration 400 / 1500: loss 2.023324\n",
      "iteration 500 / 1500: loss 2.148037\n",
      "iteration 600 / 1500: loss 2.055489\n",
      "iteration 700 / 1500: loss 2.127970\n",
      "iteration 800 / 1500: loss 2.059672\n",
      "iteration 900 / 1500: loss 2.115553\n",
      "iteration 1000 / 1500: loss 2.082918\n",
      "iteration 1100 / 1500: loss 2.052444\n",
      "iteration 1200 / 1500: loss 2.060049\n",
      "iteration 1300 / 1500: loss 2.073303\n",
      "iteration 1400 / 1500: loss 2.096059\n",
      "iteration 0 / 1500: loss 1134.143974\n",
      "iteration 100 / 1500: loss 11.909777\n",
      "iteration 200 / 1500: loss 2.188519\n",
      "iteration 300 / 1500: loss 2.133776\n",
      "iteration 400 / 1500: loss 2.081855\n",
      "iteration 500 / 1500: loss 2.089978\n",
      "iteration 600 / 1500: loss 2.064318\n",
      "iteration 700 / 1500: loss 2.071535\n",
      "iteration 800 / 1500: loss 2.090327\n",
      "iteration 900 / 1500: loss 2.095060\n",
      "iteration 1000 / 1500: loss 2.121634\n",
      "iteration 1100 / 1500: loss 2.104395\n",
      "iteration 1200 / 1500: loss 2.082001\n",
      "iteration 1300 / 1500: loss 2.146017\n",
      "iteration 1400 / 1500: loss 2.085136\n",
      "iteration 0 / 1500: loss 1174.371802\n",
      "iteration 100 / 1500: loss 11.024444\n",
      "iteration 200 / 1500: loss 2.167895\n",
      "iteration 300 / 1500: loss 2.110191\n",
      "iteration 400 / 1500: loss 2.070381\n",
      "iteration 500 / 1500: loss 2.087039\n",
      "iteration 600 / 1500: loss 2.142407\n",
      "iteration 700 / 1500: loss 2.114570\n",
      "iteration 800 / 1500: loss 2.111293\n",
      "iteration 900 / 1500: loss 2.085999\n",
      "iteration 1000 / 1500: loss 2.090822\n",
      "iteration 1100 / 1500: loss 2.098011\n",
      "iteration 1200 / 1500: loss 2.139492\n",
      "iteration 1300 / 1500: loss 2.040722\n",
      "iteration 1400 / 1500: loss 2.087513\n",
      "iteration 0 / 1500: loss 1198.413550\n",
      "iteration 100 / 1500: loss 10.088867\n",
      "iteration 200 / 1500: loss 2.118449\n",
      "iteration 300 / 1500: loss 2.150172\n",
      "iteration 400 / 1500: loss 2.078898\n",
      "iteration 500 / 1500: loss 2.119813\n",
      "iteration 600 / 1500: loss 2.095298\n",
      "iteration 700 / 1500: loss 2.056834\n",
      "iteration 800 / 1500: loss 2.098857\n",
      "iteration 900 / 1500: loss 2.157088\n",
      "iteration 1000 / 1500: loss 2.133373\n",
      "iteration 1100 / 1500: loss 2.100284\n",
      "iteration 1200 / 1500: loss 2.133423\n",
      "iteration 1300 / 1500: loss 2.133472\n",
      "iteration 1400 / 1500: loss 2.066363\n",
      "iteration 0 / 1500: loss 1225.861921\n",
      "iteration 100 / 1500: loss 9.319980\n",
      "iteration 200 / 1500: loss 2.173839\n",
      "iteration 300 / 1500: loss 2.164080\n",
      "iteration 400 / 1500: loss 2.045561\n",
      "iteration 500 / 1500: loss 2.098087\n",
      "iteration 600 / 1500: loss 2.043930\n",
      "iteration 700 / 1500: loss 2.089383\n",
      "iteration 800 / 1500: loss 2.114035\n",
      "iteration 900 / 1500: loss 2.132426\n",
      "iteration 1000 / 1500: loss 2.042239\n",
      "iteration 1100 / 1500: loss 2.116002\n",
      "iteration 1200 / 1500: loss 2.118760\n",
      "iteration 1300 / 1500: loss 2.139118\n",
      "iteration 1400 / 1500: loss 2.070292\n",
      "iteration 0 / 1500: loss 1272.801461\n",
      "iteration 100 / 1500: loss 8.625089\n",
      "iteration 200 / 1500: loss 2.100007\n",
      "iteration 300 / 1500: loss 2.095178\n",
      "iteration 400 / 1500: loss 2.059217\n",
      "iteration 500 / 1500: loss 2.099176\n",
      "iteration 600 / 1500: loss 2.089096\n",
      "iteration 700 / 1500: loss 2.121016\n",
      "iteration 800 / 1500: loss 2.098946\n",
      "iteration 900 / 1500: loss 2.092803\n",
      "iteration 1000 / 1500: loss 2.089108\n",
      "iteration 1100 / 1500: loss 2.114649\n",
      "iteration 1200 / 1500: loss 2.059485\n",
      "iteration 1300 / 1500: loss 2.121060\n",
      "iteration 1400 / 1500: loss 2.119259\n",
      "iteration 0 / 1500: loss 1298.885111\n",
      "iteration 100 / 1500: loss 7.934229\n",
      "iteration 200 / 1500: loss 2.170903\n",
      "iteration 300 / 1500: loss 2.103470\n",
      "iteration 400 / 1500: loss 2.106407\n",
      "iteration 500 / 1500: loss 2.093424\n",
      "iteration 600 / 1500: loss 2.138443\n",
      "iteration 700 / 1500: loss 2.132343\n",
      "iteration 800 / 1500: loss 2.116971\n",
      "iteration 900 / 1500: loss 2.127089\n",
      "iteration 1000 / 1500: loss 2.074317\n",
      "iteration 1100 / 1500: loss 2.085419\n",
      "iteration 1200 / 1500: loss 2.151374\n",
      "iteration 1300 / 1500: loss 2.089700\n",
      "iteration 1400 / 1500: loss 2.081807\n",
      "iteration 0 / 1500: loss 1333.173334\n",
      "iteration 100 / 1500: loss 7.428844\n",
      "iteration 200 / 1500: loss 2.150855\n",
      "iteration 300 / 1500: loss 2.154179\n",
      "iteration 400 / 1500: loss 2.138315\n",
      "iteration 500 / 1500: loss 2.109266\n",
      "iteration 600 / 1500: loss 2.091595\n",
      "iteration 700 / 1500: loss 2.061694\n",
      "iteration 800 / 1500: loss 2.165640\n",
      "iteration 900 / 1500: loss 2.161487\n",
      "iteration 1000 / 1500: loss 2.118592\n",
      "iteration 1100 / 1500: loss 2.068369\n",
      "iteration 1200 / 1500: loss 2.142723\n",
      "iteration 1300 / 1500: loss 2.172964\n",
      "iteration 1400 / 1500: loss 2.177933\n",
      "iteration 0 / 1500: loss 1361.538355\n",
      "iteration 100 / 1500: loss 6.860946\n",
      "iteration 200 / 1500: loss 2.091843\n",
      "iteration 300 / 1500: loss 2.091533\n",
      "iteration 400 / 1500: loss 2.076864\n",
      "iteration 500 / 1500: loss 2.070096\n",
      "iteration 600 / 1500: loss 2.112101\n",
      "iteration 700 / 1500: loss 2.154712\n",
      "iteration 800 / 1500: loss 2.130546\n",
      "iteration 900 / 1500: loss 2.101989\n",
      "iteration 1000 / 1500: loss 2.117464\n",
      "iteration 1100 / 1500: loss 2.105334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 2.079050\n",
      "iteration 1300 / 1500: loss 2.104826\n",
      "iteration 1400 / 1500: loss 2.089769\n",
      "iteration 0 / 1500: loss 1371.060935\n",
      "iteration 100 / 1500: loss 6.315298\n",
      "iteration 200 / 1500: loss 2.129799\n",
      "iteration 300 / 1500: loss 2.110657\n",
      "iteration 400 / 1500: loss 2.122975\n",
      "iteration 500 / 1500: loss 2.110365\n",
      "iteration 600 / 1500: loss 2.107098\n",
      "iteration 700 / 1500: loss 2.146224\n",
      "iteration 800 / 1500: loss 2.124377\n",
      "iteration 900 / 1500: loss 2.052627\n",
      "iteration 1000 / 1500: loss 2.159308\n",
      "iteration 1100 / 1500: loss 2.081901\n",
      "iteration 1200 / 1500: loss 2.078832\n",
      "iteration 1300 / 1500: loss 2.081757\n",
      "iteration 1400 / 1500: loss 2.096392\n",
      "iteration 0 / 1500: loss 1443.349919\n",
      "iteration 100 / 1500: loss 5.938900\n",
      "iteration 200 / 1500: loss 2.169588\n",
      "iteration 300 / 1500: loss 2.162665\n",
      "iteration 400 / 1500: loss 2.128862\n",
      "iteration 500 / 1500: loss 2.136690\n",
      "iteration 600 / 1500: loss 2.135867\n",
      "iteration 700 / 1500: loss 2.089406\n",
      "iteration 800 / 1500: loss 2.102790\n",
      "iteration 900 / 1500: loss 2.132591\n",
      "iteration 1000 / 1500: loss 2.082068\n",
      "iteration 1100 / 1500: loss 2.109327\n",
      "iteration 1200 / 1500: loss 2.109305\n",
      "iteration 1300 / 1500: loss 2.119164\n",
      "iteration 1400 / 1500: loss 2.108030\n",
      "iteration 0 / 1500: loss 1466.529876\n",
      "iteration 100 / 1500: loss 5.586540\n",
      "iteration 200 / 1500: loss 2.135309\n",
      "iteration 300 / 1500: loss 2.113203\n",
      "iteration 400 / 1500: loss 2.110819\n",
      "iteration 500 / 1500: loss 2.108116\n",
      "iteration 600 / 1500: loss 2.067145\n",
      "iteration 700 / 1500: loss 2.154926\n",
      "iteration 800 / 1500: loss 2.147593\n",
      "iteration 900 / 1500: loss 2.146993\n",
      "iteration 1000 / 1500: loss 2.120791\n",
      "iteration 1100 / 1500: loss 2.128259\n",
      "iteration 1200 / 1500: loss 2.122397\n",
      "iteration 1300 / 1500: loss 2.117264\n",
      "iteration 1400 / 1500: loss 2.131411\n",
      "iteration 0 / 1500: loss 1483.617262\n",
      "iteration 100 / 1500: loss 5.169693\n",
      "iteration 200 / 1500: loss 2.095653\n",
      "iteration 300 / 1500: loss 2.131488\n",
      "iteration 400 / 1500: loss 2.139875\n",
      "iteration 500 / 1500: loss 2.167291\n",
      "iteration 600 / 1500: loss 2.085526\n",
      "iteration 700 / 1500: loss 2.096845\n",
      "iteration 800 / 1500: loss 2.080458\n",
      "iteration 900 / 1500: loss 2.118557\n",
      "iteration 1000 / 1500: loss 2.080189\n",
      "iteration 1100 / 1500: loss 2.211994\n",
      "iteration 1200 / 1500: loss 2.141220\n",
      "iteration 1300 / 1500: loss 2.113522\n",
      "iteration 1400 / 1500: loss 2.076465\n",
      "iteration 0 / 1500: loss 1509.390035\n",
      "iteration 100 / 1500: loss 4.865806\n",
      "iteration 200 / 1500: loss 2.077816\n",
      "iteration 300 / 1500: loss 2.111481\n",
      "iteration 400 / 1500: loss 2.111835\n",
      "iteration 500 / 1500: loss 2.106884\n",
      "iteration 600 / 1500: loss 2.079620\n",
      "iteration 700 / 1500: loss 2.108044\n",
      "iteration 800 / 1500: loss 2.154452\n",
      "iteration 900 / 1500: loss 2.150031\n",
      "iteration 1000 / 1500: loss 2.123351\n",
      "iteration 1100 / 1500: loss 2.090347\n",
      "iteration 1200 / 1500: loss 2.080567\n",
      "iteration 1300 / 1500: loss 2.168344\n",
      "iteration 1400 / 1500: loss 2.181753\n",
      "iteration 0 / 1500: loss 1550.794977\n",
      "iteration 100 / 1500: loss 4.575861\n",
      "iteration 200 / 1500: loss 2.144281\n",
      "iteration 300 / 1500: loss 2.148271\n",
      "iteration 400 / 1500: loss 2.102945\n",
      "iteration 500 / 1500: loss 2.129887\n",
      "iteration 600 / 1500: loss 2.159359\n",
      "iteration 700 / 1500: loss 2.177832\n",
      "iteration 800 / 1500: loss 2.170623\n",
      "iteration 900 / 1500: loss 2.073868\n",
      "iteration 1000 / 1500: loss 2.090740\n",
      "iteration 1100 / 1500: loss 2.089345\n",
      "iteration 1200 / 1500: loss 2.130650\n",
      "iteration 1300 / 1500: loss 2.121581\n",
      "iteration 1400 / 1500: loss 2.127546\n",
      "iteration 0 / 1500: loss 767.492071\n",
      "iteration 100 / 1500: loss 15.606490\n",
      "iteration 200 / 1500: loss 2.327602\n",
      "iteration 300 / 1500: loss 2.128114\n",
      "iteration 400 / 1500: loss 2.039854\n",
      "iteration 500 / 1500: loss 2.103891\n",
      "iteration 600 / 1500: loss 2.080306\n",
      "iteration 700 / 1500: loss 2.106672\n",
      "iteration 800 / 1500: loss 2.022070\n",
      "iteration 900 / 1500: loss 1.973355\n",
      "iteration 1000 / 1500: loss 2.030522\n",
      "iteration 1100 / 1500: loss 2.063706\n",
      "iteration 1200 / 1500: loss 2.132703\n",
      "iteration 1300 / 1500: loss 2.041176\n",
      "iteration 1400 / 1500: loss 2.073752\n",
      "iteration 0 / 1500: loss 806.336101\n",
      "iteration 100 / 1500: loss 14.185214\n",
      "iteration 200 / 1500: loss 2.204492\n",
      "iteration 300 / 1500: loss 2.020394\n",
      "iteration 400 / 1500: loss 2.069103\n",
      "iteration 500 / 1500: loss 2.078793\n",
      "iteration 600 / 1500: loss 2.023866\n",
      "iteration 700 / 1500: loss 2.128771\n",
      "iteration 800 / 1500: loss 2.021076\n",
      "iteration 900 / 1500: loss 2.054661\n",
      "iteration 1000 / 1500: loss 2.075690\n",
      "iteration 1100 / 1500: loss 2.042244\n",
      "iteration 1200 / 1500: loss 2.049965\n",
      "iteration 1300 / 1500: loss 2.074961\n",
      "iteration 1400 / 1500: loss 2.080463\n",
      "iteration 0 / 1500: loss 832.838126\n",
      "iteration 100 / 1500: loss 12.689391\n",
      "iteration 200 / 1500: loss 2.228952\n",
      "iteration 300 / 1500: loss 2.030136\n",
      "iteration 400 / 1500: loss 2.048138\n",
      "iteration 500 / 1500: loss 2.132975\n",
      "iteration 600 / 1500: loss 2.109204\n",
      "iteration 700 / 1500: loss 2.043959\n",
      "iteration 800 / 1500: loss 2.040776\n",
      "iteration 900 / 1500: loss 2.067229\n",
      "iteration 1000 / 1500: loss 2.000568\n",
      "iteration 1100 / 1500: loss 2.066229\n",
      "iteration 1200 / 1500: loss 2.059142\n",
      "iteration 1300 / 1500: loss 2.029567\n",
      "iteration 1400 / 1500: loss 2.070067\n",
      "iteration 0 / 1500: loss 863.353607\n",
      "iteration 100 / 1500: loss 11.443281\n",
      "iteration 200 / 1500: loss 2.166443\n",
      "iteration 300 / 1500: loss 2.020669\n",
      "iteration 400 / 1500: loss 2.052769\n",
      "iteration 500 / 1500: loss 2.062470\n",
      "iteration 600 / 1500: loss 2.010701\n",
      "iteration 700 / 1500: loss 2.005583\n",
      "iteration 800 / 1500: loss 2.109874\n",
      "iteration 900 / 1500: loss 2.030153\n",
      "iteration 1000 / 1500: loss 2.116763\n",
      "iteration 1100 / 1500: loss 2.088621\n",
      "iteration 1200 / 1500: loss 2.078407\n",
      "iteration 1300 / 1500: loss 2.103639\n",
      "iteration 1400 / 1500: loss 2.088227\n",
      "iteration 0 / 1500: loss 891.114207\n",
      "iteration 100 / 1500: loss 10.308896\n",
      "iteration 200 / 1500: loss 2.104684\n",
      "iteration 300 / 1500: loss 2.123911\n",
      "iteration 400 / 1500: loss 2.113805\n",
      "iteration 500 / 1500: loss 2.050021\n",
      "iteration 600 / 1500: loss 2.084339\n",
      "iteration 700 / 1500: loss 2.048667\n",
      "iteration 800 / 1500: loss 1.987345\n",
      "iteration 900 / 1500: loss 2.079419\n",
      "iteration 1000 / 1500: loss 2.054222\n",
      "iteration 1100 / 1500: loss 2.072297\n",
      "iteration 1200 / 1500: loss 2.069804\n",
      "iteration 1300 / 1500: loss 2.009829\n",
      "iteration 1400 / 1500: loss 2.068538\n",
      "iteration 0 / 1500: loss 922.196917\n",
      "iteration 100 / 1500: loss 9.300910\n",
      "iteration 200 / 1500: loss 2.133862\n",
      "iteration 300 / 1500: loss 2.095217\n",
      "iteration 400 / 1500: loss 2.095380\n",
      "iteration 500 / 1500: loss 2.083488\n",
      "iteration 600 / 1500: loss 2.109509\n",
      "iteration 700 / 1500: loss 2.061858\n",
      "iteration 800 / 1500: loss 2.080826\n",
      "iteration 900 / 1500: loss 2.124002\n",
      "iteration 1000 / 1500: loss 2.041598\n",
      "iteration 1100 / 1500: loss 2.072852\n",
      "iteration 1200 / 1500: loss 2.059559\n",
      "iteration 1300 / 1500: loss 2.034907\n",
      "iteration 1400 / 1500: loss 2.099757\n",
      "iteration 0 / 1500: loss 957.931322\n",
      "iteration 100 / 1500: loss 8.454293\n",
      "iteration 200 / 1500: loss 2.120355\n",
      "iteration 300 / 1500: loss 2.108095\n",
      "iteration 400 / 1500: loss 2.067045\n",
      "iteration 500 / 1500: loss 2.080190\n",
      "iteration 600 / 1500: loss 2.083974\n",
      "iteration 700 / 1500: loss 2.066052\n",
      "iteration 800 / 1500: loss 2.099336\n",
      "iteration 900 / 1500: loss 2.106089\n",
      "iteration 1000 / 1500: loss 2.094420\n",
      "iteration 1100 / 1500: loss 2.066723\n",
      "iteration 1200 / 1500: loss 2.101769\n",
      "iteration 1300 / 1500: loss 2.076214\n",
      "iteration 1400 / 1500: loss 2.066257\n",
      "iteration 0 / 1500: loss 983.558392\n",
      "iteration 100 / 1500: loss 7.669826\n",
      "iteration 200 / 1500: loss 2.093826\n",
      "iteration 300 / 1500: loss 2.093682\n",
      "iteration 400 / 1500: loss 2.067431\n",
      "iteration 500 / 1500: loss 2.080630\n",
      "iteration 600 / 1500: loss 2.101544\n",
      "iteration 700 / 1500: loss 2.043542\n",
      "iteration 800 / 1500: loss 2.090811\n",
      "iteration 900 / 1500: loss 2.130234\n",
      "iteration 1000 / 1500: loss 2.095772\n",
      "iteration 1100 / 1500: loss 2.032157\n",
      "iteration 1200 / 1500: loss 2.067730\n",
      "iteration 1300 / 1500: loss 2.028131\n",
      "iteration 1400 / 1500: loss 2.053974\n",
      "iteration 0 / 1500: loss 1004.931896\n",
      "iteration 100 / 1500: loss 6.899270\n",
      "iteration 200 / 1500: loss 2.120576\n",
      "iteration 300 / 1500: loss 2.047002\n",
      "iteration 400 / 1500: loss 2.006826\n",
      "iteration 500 / 1500: loss 2.075398\n",
      "iteration 600 / 1500: loss 2.079882\n",
      "iteration 700 / 1500: loss 2.118918\n",
      "iteration 800 / 1500: loss 2.066959\n",
      "iteration 900 / 1500: loss 2.119151\n",
      "iteration 1000 / 1500: loss 2.106388\n",
      "iteration 1100 / 1500: loss 2.089251\n",
      "iteration 1200 / 1500: loss 2.053287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300 / 1500: loss 2.104992\n",
      "iteration 1400 / 1500: loss 2.086084\n",
      "iteration 0 / 1500: loss 1062.474604\n",
      "iteration 100 / 1500: loss 6.420374\n",
      "iteration 200 / 1500: loss 2.153842\n",
      "iteration 300 / 1500: loss 2.042899\n",
      "iteration 400 / 1500: loss 2.128570\n",
      "iteration 500 / 1500: loss 2.089753\n",
      "iteration 600 / 1500: loss 2.089758\n",
      "iteration 700 / 1500: loss 2.091101\n",
      "iteration 800 / 1500: loss 2.069597\n",
      "iteration 900 / 1500: loss 2.057065\n",
      "iteration 1000 / 1500: loss 2.135297\n",
      "iteration 1100 / 1500: loss 2.075580\n",
      "iteration 1200 / 1500: loss 2.100025\n",
      "iteration 1300 / 1500: loss 2.108500\n",
      "iteration 1400 / 1500: loss 2.128042\n",
      "iteration 0 / 1500: loss 1080.197085\n",
      "iteration 100 / 1500: loss 5.813871\n",
      "iteration 200 / 1500: loss 2.149266\n",
      "iteration 300 / 1500: loss 2.082760\n",
      "iteration 400 / 1500: loss 2.108168\n",
      "iteration 500 / 1500: loss 2.118291\n",
      "iteration 600 / 1500: loss 2.087793\n",
      "iteration 700 / 1500: loss 2.070856\n",
      "iteration 800 / 1500: loss 2.103092\n",
      "iteration 900 / 1500: loss 2.135463\n",
      "iteration 1000 / 1500: loss 2.057027\n",
      "iteration 1100 / 1500: loss 2.091198\n",
      "iteration 1200 / 1500: loss 2.081089\n",
      "iteration 1300 / 1500: loss 2.059877\n",
      "iteration 1400 / 1500: loss 2.083952\n",
      "iteration 0 / 1500: loss 1136.237285\n",
      "iteration 100 / 1500: loss 5.496003\n",
      "iteration 200 / 1500: loss 2.059432\n",
      "iteration 300 / 1500: loss 2.107465\n",
      "iteration 400 / 1500: loss 2.084254\n",
      "iteration 500 / 1500: loss 2.061356\n",
      "iteration 600 / 1500: loss 2.141245\n",
      "iteration 700 / 1500: loss 2.083764\n",
      "iteration 800 / 1500: loss 2.097760\n",
      "iteration 900 / 1500: loss 2.077014\n",
      "iteration 1000 / 1500: loss 2.070000\n",
      "iteration 1100 / 1500: loss 2.118190\n",
      "iteration 1200 / 1500: loss 2.125976\n",
      "iteration 1300 / 1500: loss 2.077905\n",
      "iteration 1400 / 1500: loss 2.135519\n",
      "iteration 0 / 1500: loss 1151.048990\n",
      "iteration 100 / 1500: loss 4.962764\n",
      "iteration 200 / 1500: loss 2.087221\n",
      "iteration 300 / 1500: loss 2.088275\n",
      "iteration 400 / 1500: loss 2.072874\n",
      "iteration 500 / 1500: loss 2.178835\n",
      "iteration 600 / 1500: loss 2.083793\n",
      "iteration 700 / 1500: loss 2.086501\n",
      "iteration 800 / 1500: loss 2.107552\n",
      "iteration 900 / 1500: loss 2.062639\n",
      "iteration 1000 / 1500: loss 2.140604\n",
      "iteration 1100 / 1500: loss 2.112913\n",
      "iteration 1200 / 1500: loss 2.065623\n",
      "iteration 1300 / 1500: loss 2.115125\n",
      "iteration 1400 / 1500: loss 2.142003\n",
      "iteration 0 / 1500: loss 1164.570110\n",
      "iteration 100 / 1500: loss 4.532282\n",
      "iteration 200 / 1500: loss 2.073696\n",
      "iteration 300 / 1500: loss 2.066918\n",
      "iteration 400 / 1500: loss 2.124138\n",
      "iteration 500 / 1500: loss 2.085617\n",
      "iteration 600 / 1500: loss 2.140019\n",
      "iteration 700 / 1500: loss 2.110834\n",
      "iteration 800 / 1500: loss 2.109863\n",
      "iteration 900 / 1500: loss 2.121422\n",
      "iteration 1000 / 1500: loss 2.080461\n",
      "iteration 1100 / 1500: loss 2.112600\n",
      "iteration 1200 / 1500: loss 2.140141\n",
      "iteration 1300 / 1500: loss 2.071924\n",
      "iteration 1400 / 1500: loss 2.057525\n",
      "iteration 0 / 1500: loss 1213.485640\n",
      "iteration 100 / 1500: loss 4.306554\n",
      "iteration 200 / 1500: loss 2.116694\n",
      "iteration 300 / 1500: loss 2.165922\n",
      "iteration 400 / 1500: loss 2.086833\n",
      "iteration 500 / 1500: loss 2.093819\n",
      "iteration 600 / 1500: loss 2.063601\n",
      "iteration 700 / 1500: loss 2.049602\n",
      "iteration 800 / 1500: loss 2.111756\n",
      "iteration 900 / 1500: loss 2.126076\n",
      "iteration 1000 / 1500: loss 2.135493\n",
      "iteration 1100 / 1500: loss 2.144161\n",
      "iteration 1200 / 1500: loss 2.109782\n",
      "iteration 1300 / 1500: loss 2.107505\n",
      "iteration 1400 / 1500: loss 2.163436\n",
      "iteration 0 / 1500: loss 1222.235162\n",
      "iteration 100 / 1500: loss 3.915932\n",
      "iteration 200 / 1500: loss 2.122329\n",
      "iteration 300 / 1500: loss 2.079192\n",
      "iteration 400 / 1500: loss 2.048639\n",
      "iteration 500 / 1500: loss 2.116989\n",
      "iteration 600 / 1500: loss 2.093069\n",
      "iteration 700 / 1500: loss 2.136923\n",
      "iteration 800 / 1500: loss 2.051720\n",
      "iteration 900 / 1500: loss 2.066906\n",
      "iteration 1000 / 1500: loss 2.130499\n",
      "iteration 1100 / 1500: loss 2.083931\n",
      "iteration 1200 / 1500: loss 2.083231\n",
      "iteration 1300 / 1500: loss 2.078000\n",
      "iteration 1400 / 1500: loss 2.123700\n",
      "iteration 0 / 1500: loss 1267.371338\n",
      "iteration 100 / 1500: loss 3.715159\n",
      "iteration 200 / 1500: loss 2.126561\n",
      "iteration 300 / 1500: loss 2.098646\n",
      "iteration 400 / 1500: loss 2.180384\n",
      "iteration 500 / 1500: loss 2.104545\n",
      "iteration 600 / 1500: loss 2.098804\n",
      "iteration 700 / 1500: loss 2.118885\n",
      "iteration 800 / 1500: loss 2.130315\n",
      "iteration 900 / 1500: loss 2.127837\n",
      "iteration 1000 / 1500: loss 2.075459\n",
      "iteration 1100 / 1500: loss 2.099185\n",
      "iteration 1200 / 1500: loss 2.102591\n",
      "iteration 1300 / 1500: loss 2.047655\n",
      "iteration 1400 / 1500: loss 2.091156\n",
      "iteration 0 / 1500: loss 1310.482726\n",
      "iteration 100 / 1500: loss 3.533634\n",
      "iteration 200 / 1500: loss 2.109578\n",
      "iteration 300 / 1500: loss 2.142429\n",
      "iteration 400 / 1500: loss 2.089449\n",
      "iteration 500 / 1500: loss 2.118466\n",
      "iteration 600 / 1500: loss 2.083795\n",
      "iteration 700 / 1500: loss 2.042529\n",
      "iteration 800 / 1500: loss 2.081787\n",
      "iteration 900 / 1500: loss 2.115096\n",
      "iteration 1000 / 1500: loss 2.133774\n",
      "iteration 1100 / 1500: loss 2.107685\n",
      "iteration 1200 / 1500: loss 2.113716\n",
      "iteration 1300 / 1500: loss 2.140220\n",
      "iteration 1400 / 1500: loss 2.106599\n",
      "iteration 0 / 1500: loss 1326.011003\n",
      "iteration 100 / 1500: loss 3.359471\n",
      "iteration 200 / 1500: loss 2.136160\n",
      "iteration 300 / 1500: loss 2.142866\n",
      "iteration 400 / 1500: loss 2.156948\n",
      "iteration 500 / 1500: loss 2.138026\n",
      "iteration 600 / 1500: loss 2.092217\n",
      "iteration 700 / 1500: loss 2.106326\n",
      "iteration 800 / 1500: loss 2.085990\n",
      "iteration 900 / 1500: loss 2.126982\n",
      "iteration 1000 / 1500: loss 2.070518\n",
      "iteration 1100 / 1500: loss 2.164701\n",
      "iteration 1200 / 1500: loss 2.082731\n",
      "iteration 1300 / 1500: loss 2.116769\n",
      "iteration 1400 / 1500: loss 2.110214\n",
      "iteration 0 / 1500: loss 1375.660984\n",
      "iteration 100 / 1500: loss 3.198635\n",
      "iteration 200 / 1500: loss 2.081394\n",
      "iteration 300 / 1500: loss 2.097561\n",
      "iteration 400 / 1500: loss 2.085495\n",
      "iteration 500 / 1500: loss 2.076179\n",
      "iteration 600 / 1500: loss 2.131687\n",
      "iteration 700 / 1500: loss 2.128526\n",
      "iteration 800 / 1500: loss 2.116830\n",
      "iteration 900 / 1500: loss 2.122691\n",
      "iteration 1000 / 1500: loss 2.097444\n",
      "iteration 1100 / 1500: loss 2.148375\n",
      "iteration 1200 / 1500: loss 2.088037\n",
      "iteration 1300 / 1500: loss 2.174277\n",
      "iteration 1400 / 1500: loss 2.092954\n",
      "iteration 0 / 1500: loss 1371.592078\n",
      "iteration 100 / 1500: loss 3.050096\n",
      "iteration 200 / 1500: loss 2.066580\n",
      "iteration 300 / 1500: loss 2.115743\n",
      "iteration 400 / 1500: loss 2.035741\n",
      "iteration 500 / 1500: loss 2.122957\n",
      "iteration 600 / 1500: loss 2.128161\n",
      "iteration 700 / 1500: loss 2.096410\n",
      "iteration 800 / 1500: loss 2.109087\n",
      "iteration 900 / 1500: loss 2.166133\n",
      "iteration 1000 / 1500: loss 2.150899\n",
      "iteration 1100 / 1500: loss 2.178163\n",
      "iteration 1200 / 1500: loss 2.080166\n",
      "iteration 1300 / 1500: loss 2.097194\n",
      "iteration 1400 / 1500: loss 2.118123\n",
      "iteration 0 / 1500: loss 1416.868102\n",
      "iteration 100 / 1500: loss 2.951703\n",
      "iteration 200 / 1500: loss 2.062004\n",
      "iteration 300 / 1500: loss 2.096620\n",
      "iteration 400 / 1500: loss 2.150451\n",
      "iteration 500 / 1500: loss 2.102383\n",
      "iteration 600 / 1500: loss 2.149994\n",
      "iteration 700 / 1500: loss 2.071692\n",
      "iteration 800 / 1500: loss 2.105479\n",
      "iteration 900 / 1500: loss 2.108983\n",
      "iteration 1000 / 1500: loss 2.153530\n",
      "iteration 1100 / 1500: loss 2.076559\n",
      "iteration 1200 / 1500: loss 2.113523\n",
      "iteration 1300 / 1500: loss 2.078974\n",
      "iteration 1400 / 1500: loss 2.183625\n",
      "iteration 0 / 1500: loss 1455.126625\n",
      "iteration 100 / 1500: loss 2.794660\n",
      "iteration 200 / 1500: loss 2.128811\n",
      "iteration 300 / 1500: loss 2.116021\n",
      "iteration 400 / 1500: loss 2.114571\n",
      "iteration 500 / 1500: loss 2.111029\n",
      "iteration 600 / 1500: loss 2.124846\n",
      "iteration 700 / 1500: loss 2.062182\n",
      "iteration 800 / 1500: loss 2.124092\n",
      "iteration 900 / 1500: loss 2.135611\n",
      "iteration 1000 / 1500: loss 2.097338\n",
      "iteration 1100 / 1500: loss 2.137865\n",
      "iteration 1200 / 1500: loss 2.168734\n",
      "iteration 1300 / 1500: loss 2.140686\n",
      "iteration 1400 / 1500: loss 2.098776\n",
      "iteration 0 / 1500: loss 1498.126272\n",
      "iteration 100 / 1500: loss 2.830628\n",
      "iteration 200 / 1500: loss 2.113962\n",
      "iteration 300 / 1500: loss 2.104491\n",
      "iteration 400 / 1500: loss 2.136060\n",
      "iteration 500 / 1500: loss 2.122994\n",
      "iteration 600 / 1500: loss 2.099543\n",
      "iteration 700 / 1500: loss 2.069414\n",
      "iteration 800 / 1500: loss 2.124160\n",
      "iteration 900 / 1500: loss 2.086234\n",
      "iteration 1000 / 1500: loss 2.144384\n",
      "iteration 1100 / 1500: loss 2.109548\n",
      "iteration 1200 / 1500: loss 2.061375\n",
      "iteration 1300 / 1500: loss 2.139979\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.053488\n",
      "iteration 0 / 1500: loss 1514.869463\n",
      "iteration 100 / 1500: loss 2.644041\n",
      "iteration 200 / 1500: loss 2.143572\n",
      "iteration 300 / 1500: loss 2.082998\n",
      "iteration 400 / 1500: loss 2.132505\n",
      "iteration 500 / 1500: loss 2.130655\n",
      "iteration 600 / 1500: loss 2.143998\n",
      "iteration 700 / 1500: loss 2.113641\n",
      "iteration 800 / 1500: loss 2.077664\n",
      "iteration 900 / 1500: loss 2.115387\n",
      "iteration 1000 / 1500: loss 2.145100\n",
      "iteration 1100 / 1500: loss 2.153052\n",
      "iteration 1200 / 1500: loss 2.107081\n",
      "iteration 1300 / 1500: loss 2.104079\n",
      "iteration 1400 / 1500: loss 2.152787\n",
      "iteration 0 / 1500: loss 1525.971318\n",
      "iteration 100 / 1500: loss 2.558530\n",
      "iteration 200 / 1500: loss 2.124518\n",
      "iteration 300 / 1500: loss 2.109562\n",
      "iteration 400 / 1500: loss 2.038855\n",
      "iteration 500 / 1500: loss 2.202164\n",
      "iteration 600 / 1500: loss 2.091588\n",
      "iteration 700 / 1500: loss 2.099551\n",
      "iteration 800 / 1500: loss 2.137078\n",
      "iteration 900 / 1500: loss 2.165801\n",
      "iteration 1000 / 1500: loss 2.090030\n",
      "iteration 1100 / 1500: loss 2.102792\n",
      "iteration 1200 / 1500: loss 2.141558\n",
      "iteration 1300 / 1500: loss 2.130985\n",
      "iteration 1400 / 1500: loss 2.154701\n",
      "iteration 0 / 1500: loss 778.881346\n",
      "iteration 100 / 1500: loss 6.883009\n",
      "iteration 200 / 1500: loss 2.006053\n",
      "iteration 300 / 1500: loss 2.083194\n",
      "iteration 400 / 1500: loss 2.077954\n",
      "iteration 500 / 1500: loss 2.045875\n",
      "iteration 600 / 1500: loss 2.078113\n",
      "iteration 700 / 1500: loss 2.062302\n",
      "iteration 800 / 1500: loss 2.021569\n",
      "iteration 900 / 1500: loss 2.029327\n",
      "iteration 1000 / 1500: loss 2.039586\n",
      "iteration 1100 / 1500: loss 2.074584\n",
      "iteration 1200 / 1500: loss 2.055590\n",
      "iteration 1300 / 1500: loss 2.050772\n",
      "iteration 1400 / 1500: loss 2.078999\n",
      "iteration 0 / 1500: loss 804.379972\n",
      "iteration 100 / 1500: loss 6.085934\n",
      "iteration 200 / 1500: loss 2.027553\n",
      "iteration 300 / 1500: loss 2.026756\n",
      "iteration 400 / 1500: loss 2.022184\n",
      "iteration 500 / 1500: loss 2.045957\n",
      "iteration 600 / 1500: loss 2.072306\n",
      "iteration 700 / 1500: loss 2.022895\n",
      "iteration 800 / 1500: loss 2.121184\n",
      "iteration 900 / 1500: loss 2.060087\n",
      "iteration 1000 / 1500: loss 2.052758\n",
      "iteration 1100 / 1500: loss 2.059019\n",
      "iteration 1200 / 1500: loss 2.071035\n",
      "iteration 1300 / 1500: loss 2.106498\n",
      "iteration 1400 / 1500: loss 2.056773\n",
      "iteration 0 / 1500: loss 838.880271\n",
      "iteration 100 / 1500: loss 5.428251\n",
      "iteration 200 / 1500: loss 2.122009\n",
      "iteration 300 / 1500: loss 2.020973\n",
      "iteration 400 / 1500: loss 2.063517\n",
      "iteration 500 / 1500: loss 2.070022\n",
      "iteration 600 / 1500: loss 2.015828\n",
      "iteration 700 / 1500: loss 2.069706\n",
      "iteration 800 / 1500: loss 1.993684\n",
      "iteration 900 / 1500: loss 2.058312\n",
      "iteration 1000 / 1500: loss 2.011360\n",
      "iteration 1100 / 1500: loss 2.034819\n",
      "iteration 1200 / 1500: loss 2.024420\n",
      "iteration 1300 / 1500: loss 2.045040\n",
      "iteration 1400 / 1500: loss 2.134558\n",
      "iteration 0 / 1500: loss 869.272924\n",
      "iteration 100 / 1500: loss 4.966245\n",
      "iteration 200 / 1500: loss 2.031098\n",
      "iteration 300 / 1500: loss 2.105093\n",
      "iteration 400 / 1500: loss 2.070403\n",
      "iteration 500 / 1500: loss 2.077745\n",
      "iteration 600 / 1500: loss 2.091279\n",
      "iteration 700 / 1500: loss 2.039799\n",
      "iteration 800 / 1500: loss 2.069932\n",
      "iteration 900 / 1500: loss 2.083318\n",
      "iteration 1000 / 1500: loss 2.098341\n",
      "iteration 1100 / 1500: loss 2.060971\n",
      "iteration 1200 / 1500: loss 2.097634\n",
      "iteration 1300 / 1500: loss 2.061700\n",
      "iteration 1400 / 1500: loss 2.102553\n",
      "iteration 0 / 1500: loss 898.637934\n",
      "iteration 100 / 1500: loss 4.482389\n",
      "iteration 200 / 1500: loss 2.065941\n",
      "iteration 300 / 1500: loss 2.080560\n",
      "iteration 400 / 1500: loss 2.077678\n",
      "iteration 500 / 1500: loss 2.165465\n",
      "iteration 600 / 1500: loss 2.124162\n",
      "iteration 700 / 1500: loss 2.040218\n",
      "iteration 800 / 1500: loss 2.048473\n",
      "iteration 900 / 1500: loss 2.113337\n",
      "iteration 1000 / 1500: loss 2.070429\n",
      "iteration 1100 / 1500: loss 2.055322\n",
      "iteration 1200 / 1500: loss 2.054263\n",
      "iteration 1300 / 1500: loss 2.059273\n",
      "iteration 1400 / 1500: loss 2.060067\n",
      "iteration 0 / 1500: loss 930.348486\n",
      "iteration 100 / 1500: loss 4.114295\n",
      "iteration 200 / 1500: loss 2.158083\n",
      "iteration 300 / 1500: loss 2.095276\n",
      "iteration 400 / 1500: loss 2.101846\n",
      "iteration 500 / 1500: loss 2.002109\n",
      "iteration 600 / 1500: loss 2.052518\n",
      "iteration 700 / 1500: loss 2.082394\n",
      "iteration 800 / 1500: loss 2.070841\n",
      "iteration 900 / 1500: loss 2.062657\n",
      "iteration 1000 / 1500: loss 2.092257\n",
      "iteration 1100 / 1500: loss 2.121346\n",
      "iteration 1200 / 1500: loss 2.149060\n",
      "iteration 1300 / 1500: loss 2.047645\n",
      "iteration 1400 / 1500: loss 1.997594\n",
      "iteration 0 / 1500: loss 962.140431\n",
      "iteration 100 / 1500: loss 3.772694\n",
      "iteration 200 / 1500: loss 2.066667\n",
      "iteration 300 / 1500: loss 2.052119\n",
      "iteration 400 / 1500: loss 2.128877\n",
      "iteration 500 / 1500: loss 2.134567\n",
      "iteration 600 / 1500: loss 2.109250\n",
      "iteration 700 / 1500: loss 2.038499\n",
      "iteration 800 / 1500: loss 2.073519\n",
      "iteration 900 / 1500: loss 2.124479\n",
      "iteration 1000 / 1500: loss 2.064725\n",
      "iteration 1100 / 1500: loss 2.146671\n",
      "iteration 1200 / 1500: loss 2.130238\n",
      "iteration 1300 / 1500: loss 2.044080\n",
      "iteration 1400 / 1500: loss 2.050899\n",
      "iteration 0 / 1500: loss 1011.652281\n",
      "iteration 100 / 1500: loss 3.527171\n",
      "iteration 200 / 1500: loss 2.089453\n",
      "iteration 300 / 1500: loss 2.086679\n",
      "iteration 400 / 1500: loss 2.069842\n",
      "iteration 500 / 1500: loss 2.111371\n",
      "iteration 600 / 1500: loss 2.042703\n",
      "iteration 700 / 1500: loss 2.133053\n",
      "iteration 800 / 1500: loss 2.119800\n",
      "iteration 900 / 1500: loss 2.063548\n",
      "iteration 1000 / 1500: loss 2.098886\n",
      "iteration 1100 / 1500: loss 2.008366\n",
      "iteration 1200 / 1500: loss 2.053934\n",
      "iteration 1300 / 1500: loss 2.086037\n",
      "iteration 1400 / 1500: loss 2.071455\n",
      "iteration 0 / 1500: loss 1007.958055\n",
      "iteration 100 / 1500: loss 3.225729\n",
      "iteration 200 / 1500: loss 2.114551\n",
      "iteration 300 / 1500: loss 2.127320\n",
      "iteration 400 / 1500: loss 2.083598\n",
      "iteration 500 / 1500: loss 2.020340\n",
      "iteration 600 / 1500: loss 2.062837\n",
      "iteration 700 / 1500: loss 2.136192\n",
      "iteration 800 / 1500: loss 2.108535\n",
      "iteration 900 / 1500: loss 2.097439\n",
      "iteration 1000 / 1500: loss 2.093384\n",
      "iteration 1100 / 1500: loss 2.110290\n",
      "iteration 1200 / 1500: loss 2.054932\n",
      "iteration 1300 / 1500: loss 2.077555\n",
      "iteration 1400 / 1500: loss 2.092289\n",
      "iteration 0 / 1500: loss 1050.270514\n",
      "iteration 100 / 1500: loss 3.120774\n",
      "iteration 200 / 1500: loss 1.998820\n",
      "iteration 300 / 1500: loss 2.067725\n",
      "iteration 400 / 1500: loss 2.080156\n",
      "iteration 500 / 1500: loss 2.084068\n",
      "iteration 600 / 1500: loss 2.079356\n",
      "iteration 700 / 1500: loss 2.056640\n",
      "iteration 800 / 1500: loss 2.115196\n",
      "iteration 900 / 1500: loss 2.093655\n",
      "iteration 1000 / 1500: loss 2.102577\n",
      "iteration 1100 / 1500: loss 2.053963\n",
      "iteration 1200 / 1500: loss 2.094101\n",
      "iteration 1300 / 1500: loss 2.049389\n",
      "iteration 1400 / 1500: loss 2.049595\n",
      "iteration 0 / 1500: loss 1090.232201\n",
      "iteration 100 / 1500: loss 2.920966\n",
      "iteration 200 / 1500: loss 2.090802\n",
      "iteration 300 / 1500: loss 2.110340\n",
      "iteration 400 / 1500: loss 2.117768\n",
      "iteration 500 / 1500: loss 2.097824\n",
      "iteration 600 / 1500: loss 2.050458\n",
      "iteration 700 / 1500: loss 2.110678\n",
      "iteration 800 / 1500: loss 2.083373\n",
      "iteration 900 / 1500: loss 2.087877\n",
      "iteration 1000 / 1500: loss 2.103555\n",
      "iteration 1100 / 1500: loss 2.070135\n",
      "iteration 1200 / 1500: loss 2.127567\n",
      "iteration 1300 / 1500: loss 2.124360\n",
      "iteration 1400 / 1500: loss 2.122282\n",
      "iteration 0 / 1500: loss 1106.208068\n",
      "iteration 100 / 1500: loss 2.767212\n",
      "iteration 200 / 1500: loss 2.117271\n",
      "iteration 300 / 1500: loss 2.132714\n",
      "iteration 400 / 1500: loss 2.083615\n",
      "iteration 500 / 1500: loss 2.117520\n",
      "iteration 600 / 1500: loss 2.094599\n",
      "iteration 700 / 1500: loss 2.091267\n",
      "iteration 800 / 1500: loss 2.133840\n",
      "iteration 900 / 1500: loss 2.128301\n",
      "iteration 1000 / 1500: loss 2.110414\n",
      "iteration 1100 / 1500: loss 2.093211\n",
      "iteration 1200 / 1500: loss 2.118094\n",
      "iteration 1300 / 1500: loss 2.150833\n",
      "iteration 1400 / 1500: loss 2.092974\n",
      "iteration 0 / 1500: loss 1143.100163\n",
      "iteration 100 / 1500: loss 2.663497\n",
      "iteration 200 / 1500: loss 2.084066\n",
      "iteration 300 / 1500: loss 2.093245\n",
      "iteration 400 / 1500: loss 2.105583\n",
      "iteration 500 / 1500: loss 2.082358\n",
      "iteration 600 / 1500: loss 2.106026\n",
      "iteration 700 / 1500: loss 2.139776\n",
      "iteration 800 / 1500: loss 2.082623\n",
      "iteration 900 / 1500: loss 2.099514\n",
      "iteration 1000 / 1500: loss 2.081317\n",
      "iteration 1100 / 1500: loss 2.119083\n",
      "iteration 1200 / 1500: loss 2.129993\n",
      "iteration 1300 / 1500: loss 2.123059\n",
      "iteration 1400 / 1500: loss 2.035354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 1164.455292\n",
      "iteration 100 / 1500: loss 2.586347\n",
      "iteration 200 / 1500: loss 2.124645\n",
      "iteration 300 / 1500: loss 2.119677\n",
      "iteration 400 / 1500: loss 2.096994\n",
      "iteration 500 / 1500: loss 2.073655\n",
      "iteration 600 / 1500: loss 2.078814\n",
      "iteration 700 / 1500: loss 2.103132\n",
      "iteration 800 / 1500: loss 2.149204\n",
      "iteration 900 / 1500: loss 2.065797\n",
      "iteration 1000 / 1500: loss 2.085514\n",
      "iteration 1100 / 1500: loss 2.121470\n",
      "iteration 1200 / 1500: loss 2.062319\n",
      "iteration 1300 / 1500: loss 2.027958\n",
      "iteration 1400 / 1500: loss 2.103277\n",
      "iteration 0 / 1500: loss 1188.856747\n",
      "iteration 100 / 1500: loss 2.538798\n",
      "iteration 200 / 1500: loss 2.034433\n",
      "iteration 300 / 1500: loss 2.072804\n",
      "iteration 400 / 1500: loss 2.069388\n",
      "iteration 500 / 1500: loss 2.154742\n",
      "iteration 600 / 1500: loss 2.107696\n",
      "iteration 700 / 1500: loss 2.128052\n",
      "iteration 800 / 1500: loss 2.123281\n",
      "iteration 900 / 1500: loss 2.109288\n",
      "iteration 1000 / 1500: loss 2.127830\n",
      "iteration 1100 / 1500: loss 2.072291\n",
      "iteration 1200 / 1500: loss 2.132654\n",
      "iteration 1300 / 1500: loss 2.090838\n",
      "iteration 1400 / 1500: loss 2.098150\n",
      "iteration 0 / 1500: loss 1233.823991\n",
      "iteration 100 / 1500: loss 2.437912\n",
      "iteration 200 / 1500: loss 2.084264\n",
      "iteration 300 / 1500: loss 2.068597\n",
      "iteration 400 / 1500: loss 2.102948\n",
      "iteration 500 / 1500: loss 2.058313\n",
      "iteration 600 / 1500: loss 2.086532\n",
      "iteration 700 / 1500: loss 2.104063\n",
      "iteration 800 / 1500: loss 2.140282\n",
      "iteration 900 / 1500: loss 2.111138\n",
      "iteration 1000 / 1500: loss 2.117992\n",
      "iteration 1100 / 1500: loss 2.084416\n",
      "iteration 1200 / 1500: loss 2.108211\n",
      "iteration 1300 / 1500: loss 2.101645\n",
      "iteration 1400 / 1500: loss 2.092058\n",
      "iteration 0 / 1500: loss 1274.463275\n",
      "iteration 100 / 1500: loss 2.413566\n",
      "iteration 200 / 1500: loss 2.139020\n",
      "iteration 300 / 1500: loss 2.080464\n",
      "iteration 400 / 1500: loss 2.112172\n",
      "iteration 500 / 1500: loss 2.078578\n",
      "iteration 600 / 1500: loss 2.103201\n",
      "iteration 700 / 1500: loss 2.124209\n",
      "iteration 800 / 1500: loss 2.079508\n",
      "iteration 900 / 1500: loss 2.022343\n",
      "iteration 1000 / 1500: loss 2.076377\n",
      "iteration 1100 / 1500: loss 2.113861\n",
      "iteration 1200 / 1500: loss 2.112970\n",
      "iteration 1300 / 1500: loss 2.114409\n",
      "iteration 1400 / 1500: loss 2.078851\n",
      "iteration 0 / 1500: loss 1294.500023\n",
      "iteration 100 / 1500: loss 2.376412\n",
      "iteration 200 / 1500: loss 2.139319\n",
      "iteration 300 / 1500: loss 2.108596\n",
      "iteration 400 / 1500: loss 2.134882\n",
      "iteration 500 / 1500: loss 2.146797\n",
      "iteration 600 / 1500: loss 2.099013\n",
      "iteration 700 / 1500: loss 2.072507\n",
      "iteration 800 / 1500: loss 2.149587\n",
      "iteration 900 / 1500: loss 2.146904\n",
      "iteration 1000 / 1500: loss 2.128292\n",
      "iteration 1100 / 1500: loss 2.057748\n",
      "iteration 1200 / 1500: loss 2.122197\n",
      "iteration 1300 / 1500: loss 2.126282\n",
      "iteration 1400 / 1500: loss 2.092655\n",
      "iteration 0 / 1500: loss 1314.564350\n",
      "iteration 100 / 1500: loss 2.337046\n",
      "iteration 200 / 1500: loss 2.113667\n",
      "iteration 300 / 1500: loss 2.158324\n",
      "iteration 400 / 1500: loss 2.140986\n",
      "iteration 500 / 1500: loss 2.115387\n",
      "iteration 600 / 1500: loss 2.151996\n",
      "iteration 700 / 1500: loss 2.100848\n",
      "iteration 800 / 1500: loss 2.101884\n",
      "iteration 900 / 1500: loss 2.146881\n",
      "iteration 1000 / 1500: loss 2.142416\n",
      "iteration 1100 / 1500: loss 2.109301\n",
      "iteration 1200 / 1500: loss 2.090168\n",
      "iteration 1300 / 1500: loss 2.065288\n",
      "iteration 1400 / 1500: loss 2.105398\n",
      "iteration 0 / 1500: loss 1358.703482\n",
      "iteration 100 / 1500: loss 2.240654\n",
      "iteration 200 / 1500: loss 2.113557\n",
      "iteration 300 / 1500: loss 2.128121\n",
      "iteration 400 / 1500: loss 2.167662\n",
      "iteration 500 / 1500: loss 2.161516\n",
      "iteration 600 / 1500: loss 2.161357\n",
      "iteration 700 / 1500: loss 2.103968\n",
      "iteration 800 / 1500: loss 2.126709\n",
      "iteration 900 / 1500: loss 2.125012\n",
      "iteration 1000 / 1500: loss 2.102490\n",
      "iteration 1100 / 1500: loss 2.115737\n",
      "iteration 1200 / 1500: loss 2.087595\n",
      "iteration 1300 / 1500: loss 2.032047\n",
      "iteration 1400 / 1500: loss 2.080674\n",
      "iteration 0 / 1500: loss 1382.516938\n",
      "iteration 100 / 1500: loss 2.254590\n",
      "iteration 200 / 1500: loss 2.187053\n",
      "iteration 300 / 1500: loss 2.124533\n",
      "iteration 400 / 1500: loss 2.135069\n",
      "iteration 500 / 1500: loss 2.081527\n",
      "iteration 600 / 1500: loss 2.120916\n",
      "iteration 700 / 1500: loss 2.114939\n",
      "iteration 800 / 1500: loss 2.113312\n",
      "iteration 900 / 1500: loss 2.089664\n",
      "iteration 1000 / 1500: loss 2.089172\n",
      "iteration 1100 / 1500: loss 2.132469\n",
      "iteration 1200 / 1500: loss 2.137490\n",
      "iteration 1300 / 1500: loss 2.125321\n",
      "iteration 1400 / 1500: loss 2.106425\n",
      "iteration 0 / 1500: loss 1411.447189\n",
      "iteration 100 / 1500: loss 2.233920\n",
      "iteration 200 / 1500: loss 2.102520\n",
      "iteration 300 / 1500: loss 2.114698\n",
      "iteration 400 / 1500: loss 2.133172\n",
      "iteration 500 / 1500: loss 2.039950\n",
      "iteration 600 / 1500: loss 2.121734\n",
      "iteration 700 / 1500: loss 2.119536\n",
      "iteration 800 / 1500: loss 2.117964\n",
      "iteration 900 / 1500: loss 2.124827\n",
      "iteration 1000 / 1500: loss 2.159693\n",
      "iteration 1100 / 1500: loss 2.068949\n",
      "iteration 1200 / 1500: loss 2.080446\n",
      "iteration 1300 / 1500: loss 2.085210\n",
      "iteration 1400 / 1500: loss 2.130909\n",
      "iteration 0 / 1500: loss 1420.984808\n",
      "iteration 100 / 1500: loss 2.209638\n",
      "iteration 200 / 1500: loss 2.112858\n",
      "iteration 300 / 1500: loss 2.141989\n",
      "iteration 400 / 1500: loss 2.130651\n",
      "iteration 500 / 1500: loss 2.112744\n",
      "iteration 600 / 1500: loss 2.106076\n",
      "iteration 700 / 1500: loss 2.088131\n",
      "iteration 800 / 1500: loss 2.143266\n",
      "iteration 900 / 1500: loss 2.122620\n",
      "iteration 1000 / 1500: loss 2.130338\n",
      "iteration 1100 / 1500: loss 2.147280\n",
      "iteration 1200 / 1500: loss 2.169702\n",
      "iteration 1300 / 1500: loss 2.184460\n",
      "iteration 1400 / 1500: loss 2.124260\n",
      "iteration 0 / 1500: loss 1470.534557\n",
      "iteration 100 / 1500: loss 2.195792\n",
      "iteration 200 / 1500: loss 2.154416\n",
      "iteration 300 / 1500: loss 2.162364\n",
      "iteration 400 / 1500: loss 2.136945\n",
      "iteration 500 / 1500: loss 2.144203\n",
      "iteration 600 / 1500: loss 2.116311\n",
      "iteration 700 / 1500: loss 2.121344\n",
      "iteration 800 / 1500: loss 2.146919\n",
      "iteration 900 / 1500: loss 2.073824\n",
      "iteration 1000 / 1500: loss 2.199820\n",
      "iteration 1100 / 1500: loss 2.144895\n",
      "iteration 1200 / 1500: loss 2.109481\n",
      "iteration 1300 / 1500: loss 2.072178\n",
      "iteration 1400 / 1500: loss 2.129124\n",
      "iteration 0 / 1500: loss 1514.232320\n",
      "iteration 100 / 1500: loss 2.221119\n",
      "iteration 200 / 1500: loss 2.154746\n",
      "iteration 300 / 1500: loss 2.129562\n",
      "iteration 400 / 1500: loss 2.115067\n",
      "iteration 500 / 1500: loss 2.124726\n",
      "iteration 600 / 1500: loss 2.113248\n",
      "iteration 700 / 1500: loss 2.148920\n",
      "iteration 800 / 1500: loss 2.169504\n",
      "iteration 900 / 1500: loss 2.143213\n",
      "iteration 1000 / 1500: loss 2.187636\n",
      "iteration 1100 / 1500: loss 2.135066\n",
      "iteration 1200 / 1500: loss 2.057516\n",
      "iteration 1300 / 1500: loss 2.147540\n",
      "iteration 1400 / 1500: loss 2.078743\n",
      "iteration 0 / 1500: loss 1554.662129\n",
      "iteration 100 / 1500: loss 2.162503\n",
      "iteration 200 / 1500: loss 2.102168\n",
      "iteration 300 / 1500: loss 2.145538\n",
      "iteration 400 / 1500: loss 2.166851\n",
      "iteration 500 / 1500: loss 2.127572\n",
      "iteration 600 / 1500: loss 2.156149\n",
      "iteration 700 / 1500: loss 2.131408\n",
      "iteration 800 / 1500: loss 2.123854\n",
      "iteration 900 / 1500: loss 2.138369\n",
      "iteration 1000 / 1500: loss 2.116819\n",
      "iteration 1100 / 1500: loss 2.102078\n",
      "iteration 1200 / 1500: loss 2.129049\n",
      "iteration 1300 / 1500: loss 2.089509\n",
      "iteration 1400 / 1500: loss 2.110276\n",
      "iteration 0 / 1500: loss 768.753651\n",
      "iteration 100 / 1500: loss 3.238038\n",
      "iteration 200 / 1500: loss 2.067051\n",
      "iteration 300 / 1500: loss 2.015987\n",
      "iteration 400 / 1500: loss 2.027682\n",
      "iteration 500 / 1500: loss 2.052803\n",
      "iteration 600 / 1500: loss 2.098839\n",
      "iteration 700 / 1500: loss 2.052824\n",
      "iteration 800 / 1500: loss 2.022651\n",
      "iteration 900 / 1500: loss 2.053547\n",
      "iteration 1000 / 1500: loss 2.057303\n",
      "iteration 1100 / 1500: loss 2.002761\n",
      "iteration 1200 / 1500: loss 2.054220\n",
      "iteration 1300 / 1500: loss 2.066948\n",
      "iteration 1400 / 1500: loss 2.031307\n",
      "iteration 0 / 1500: loss 810.216895\n",
      "iteration 100 / 1500: loss 3.054232\n",
      "iteration 200 / 1500: loss 2.047567\n",
      "iteration 300 / 1500: loss 2.002642\n",
      "iteration 400 / 1500: loss 2.105882\n",
      "iteration 500 / 1500: loss 2.137224\n",
      "iteration 600 / 1500: loss 2.089914\n",
      "iteration 700 / 1500: loss 2.067593\n",
      "iteration 800 / 1500: loss 2.094568\n",
      "iteration 900 / 1500: loss 2.070401\n",
      "iteration 1000 / 1500: loss 2.112954\n",
      "iteration 1100 / 1500: loss 2.066347\n",
      "iteration 1200 / 1500: loss 2.021061\n",
      "iteration 1300 / 1500: loss 2.026824\n",
      "iteration 1400 / 1500: loss 2.084853\n",
      "iteration 0 / 1500: loss 838.439461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss 2.822584\n",
      "iteration 200 / 1500: loss 2.107781\n",
      "iteration 300 / 1500: loss 2.070606\n",
      "iteration 400 / 1500: loss 2.047094\n",
      "iteration 500 / 1500: loss 2.063703\n",
      "iteration 600 / 1500: loss 2.075246\n",
      "iteration 700 / 1500: loss 2.060188\n",
      "iteration 800 / 1500: loss 2.041445\n",
      "iteration 900 / 1500: loss 2.069579\n",
      "iteration 1000 / 1500: loss 2.040740\n",
      "iteration 1100 / 1500: loss 1.997599\n",
      "iteration 1200 / 1500: loss 2.092979\n",
      "iteration 1300 / 1500: loss 2.121568\n",
      "iteration 1400 / 1500: loss 2.134590\n",
      "iteration 0 / 1500: loss 863.561158\n",
      "iteration 100 / 1500: loss 2.749297\n",
      "iteration 200 / 1500: loss 2.032980\n",
      "iteration 300 / 1500: loss 2.079801\n",
      "iteration 400 / 1500: loss 2.070839\n",
      "iteration 500 / 1500: loss 2.048609\n",
      "iteration 600 / 1500: loss 2.067384\n",
      "iteration 700 / 1500: loss 2.063683\n",
      "iteration 800 / 1500: loss 2.108577\n",
      "iteration 900 / 1500: loss 2.040509\n",
      "iteration 1000 / 1500: loss 2.082957\n",
      "iteration 1100 / 1500: loss 2.031563\n",
      "iteration 1200 / 1500: loss 2.035442\n",
      "iteration 1300 / 1500: loss 2.045728\n",
      "iteration 1400 / 1500: loss 2.022608\n",
      "iteration 0 / 1500: loss 902.025643\n",
      "iteration 100 / 1500: loss 2.594230\n",
      "iteration 200 / 1500: loss 2.046407\n",
      "iteration 300 / 1500: loss 2.117418\n",
      "iteration 400 / 1500: loss 2.063952\n",
      "iteration 500 / 1500: loss 2.056498\n",
      "iteration 600 / 1500: loss 2.094290\n",
      "iteration 700 / 1500: loss 2.069956\n",
      "iteration 800 / 1500: loss 2.099420\n",
      "iteration 900 / 1500: loss 2.080696\n",
      "iteration 1000 / 1500: loss 2.025175\n",
      "iteration 1100 / 1500: loss 2.052422\n",
      "iteration 1200 / 1500: loss 2.046321\n",
      "iteration 1300 / 1500: loss 2.032111\n",
      "iteration 1400 / 1500: loss 2.060593\n",
      "iteration 0 / 1500: loss 931.178027\n",
      "iteration 100 / 1500: loss 2.519975\n",
      "iteration 200 / 1500: loss 2.119707\n",
      "iteration 300 / 1500: loss 2.051788\n",
      "iteration 400 / 1500: loss 2.049482\n",
      "iteration 500 / 1500: loss 2.072590\n",
      "iteration 600 / 1500: loss 2.118816\n",
      "iteration 700 / 1500: loss 2.087537\n",
      "iteration 800 / 1500: loss 2.080591\n",
      "iteration 900 / 1500: loss 2.051135\n",
      "iteration 1000 / 1500: loss 2.116034\n",
      "iteration 1100 / 1500: loss 2.069342\n",
      "iteration 1200 / 1500: loss 2.059557\n",
      "iteration 1300 / 1500: loss 2.110823\n",
      "iteration 1400 / 1500: loss 2.123931\n",
      "iteration 0 / 1500: loss 956.184180\n",
      "iteration 100 / 1500: loss 2.425444\n",
      "iteration 200 / 1500: loss 2.053454\n",
      "iteration 300 / 1500: loss 2.074403\n",
      "iteration 400 / 1500: loss 2.067812\n",
      "iteration 500 / 1500: loss 2.099926\n",
      "iteration 600 / 1500: loss 2.155161\n",
      "iteration 700 / 1500: loss 2.049757\n",
      "iteration 800 / 1500: loss 2.048112\n",
      "iteration 900 / 1500: loss 2.080088\n",
      "iteration 1000 / 1500: loss 2.058578\n",
      "iteration 1100 / 1500: loss 2.074803\n",
      "iteration 1200 / 1500: loss 2.071655\n",
      "iteration 1300 / 1500: loss 2.111795\n",
      "iteration 1400 / 1500: loss 2.073392\n",
      "iteration 0 / 1500: loss 988.346717\n",
      "iteration 100 / 1500: loss 2.350432\n",
      "iteration 200 / 1500: loss 2.113899\n",
      "iteration 300 / 1500: loss 2.116884\n",
      "iteration 400 / 1500: loss 2.063323\n",
      "iteration 500 / 1500: loss 2.040310\n",
      "iteration 600 / 1500: loss 2.096142\n",
      "iteration 700 / 1500: loss 2.080263\n",
      "iteration 800 / 1500: loss 2.114050\n",
      "iteration 900 / 1500: loss 2.132668\n",
      "iteration 1000 / 1500: loss 2.070823\n",
      "iteration 1100 / 1500: loss 2.074400\n",
      "iteration 1200 / 1500: loss 2.045741\n",
      "iteration 1300 / 1500: loss 2.159330\n",
      "iteration 1400 / 1500: loss 2.091705\n",
      "iteration 0 / 1500: loss 1027.561591\n",
      "iteration 100 / 1500: loss 2.324808\n",
      "iteration 200 / 1500: loss 2.082320\n",
      "iteration 300 / 1500: loss 2.131197\n",
      "iteration 400 / 1500: loss 2.067290\n",
      "iteration 500 / 1500: loss 2.104769\n",
      "iteration 600 / 1500: loss 2.096784\n",
      "iteration 700 / 1500: loss 2.125152\n",
      "iteration 800 / 1500: loss 2.151346\n",
      "iteration 900 / 1500: loss 2.085623\n",
      "iteration 1000 / 1500: loss 2.128592\n",
      "iteration 1100 / 1500: loss 2.089708\n",
      "iteration 1200 / 1500: loss 2.033263\n",
      "iteration 1300 / 1500: loss 2.142766\n",
      "iteration 1400 / 1500: loss 2.081120\n",
      "iteration 0 / 1500: loss 1048.082735\n",
      "iteration 100 / 1500: loss 2.191863\n",
      "iteration 200 / 1500: loss 2.045032\n",
      "iteration 300 / 1500: loss 2.102347\n",
      "iteration 400 / 1500: loss 2.053394\n",
      "iteration 500 / 1500: loss 2.095242\n",
      "iteration 600 / 1500: loss 2.151587\n",
      "iteration 700 / 1500: loss 2.135321\n",
      "iteration 800 / 1500: loss 2.105483\n",
      "iteration 900 / 1500: loss 2.089441\n",
      "iteration 1000 / 1500: loss 2.105126\n",
      "iteration 1100 / 1500: loss 2.058417\n",
      "iteration 1200 / 1500: loss 2.054227\n",
      "iteration 1300 / 1500: loss 2.086951\n",
      "iteration 1400 / 1500: loss 2.107565\n",
      "iteration 0 / 1500: loss 1082.127553\n",
      "iteration 100 / 1500: loss 2.230830\n",
      "iteration 200 / 1500: loss 2.101109\n",
      "iteration 300 / 1500: loss 2.054514\n",
      "iteration 400 / 1500: loss 2.053297\n",
      "iteration 500 / 1500: loss 2.085946\n",
      "iteration 600 / 1500: loss 2.055796\n",
      "iteration 700 / 1500: loss 2.084886\n",
      "iteration 800 / 1500: loss 2.046543\n",
      "iteration 900 / 1500: loss 2.058881\n",
      "iteration 1000 / 1500: loss 2.137374\n",
      "iteration 1100 / 1500: loss 2.098808\n",
      "iteration 1200 / 1500: loss 2.112304\n",
      "iteration 1300 / 1500: loss 2.060391\n",
      "iteration 1400 / 1500: loss 2.115793\n",
      "iteration 0 / 1500: loss 1112.578916\n",
      "iteration 100 / 1500: loss 2.189668\n",
      "iteration 200 / 1500: loss 2.100017\n",
      "iteration 300 / 1500: loss 2.117331\n",
      "iteration 400 / 1500: loss 2.114147\n",
      "iteration 500 / 1500: loss 2.129569\n",
      "iteration 600 / 1500: loss 2.053062\n",
      "iteration 700 / 1500: loss 2.118776\n",
      "iteration 800 / 1500: loss 2.065416\n",
      "iteration 900 / 1500: loss 2.145113\n",
      "iteration 1000 / 1500: loss 2.092193\n",
      "iteration 1100 / 1500: loss 2.095375\n",
      "iteration 1200 / 1500: loss 2.091512\n",
      "iteration 1300 / 1500: loss 2.073973\n",
      "iteration 1400 / 1500: loss 2.056218\n",
      "iteration 0 / 1500: loss 1128.187243\n",
      "iteration 100 / 1500: loss 2.194483\n",
      "iteration 200 / 1500: loss 2.182891\n",
      "iteration 300 / 1500: loss 2.091708\n",
      "iteration 400 / 1500: loss 2.050392\n",
      "iteration 500 / 1500: loss 2.090514\n",
      "iteration 600 / 1500: loss 2.157396\n",
      "iteration 700 / 1500: loss 2.087534\n",
      "iteration 800 / 1500: loss 2.101685\n",
      "iteration 900 / 1500: loss 2.101231\n",
      "iteration 1000 / 1500: loss 2.086705\n",
      "iteration 1100 / 1500: loss 2.113858\n",
      "iteration 1200 / 1500: loss 2.119372\n",
      "iteration 1300 / 1500: loss 2.027779\n",
      "iteration 1400 / 1500: loss 2.090373\n",
      "iteration 0 / 1500: loss 1179.120231\n",
      "iteration 100 / 1500: loss 2.144937\n",
      "iteration 200 / 1500: loss 2.093688\n",
      "iteration 300 / 1500: loss 2.123293\n",
      "iteration 400 / 1500: loss 2.118369\n",
      "iteration 500 / 1500: loss 2.075411\n",
      "iteration 600 / 1500: loss 2.109294\n",
      "iteration 700 / 1500: loss 2.082179\n",
      "iteration 800 / 1500: loss 2.062830\n",
      "iteration 900 / 1500: loss 2.134799\n",
      "iteration 1000 / 1500: loss 2.040854\n",
      "iteration 1100 / 1500: loss 2.099927\n",
      "iteration 1200 / 1500: loss 2.068866\n",
      "iteration 1300 / 1500: loss 2.025799\n",
      "iteration 1400 / 1500: loss 2.114872\n",
      "iteration 0 / 1500: loss 1196.161670\n",
      "iteration 100 / 1500: loss 2.166762\n",
      "iteration 200 / 1500: loss 2.105075\n",
      "iteration 300 / 1500: loss 2.120572\n",
      "iteration 400 / 1500: loss 2.081779\n",
      "iteration 500 / 1500: loss 2.098629\n",
      "iteration 600 / 1500: loss 2.091092\n",
      "iteration 700 / 1500: loss 2.085101\n",
      "iteration 800 / 1500: loss 2.139114\n",
      "iteration 900 / 1500: loss 2.148446\n",
      "iteration 1000 / 1500: loss 2.131627\n",
      "iteration 1100 / 1500: loss 2.129680\n",
      "iteration 1200 / 1500: loss 2.114449\n",
      "iteration 1300 / 1500: loss 2.119832\n",
      "iteration 1400 / 1500: loss 2.094517\n",
      "iteration 0 / 1500: loss 1229.135616\n",
      "iteration 100 / 1500: loss 2.159430\n",
      "iteration 200 / 1500: loss 2.126005\n",
      "iteration 300 / 1500: loss 2.121104\n",
      "iteration 400 / 1500: loss 2.081650\n",
      "iteration 500 / 1500: loss 2.119683\n",
      "iteration 600 / 1500: loss 2.074651\n",
      "iteration 700 / 1500: loss 2.116716\n",
      "iteration 800 / 1500: loss 2.105760\n",
      "iteration 900 / 1500: loss 2.102984\n",
      "iteration 1000 / 1500: loss 2.122618\n",
      "iteration 1100 / 1500: loss 2.128290\n",
      "iteration 1200 / 1500: loss 2.163063\n",
      "iteration 1300 / 1500: loss 2.061763\n",
      "iteration 1400 / 1500: loss 2.121439\n",
      "iteration 0 / 1500: loss 1284.415399\n",
      "iteration 100 / 1500: loss 2.125601\n",
      "iteration 200 / 1500: loss 2.126544\n",
      "iteration 300 / 1500: loss 2.136752\n",
      "iteration 400 / 1500: loss 2.067003\n",
      "iteration 500 / 1500: loss 2.088077\n",
      "iteration 600 / 1500: loss 2.132253\n",
      "iteration 700 / 1500: loss 2.075105\n",
      "iteration 800 / 1500: loss 2.158888\n",
      "iteration 900 / 1500: loss 2.090728\n",
      "iteration 1000 / 1500: loss 2.121498\n",
      "iteration 1100 / 1500: loss 2.156674\n",
      "iteration 1200 / 1500: loss 2.039466\n",
      "iteration 1300 / 1500: loss 2.097634\n",
      "iteration 1400 / 1500: loss 2.109381\n",
      "iteration 0 / 1500: loss 1301.406862\n",
      "iteration 100 / 1500: loss 2.163504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss 2.129089\n",
      "iteration 300 / 1500: loss 2.169580\n",
      "iteration 400 / 1500: loss 2.080425\n",
      "iteration 500 / 1500: loss 2.113698\n",
      "iteration 600 / 1500: loss 2.121296\n",
      "iteration 700 / 1500: loss 2.098989\n",
      "iteration 800 / 1500: loss 2.097949\n",
      "iteration 900 / 1500: loss 2.083440\n",
      "iteration 1000 / 1500: loss 2.081334\n",
      "iteration 1100 / 1500: loss 2.048393\n",
      "iteration 1200 / 1500: loss 2.075433\n",
      "iteration 1300 / 1500: loss 2.082639\n",
      "iteration 1400 / 1500: loss 2.118735\n",
      "iteration 0 / 1500: loss 1342.603042\n",
      "iteration 100 / 1500: loss 2.157176\n",
      "iteration 200 / 1500: loss 2.062470\n",
      "iteration 300 / 1500: loss 2.120785\n",
      "iteration 400 / 1500: loss 2.138901\n",
      "iteration 500 / 1500: loss 2.094963\n",
      "iteration 600 / 1500: loss 2.130803\n",
      "iteration 700 / 1500: loss 2.173212\n",
      "iteration 800 / 1500: loss 2.090676\n",
      "iteration 900 / 1500: loss 2.090619\n",
      "iteration 1000 / 1500: loss 2.155966\n",
      "iteration 1100 / 1500: loss 2.114272\n",
      "iteration 1200 / 1500: loss 2.109125\n",
      "iteration 1300 / 1500: loss 2.132586\n",
      "iteration 1400 / 1500: loss 2.165330\n",
      "iteration 0 / 1500: loss 1340.284696\n",
      "iteration 100 / 1500: loss 2.152633\n",
      "iteration 200 / 1500: loss 2.100054\n",
      "iteration 300 / 1500: loss 2.120636\n",
      "iteration 400 / 1500: loss 2.142493\n",
      "iteration 500 / 1500: loss 2.133033\n",
      "iteration 600 / 1500: loss 2.120923\n",
      "iteration 700 / 1500: loss 2.112510\n",
      "iteration 800 / 1500: loss 2.151495\n",
      "iteration 900 / 1500: loss 2.144509\n",
      "iteration 1000 / 1500: loss 2.147778\n",
      "iteration 1100 / 1500: loss 2.113504\n",
      "iteration 1200 / 1500: loss 2.107973\n",
      "iteration 1300 / 1500: loss 2.135996\n",
      "iteration 1400 / 1500: loss 2.124262\n",
      "iteration 0 / 1500: loss 1398.165530\n",
      "iteration 100 / 1500: loss 2.135246\n",
      "iteration 200 / 1500: loss 2.150886\n",
      "iteration 300 / 1500: loss 2.109987\n",
      "iteration 400 / 1500: loss 2.106578\n",
      "iteration 500 / 1500: loss 2.064822\n",
      "iteration 600 / 1500: loss 2.075658\n",
      "iteration 700 / 1500: loss 2.156338\n",
      "iteration 800 / 1500: loss 2.125223\n",
      "iteration 900 / 1500: loss 2.134319\n",
      "iteration 1000 / 1500: loss 2.128440\n",
      "iteration 1100 / 1500: loss 2.151138\n",
      "iteration 1200 / 1500: loss 2.162171\n",
      "iteration 1300 / 1500: loss 2.128535\n",
      "iteration 1400 / 1500: loss 2.109824\n",
      "iteration 0 / 1500: loss 1407.637035\n",
      "iteration 100 / 1500: loss 2.117956\n",
      "iteration 200 / 1500: loss 2.135308\n",
      "iteration 300 / 1500: loss 2.134763\n",
      "iteration 400 / 1500: loss 2.107523\n",
      "iteration 500 / 1500: loss 2.150799\n",
      "iteration 600 / 1500: loss 2.142223\n",
      "iteration 700 / 1500: loss 2.120792\n",
      "iteration 800 / 1500: loss 2.133056\n",
      "iteration 900 / 1500: loss 2.182613\n",
      "iteration 1000 / 1500: loss 2.098876\n",
      "iteration 1100 / 1500: loss 2.107744\n",
      "iteration 1200 / 1500: loss 2.045222\n",
      "iteration 1300 / 1500: loss 2.201945\n",
      "iteration 1400 / 1500: loss 2.077051\n",
      "iteration 0 / 1500: loss 1447.824960\n",
      "iteration 100 / 1500: loss 2.159366\n",
      "iteration 200 / 1500: loss 2.103441\n",
      "iteration 300 / 1500: loss 2.114517\n",
      "iteration 400 / 1500: loss 2.123392\n",
      "iteration 500 / 1500: loss 2.121103\n",
      "iteration 600 / 1500: loss 2.175218\n",
      "iteration 700 / 1500: loss 2.092545\n",
      "iteration 800 / 1500: loss 2.127783\n",
      "iteration 900 / 1500: loss 2.093521\n",
      "iteration 1000 / 1500: loss 2.170968\n",
      "iteration 1100 / 1500: loss 2.111087\n",
      "iteration 1200 / 1500: loss 2.093671\n",
      "iteration 1300 / 1500: loss 2.116556\n",
      "iteration 1400 / 1500: loss 2.028251\n",
      "iteration 0 / 1500: loss 1477.474637\n",
      "iteration 100 / 1500: loss 2.158237\n",
      "iteration 200 / 1500: loss 2.145334\n",
      "iteration 300 / 1500: loss 2.045894\n",
      "iteration 400 / 1500: loss 2.178976\n",
      "iteration 500 / 1500: loss 2.092336\n",
      "iteration 600 / 1500: loss 2.169841\n",
      "iteration 700 / 1500: loss 2.121633\n",
      "iteration 800 / 1500: loss 2.158361\n",
      "iteration 900 / 1500: loss 2.108017\n",
      "iteration 1000 / 1500: loss 2.101507\n",
      "iteration 1100 / 1500: loss 2.154427\n",
      "iteration 1200 / 1500: loss 2.075621\n",
      "iteration 1300 / 1500: loss 2.142796\n",
      "iteration 1400 / 1500: loss 2.073364\n",
      "iteration 0 / 1500: loss 1515.015653\n",
      "iteration 100 / 1500: loss 2.128881\n",
      "iteration 200 / 1500: loss 2.135382\n",
      "iteration 300 / 1500: loss 2.131420\n",
      "iteration 400 / 1500: loss 2.112488\n",
      "iteration 500 / 1500: loss 2.112101\n",
      "iteration 600 / 1500: loss 2.165374\n",
      "iteration 700 / 1500: loss 2.129738\n",
      "iteration 800 / 1500: loss 2.116759\n",
      "iteration 900 / 1500: loss 2.129848\n",
      "iteration 1000 / 1500: loss 2.119614\n",
      "iteration 1100 / 1500: loss 2.174605\n",
      "iteration 1200 / 1500: loss 2.125575\n",
      "iteration 1300 / 1500: loss 2.083568\n",
      "iteration 1400 / 1500: loss 2.163440\n",
      "iteration 0 / 1500: loss 1523.784774\n",
      "iteration 100 / 1500: loss 2.130551\n",
      "iteration 200 / 1500: loss 2.161105\n",
      "iteration 300 / 1500: loss 2.160424\n",
      "iteration 400 / 1500: loss 2.135547\n",
      "iteration 500 / 1500: loss 2.151361\n",
      "iteration 600 / 1500: loss 2.125419\n",
      "iteration 700 / 1500: loss 2.140547\n",
      "iteration 800 / 1500: loss 2.144178\n",
      "iteration 900 / 1500: loss 2.052236\n",
      "iteration 1000 / 1500: loss 2.158269\n",
      "iteration 1100 / 1500: loss 2.142744\n",
      "iteration 1200 / 1500: loss 2.095014\n",
      "iteration 1300 / 1500: loss 2.139033\n",
      "iteration 1400 / 1500: loss 2.158560\n",
      "iteration 0 / 1500: loss 771.733128\n",
      "iteration 100 / 1500: loss 2.281242\n",
      "iteration 200 / 1500: loss 2.037118\n",
      "iteration 300 / 1500: loss 2.080703\n",
      "iteration 400 / 1500: loss 2.031122\n",
      "iteration 500 / 1500: loss 2.033538\n",
      "iteration 600 / 1500: loss 2.127622\n",
      "iteration 700 / 1500: loss 2.064978\n",
      "iteration 800 / 1500: loss 2.059989\n",
      "iteration 900 / 1500: loss 2.062841\n",
      "iteration 1000 / 1500: loss 2.078699\n",
      "iteration 1100 / 1500: loss 2.045292\n",
      "iteration 1200 / 1500: loss 2.030603\n",
      "iteration 1300 / 1500: loss 2.053843\n",
      "iteration 1400 / 1500: loss 2.160524\n",
      "iteration 0 / 1500: loss 800.531880\n",
      "iteration 100 / 1500: loss 2.258294\n",
      "iteration 200 / 1500: loss 2.076482\n",
      "iteration 300 / 1500: loss 2.036196\n",
      "iteration 400 / 1500: loss 2.035814\n",
      "iteration 500 / 1500: loss 2.068567\n",
      "iteration 600 / 1500: loss 2.077410\n",
      "iteration 700 / 1500: loss 2.034818\n",
      "iteration 800 / 1500: loss 2.063051\n",
      "iteration 900 / 1500: loss 2.110080\n",
      "iteration 1000 / 1500: loss 2.097981\n",
      "iteration 1100 / 1500: loss 2.011724\n",
      "iteration 1200 / 1500: loss 2.100402\n",
      "iteration 1300 / 1500: loss 2.136288\n",
      "iteration 1400 / 1500: loss 2.019636\n",
      "iteration 0 / 1500: loss 833.260651\n",
      "iteration 100 / 1500: loss 2.177166\n",
      "iteration 200 / 1500: loss 2.100169\n",
      "iteration 300 / 1500: loss 2.045367\n",
      "iteration 400 / 1500: loss 2.045120\n",
      "iteration 500 / 1500: loss 2.113986\n",
      "iteration 600 / 1500: loss 2.010149\n",
      "iteration 700 / 1500: loss 2.094065\n",
      "iteration 800 / 1500: loss 2.086338\n",
      "iteration 900 / 1500: loss 2.049666\n",
      "iteration 1000 / 1500: loss 2.117501\n",
      "iteration 1100 / 1500: loss 2.060521\n",
      "iteration 1200 / 1500: loss 2.025862\n",
      "iteration 1300 / 1500: loss 2.057438\n",
      "iteration 1400 / 1500: loss 2.081241\n",
      "iteration 0 / 1500: loss 862.914710\n",
      "iteration 100 / 1500: loss 2.125830\n",
      "iteration 200 / 1500: loss 2.072734\n",
      "iteration 300 / 1500: loss 2.123931\n",
      "iteration 400 / 1500: loss 2.108785\n",
      "iteration 500 / 1500: loss 2.042430\n",
      "iteration 600 / 1500: loss 2.117581\n",
      "iteration 700 / 1500: loss 2.094342\n",
      "iteration 800 / 1500: loss 2.095271\n",
      "iteration 900 / 1500: loss 2.096672\n",
      "iteration 1000 / 1500: loss 2.066415\n",
      "iteration 1100 / 1500: loss 2.119025\n",
      "iteration 1200 / 1500: loss 2.095416\n",
      "iteration 1300 / 1500: loss 1.996449\n",
      "iteration 1400 / 1500: loss 2.107852\n",
      "iteration 0 / 1500: loss 894.357077\n",
      "iteration 100 / 1500: loss 2.158150\n",
      "iteration 200 / 1500: loss 2.068172\n",
      "iteration 300 / 1500: loss 2.055746\n",
      "iteration 400 / 1500: loss 2.077901\n",
      "iteration 500 / 1500: loss 2.047465\n",
      "iteration 600 / 1500: loss 2.060213\n",
      "iteration 700 / 1500: loss 2.090849\n",
      "iteration 800 / 1500: loss 2.082816\n",
      "iteration 900 / 1500: loss 2.042519\n",
      "iteration 1000 / 1500: loss 2.073228\n",
      "iteration 1100 / 1500: loss 2.130973\n",
      "iteration 1200 / 1500: loss 2.002389\n",
      "iteration 1300 / 1500: loss 2.030146\n",
      "iteration 1400 / 1500: loss 2.062778\n",
      "iteration 0 / 1500: loss 921.002565\n",
      "iteration 100 / 1500: loss 2.120622\n",
      "iteration 200 / 1500: loss 2.108720\n",
      "iteration 300 / 1500: loss 2.120965\n",
      "iteration 400 / 1500: loss 2.052527\n",
      "iteration 500 / 1500: loss 2.063305\n",
      "iteration 600 / 1500: loss 2.069268\n",
      "iteration 700 / 1500: loss 2.096086\n",
      "iteration 800 / 1500: loss 2.092017\n",
      "iteration 900 / 1500: loss 2.054587\n",
      "iteration 1000 / 1500: loss 2.002572\n",
      "iteration 1100 / 1500: loss 2.070258\n",
      "iteration 1200 / 1500: loss 2.082932\n",
      "iteration 1300 / 1500: loss 2.049846\n",
      "iteration 1400 / 1500: loss 2.022139\n",
      "iteration 0 / 1500: loss 970.212008\n",
      "iteration 100 / 1500: loss 2.132914\n",
      "iteration 200 / 1500: loss 2.061235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 1500: loss 2.076280\n",
      "iteration 400 / 1500: loss 2.113769\n",
      "iteration 500 / 1500: loss 2.114534\n",
      "iteration 600 / 1500: loss 2.117303\n",
      "iteration 700 / 1500: loss 2.094395\n",
      "iteration 800 / 1500: loss 2.128485\n",
      "iteration 900 / 1500: loss 2.066443\n",
      "iteration 1000 / 1500: loss 2.050036\n",
      "iteration 1100 / 1500: loss 2.080525\n",
      "iteration 1200 / 1500: loss 2.051317\n",
      "iteration 1300 / 1500: loss 2.082067\n",
      "iteration 1400 / 1500: loss 2.024756\n",
      "iteration 0 / 1500: loss 978.695944\n",
      "iteration 100 / 1500: loss 2.098553\n",
      "iteration 200 / 1500: loss 2.090715\n",
      "iteration 300 / 1500: loss 2.119644\n",
      "iteration 400 / 1500: loss 2.107542\n",
      "iteration 500 / 1500: loss 2.065764\n",
      "iteration 600 / 1500: loss 2.070316\n",
      "iteration 700 / 1500: loss 2.069194\n",
      "iteration 800 / 1500: loss 2.094242\n",
      "iteration 900 / 1500: loss 2.044448\n",
      "iteration 1000 / 1500: loss 2.126590\n",
      "iteration 1100 / 1500: loss 2.137957\n",
      "iteration 1200 / 1500: loss 2.066479\n",
      "iteration 1300 / 1500: loss 2.051874\n",
      "iteration 1400 / 1500: loss 2.106924\n",
      "iteration 0 / 1500: loss 1037.588709\n",
      "iteration 100 / 1500: loss 2.122852\n",
      "iteration 200 / 1500: loss 2.072770\n",
      "iteration 300 / 1500: loss 2.105322\n",
      "iteration 400 / 1500: loss 2.143312\n",
      "iteration 500 / 1500: loss 2.083249\n",
      "iteration 600 / 1500: loss 2.112127\n",
      "iteration 700 / 1500: loss 2.137763\n",
      "iteration 800 / 1500: loss 2.066496\n",
      "iteration 900 / 1500: loss 2.097536\n",
      "iteration 1000 / 1500: loss 2.042525\n",
      "iteration 1100 / 1500: loss 2.052792\n",
      "iteration 1200 / 1500: loss 2.102854\n",
      "iteration 1300 / 1500: loss 2.071662\n",
      "iteration 1400 / 1500: loss 2.074579\n",
      "iteration 0 / 1500: loss 1049.607328\n",
      "iteration 100 / 1500: loss 2.140973\n",
      "iteration 200 / 1500: loss 2.130531\n",
      "iteration 300 / 1500: loss 2.107482\n",
      "iteration 400 / 1500: loss 2.085962\n",
      "iteration 500 / 1500: loss 2.089754\n",
      "iteration 600 / 1500: loss 2.051961\n",
      "iteration 700 / 1500: loss 2.127193\n",
      "iteration 800 / 1500: loss 2.060854\n",
      "iteration 900 / 1500: loss 2.137817\n",
      "iteration 1000 / 1500: loss 2.063491\n",
      "iteration 1100 / 1500: loss 2.083454\n",
      "iteration 1200 / 1500: loss 2.033843\n",
      "iteration 1300 / 1500: loss 2.073274\n",
      "iteration 1400 / 1500: loss 2.080900\n",
      "iteration 0 / 1500: loss 1062.807308\n",
      "iteration 100 / 1500: loss 2.116257\n",
      "iteration 200 / 1500: loss 2.119681\n",
      "iteration 300 / 1500: loss 2.130917\n",
      "iteration 400 / 1500: loss 2.080549\n",
      "iteration 500 / 1500: loss 2.068087\n",
      "iteration 600 / 1500: loss 2.093166\n",
      "iteration 700 / 1500: loss 2.096742\n",
      "iteration 800 / 1500: loss 2.071097\n",
      "iteration 900 / 1500: loss 2.069743\n",
      "iteration 1000 / 1500: loss 2.147434\n",
      "iteration 1100 / 1500: loss 2.083202\n",
      "iteration 1200 / 1500: loss 2.092695\n",
      "iteration 1300 / 1500: loss 2.144625\n",
      "iteration 1400 / 1500: loss 2.081293\n",
      "iteration 0 / 1500: loss 1104.517762\n",
      "iteration 100 / 1500: loss 2.148915\n",
      "iteration 200 / 1500: loss 2.095524\n",
      "iteration 300 / 1500: loss 2.081495\n",
      "iteration 400 / 1500: loss 2.112270\n",
      "iteration 500 / 1500: loss 2.119066\n",
      "iteration 600 / 1500: loss 2.112991\n",
      "iteration 700 / 1500: loss 2.088434\n",
      "iteration 800 / 1500: loss 2.101437\n",
      "iteration 900 / 1500: loss 2.060961\n",
      "iteration 1000 / 1500: loss 2.102974\n",
      "iteration 1100 / 1500: loss 2.076910\n",
      "iteration 1200 / 1500: loss 2.108722\n",
      "iteration 1300 / 1500: loss 2.061280\n",
      "iteration 1400 / 1500: loss 2.064703\n",
      "iteration 0 / 1500: loss 1133.993413\n",
      "iteration 100 / 1500: loss 2.163807\n",
      "iteration 200 / 1500: loss 2.158779\n",
      "iteration 300 / 1500: loss 2.056905\n",
      "iteration 400 / 1500: loss 2.079337\n",
      "iteration 500 / 1500: loss 2.112326\n",
      "iteration 600 / 1500: loss 2.111809\n",
      "iteration 700 / 1500: loss 2.121555\n",
      "iteration 800 / 1500: loss 2.097040\n",
      "iteration 900 / 1500: loss 2.122909\n",
      "iteration 1000 / 1500: loss 2.116673\n",
      "iteration 1100 / 1500: loss 2.109336\n",
      "iteration 1200 / 1500: loss 2.069729\n",
      "iteration 1300 / 1500: loss 2.161543\n",
      "iteration 1400 / 1500: loss 2.141337\n",
      "iteration 0 / 1500: loss 1190.688271\n",
      "iteration 100 / 1500: loss 2.096944\n",
      "iteration 200 / 1500: loss 2.103030\n",
      "iteration 300 / 1500: loss 2.151285\n",
      "iteration 400 / 1500: loss 2.094549\n",
      "iteration 500 / 1500: loss 2.090428\n",
      "iteration 600 / 1500: loss 2.111294\n",
      "iteration 700 / 1500: loss 2.116363\n",
      "iteration 800 / 1500: loss 2.140022\n",
      "iteration 900 / 1500: loss 2.137620\n",
      "iteration 1000 / 1500: loss 2.081126\n",
      "iteration 1100 / 1500: loss 2.116210\n",
      "iteration 1200 / 1500: loss 2.089134\n",
      "iteration 1300 / 1500: loss 2.129436\n",
      "iteration 1400 / 1500: loss 2.157370\n",
      "iteration 0 / 1500: loss 1214.849439\n",
      "iteration 100 / 1500: loss 2.053594\n",
      "iteration 200 / 1500: loss 2.099242\n",
      "iteration 300 / 1500: loss 2.095725\n",
      "iteration 400 / 1500: loss 2.142273\n",
      "iteration 500 / 1500: loss 2.090692\n",
      "iteration 600 / 1500: loss 2.044871\n",
      "iteration 700 / 1500: loss 2.098237\n",
      "iteration 800 / 1500: loss 2.078600\n",
      "iteration 900 / 1500: loss 2.101272\n",
      "iteration 1000 / 1500: loss 2.053723\n",
      "iteration 1100 / 1500: loss 2.111061\n",
      "iteration 1200 / 1500: loss 2.117450\n",
      "iteration 1300 / 1500: loss 2.088772\n",
      "iteration 1400 / 1500: loss 2.085431\n",
      "iteration 0 / 1500: loss 1225.217740\n",
      "iteration 100 / 1500: loss 2.112415\n",
      "iteration 200 / 1500: loss 2.087770\n",
      "iteration 300 / 1500: loss 2.052690\n",
      "iteration 400 / 1500: loss 2.091821\n",
      "iteration 500 / 1500: loss 2.105614\n",
      "iteration 600 / 1500: loss 2.139008\n",
      "iteration 700 / 1500: loss 2.064176\n",
      "iteration 800 / 1500: loss 2.082972\n",
      "iteration 900 / 1500: loss 2.060282\n",
      "iteration 1000 / 1500: loss 2.084885\n",
      "iteration 1100 / 1500: loss 2.127285\n",
      "iteration 1200 / 1500: loss 2.131634\n",
      "iteration 1300 / 1500: loss 2.093325\n",
      "iteration 1400 / 1500: loss 2.076527\n",
      "iteration 0 / 1500: loss 1250.773185\n",
      "iteration 100 / 1500: loss 2.135564\n",
      "iteration 200 / 1500: loss 2.088222\n",
      "iteration 300 / 1500: loss 2.113698\n",
      "iteration 400 / 1500: loss 2.105163\n",
      "iteration 500 / 1500: loss 2.095869\n",
      "iteration 600 / 1500: loss 2.079076\n",
      "iteration 700 / 1500: loss 2.058325\n",
      "iteration 800 / 1500: loss 2.086564\n",
      "iteration 900 / 1500: loss 2.082916\n",
      "iteration 1000 / 1500: loss 2.162954\n",
      "iteration 1100 / 1500: loss 2.119595\n",
      "iteration 1200 / 1500: loss 2.092192\n",
      "iteration 1300 / 1500: loss 2.106264\n",
      "iteration 1400 / 1500: loss 2.102332\n",
      "iteration 0 / 1500: loss 1295.173936\n",
      "iteration 100 / 1500: loss 2.091181\n",
      "iteration 200 / 1500: loss 2.140030\n",
      "iteration 300 / 1500: loss 2.155107\n",
      "iteration 400 / 1500: loss 2.109610\n",
      "iteration 500 / 1500: loss 2.118997\n",
      "iteration 600 / 1500: loss 2.087264\n",
      "iteration 700 / 1500: loss 2.114495\n",
      "iteration 800 / 1500: loss 2.089742\n",
      "iteration 900 / 1500: loss 2.054462\n",
      "iteration 1000 / 1500: loss 2.150961\n",
      "iteration 1100 / 1500: loss 2.101674\n",
      "iteration 1200 / 1500: loss 2.150197\n",
      "iteration 1300 / 1500: loss 2.142209\n",
      "iteration 1400 / 1500: loss 2.160931\n",
      "iteration 0 / 1500: loss 1316.433607\n",
      "iteration 100 / 1500: loss 2.122327\n",
      "iteration 200 / 1500: loss 2.111045\n",
      "iteration 300 / 1500: loss 2.147556\n",
      "iteration 400 / 1500: loss 2.090664\n",
      "iteration 500 / 1500: loss 2.099529\n",
      "iteration 600 / 1500: loss 2.091840\n",
      "iteration 700 / 1500: loss 2.132443\n",
      "iteration 800 / 1500: loss 2.127316\n",
      "iteration 900 / 1500: loss 2.149609\n",
      "iteration 1000 / 1500: loss 2.131237\n",
      "iteration 1100 / 1500: loss 2.143736\n",
      "iteration 1200 / 1500: loss 2.134607\n",
      "iteration 1300 / 1500: loss 2.183954\n",
      "iteration 1400 / 1500: loss 2.146146\n",
      "iteration 0 / 1500: loss 1341.242246\n",
      "iteration 100 / 1500: loss 2.157194\n",
      "iteration 200 / 1500: loss 2.110613\n",
      "iteration 300 / 1500: loss 2.143032\n",
      "iteration 400 / 1500: loss 2.126384\n",
      "iteration 500 / 1500: loss 2.145143\n",
      "iteration 600 / 1500: loss 2.146091\n",
      "iteration 700 / 1500: loss 2.117654\n",
      "iteration 800 / 1500: loss 2.124226\n",
      "iteration 900 / 1500: loss 2.104614\n",
      "iteration 1000 / 1500: loss 2.150302\n",
      "iteration 1100 / 1500: loss 2.119364\n",
      "iteration 1200 / 1500: loss 2.072653\n",
      "iteration 1300 / 1500: loss 2.075081\n",
      "iteration 1400 / 1500: loss 2.160571\n",
      "iteration 0 / 1500: loss 1381.751815\n",
      "iteration 100 / 1500: loss 2.126121\n",
      "iteration 200 / 1500: loss 2.092678\n",
      "iteration 300 / 1500: loss 2.123802\n",
      "iteration 400 / 1500: loss 2.121494\n",
      "iteration 500 / 1500: loss 2.101148\n",
      "iteration 600 / 1500: loss 2.083761\n",
      "iteration 700 / 1500: loss 2.168376\n",
      "iteration 800 / 1500: loss 2.109777\n",
      "iteration 900 / 1500: loss 2.165927\n",
      "iteration 1000 / 1500: loss 2.161058\n",
      "iteration 1100 / 1500: loss 2.136618\n",
      "iteration 1200 / 1500: loss 2.111891\n",
      "iteration 1300 / 1500: loss 2.095613\n",
      "iteration 1400 / 1500: loss 2.124653\n",
      "iteration 0 / 1500: loss 1435.691678\n",
      "iteration 100 / 1500: loss 2.123340\n",
      "iteration 200 / 1500: loss 2.141622\n",
      "iteration 300 / 1500: loss 2.206439\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 1500: loss 2.092170\n",
      "iteration 500 / 1500: loss 2.115189\n",
      "iteration 600 / 1500: loss 2.113705\n",
      "iteration 700 / 1500: loss 2.136211\n",
      "iteration 800 / 1500: loss 2.085116\n",
      "iteration 900 / 1500: loss 2.077655\n",
      "iteration 1000 / 1500: loss 2.165106\n",
      "iteration 1100 / 1500: loss 2.131474\n",
      "iteration 1200 / 1500: loss 2.104995\n",
      "iteration 1300 / 1500: loss 2.157673\n",
      "iteration 1400 / 1500: loss 2.122320\n",
      "iteration 0 / 1500: loss 1459.606129\n",
      "iteration 100 / 1500: loss 2.108651\n",
      "iteration 200 / 1500: loss 2.156649\n",
      "iteration 300 / 1500: loss 2.094623\n",
      "iteration 400 / 1500: loss 2.203966\n",
      "iteration 500 / 1500: loss 2.109797\n",
      "iteration 600 / 1500: loss 2.136962\n",
      "iteration 700 / 1500: loss 2.151149\n",
      "iteration 800 / 1500: loss 2.120663\n",
      "iteration 900 / 1500: loss 2.142199\n",
      "iteration 1000 / 1500: loss 2.081744\n",
      "iteration 1100 / 1500: loss 2.093298\n",
      "iteration 1200 / 1500: loss 2.077679\n",
      "iteration 1300 / 1500: loss 2.111537\n",
      "iteration 1400 / 1500: loss 2.152694\n",
      "iteration 0 / 1500: loss 1493.931651\n",
      "iteration 100 / 1500: loss 2.186724\n",
      "iteration 200 / 1500: loss 2.113251\n",
      "iteration 300 / 1500: loss 2.162015\n",
      "iteration 400 / 1500: loss 2.168903\n",
      "iteration 500 / 1500: loss 2.121251\n",
      "iteration 600 / 1500: loss 2.109044\n",
      "iteration 700 / 1500: loss 2.093469\n",
      "iteration 800 / 1500: loss 2.134371\n",
      "iteration 900 / 1500: loss 2.154333\n",
      "iteration 1000 / 1500: loss 2.127862\n",
      "iteration 1100 / 1500: loss 2.098898\n",
      "iteration 1200 / 1500: loss 2.095720\n",
      "iteration 1300 / 1500: loss 2.134028\n",
      "iteration 1400 / 1500: loss 2.101218\n",
      "iteration 0 / 1500: loss 1526.824144\n",
      "iteration 100 / 1500: loss 2.118676\n",
      "iteration 200 / 1500: loss 2.113800\n",
      "iteration 300 / 1500: loss 2.126419\n",
      "iteration 400 / 1500: loss 2.159407\n",
      "iteration 500 / 1500: loss 2.131817\n",
      "iteration 600 / 1500: loss 2.096021\n",
      "iteration 700 / 1500: loss 2.088955\n",
      "iteration 800 / 1500: loss 2.193261\n",
      "iteration 900 / 1500: loss 2.131496\n",
      "iteration 1000 / 1500: loss 2.078261\n",
      "iteration 1100 / 1500: loss 2.149297\n",
      "iteration 1200 / 1500: loss 2.133420\n",
      "iteration 1300 / 1500: loss 2.099943\n",
      "iteration 1400 / 1500: loss 2.156019\n",
      "iteration 0 / 1500: loss 1534.303363\n",
      "iteration 100 / 1500: loss 2.151566\n",
      "iteration 200 / 1500: loss 2.101280\n",
      "iteration 300 / 1500: loss 2.146982\n",
      "iteration 400 / 1500: loss 2.129352\n",
      "iteration 500 / 1500: loss 2.080543\n",
      "iteration 600 / 1500: loss 2.183479\n",
      "iteration 700 / 1500: loss 2.092169\n",
      "iteration 800 / 1500: loss 2.096841\n",
      "iteration 900 / 1500: loss 2.116254\n",
      "iteration 1000 / 1500: loss 2.162153\n",
      "iteration 1100 / 1500: loss 2.139549\n",
      "iteration 1200 / 1500: loss 2.133080\n",
      "iteration 1300 / 1500: loss 2.129025\n",
      "iteration 1400 / 1500: loss 2.117919\n",
      "iteration 0 / 1500: loss 768.538057\n",
      "iteration 100 / 1500: loss 2.054871\n",
      "iteration 200 / 1500: loss 1.946540\n",
      "iteration 300 / 1500: loss 2.049303\n",
      "iteration 400 / 1500: loss 2.074536\n",
      "iteration 500 / 1500: loss 2.046877\n",
      "iteration 600 / 1500: loss 2.049969\n",
      "iteration 700 / 1500: loss 2.128050\n",
      "iteration 800 / 1500: loss 2.047590\n",
      "iteration 900 / 1500: loss 2.085819\n",
      "iteration 1000 / 1500: loss 2.079204\n",
      "iteration 1100 / 1500: loss 2.035476\n",
      "iteration 1200 / 1500: loss 2.055986\n",
      "iteration 1300 / 1500: loss 2.095074\n",
      "iteration 1400 / 1500: loss 1.991970\n",
      "iteration 0 / 1500: loss 802.733045\n",
      "iteration 100 / 1500: loss 2.063423\n",
      "iteration 200 / 1500: loss 2.086092\n",
      "iteration 300 / 1500: loss 1.995976\n",
      "iteration 400 / 1500: loss 2.129962\n",
      "iteration 500 / 1500: loss 2.120771\n",
      "iteration 600 / 1500: loss 2.090763\n",
      "iteration 700 / 1500: loss 2.080844\n",
      "iteration 800 / 1500: loss 2.066157\n",
      "iteration 900 / 1500: loss 2.057489\n",
      "iteration 1000 / 1500: loss 2.041782\n",
      "iteration 1100 / 1500: loss 2.041600\n",
      "iteration 1200 / 1500: loss 2.146109\n",
      "iteration 1300 / 1500: loss 2.009920\n",
      "iteration 1400 / 1500: loss 2.058013\n",
      "iteration 0 / 1500: loss 833.133304\n",
      "iteration 100 / 1500: loss 2.054218\n",
      "iteration 200 / 1500: loss 2.034073\n",
      "iteration 300 / 1500: loss 2.046129\n",
      "iteration 400 / 1500: loss 2.070939\n",
      "iteration 500 / 1500: loss 2.062143\n",
      "iteration 600 / 1500: loss 2.132257\n",
      "iteration 700 / 1500: loss 2.112928\n",
      "iteration 800 / 1500: loss 2.034292\n",
      "iteration 900 / 1500: loss 2.127231\n",
      "iteration 1000 / 1500: loss 2.077761\n",
      "iteration 1100 / 1500: loss 2.025422\n",
      "iteration 1200 / 1500: loss 2.049116\n",
      "iteration 1300 / 1500: loss 2.059994\n",
      "iteration 1400 / 1500: loss 2.096591\n",
      "iteration 0 / 1500: loss 882.005238\n",
      "iteration 100 / 1500: loss 2.109512\n",
      "iteration 200 / 1500: loss 2.107447\n",
      "iteration 300 / 1500: loss 2.073185\n",
      "iteration 400 / 1500: loss 2.089969\n",
      "iteration 500 / 1500: loss 2.027942\n",
      "iteration 600 / 1500: loss 2.059496\n",
      "iteration 700 / 1500: loss 2.049303\n",
      "iteration 800 / 1500: loss 2.105030\n",
      "iteration 900 / 1500: loss 2.114901\n",
      "iteration 1000 / 1500: loss 2.063175\n",
      "iteration 1100 / 1500: loss 2.084999\n",
      "iteration 1200 / 1500: loss 2.076524\n",
      "iteration 1300 / 1500: loss 2.018955\n",
      "iteration 1400 / 1500: loss 2.021514\n",
      "iteration 0 / 1500: loss 900.264712\n",
      "iteration 100 / 1500: loss 2.080443\n",
      "iteration 200 / 1500: loss 2.056851\n",
      "iteration 300 / 1500: loss 2.058457\n",
      "iteration 400 / 1500: loss 2.011700\n",
      "iteration 500 / 1500: loss 2.003888\n",
      "iteration 600 / 1500: loss 2.095785\n",
      "iteration 700 / 1500: loss 2.033226\n",
      "iteration 800 / 1500: loss 2.104522\n",
      "iteration 900 / 1500: loss 2.070847\n",
      "iteration 1000 / 1500: loss 2.064389\n",
      "iteration 1100 / 1500: loss 2.063111\n",
      "iteration 1200 / 1500: loss 2.043881\n",
      "iteration 1300 / 1500: loss 2.084331\n",
      "iteration 1400 / 1500: loss 2.078879\n",
      "iteration 0 / 1500: loss 934.222898\n",
      "iteration 100 / 1500: loss 2.032295\n",
      "iteration 200 / 1500: loss 2.057990\n",
      "iteration 300 / 1500: loss 2.108192\n",
      "iteration 400 / 1500: loss 2.066605\n",
      "iteration 500 / 1500: loss 2.115519\n",
      "iteration 600 / 1500: loss 2.065231\n",
      "iteration 700 / 1500: loss 2.118117\n",
      "iteration 800 / 1500: loss 2.127068\n",
      "iteration 900 / 1500: loss 2.065975\n",
      "iteration 1000 / 1500: loss 2.150083\n",
      "iteration 1100 / 1500: loss 2.089977\n",
      "iteration 1200 / 1500: loss 2.050355\n",
      "iteration 1300 / 1500: loss 2.116173\n",
      "iteration 1400 / 1500: loss 2.127801\n",
      "iteration 0 / 1500: loss 967.142874\n",
      "iteration 100 / 1500: loss 2.131285\n",
      "iteration 200 / 1500: loss 2.096211\n",
      "iteration 300 / 1500: loss 2.166066\n",
      "iteration 400 / 1500: loss 2.129327\n",
      "iteration 500 / 1500: loss 2.116126\n",
      "iteration 600 / 1500: loss 2.060908\n",
      "iteration 700 / 1500: loss 2.097360\n",
      "iteration 800 / 1500: loss 2.024095\n",
      "iteration 900 / 1500: loss 2.053406\n",
      "iteration 1000 / 1500: loss 2.147216\n",
      "iteration 1100 / 1500: loss 2.039637\n",
      "iteration 1200 / 1500: loss 2.081103\n",
      "iteration 1300 / 1500: loss 2.071622\n",
      "iteration 1400 / 1500: loss 2.027730\n",
      "iteration 0 / 1500: loss 981.591894\n",
      "iteration 100 / 1500: loss 2.088175\n",
      "iteration 200 / 1500: loss 2.119981\n",
      "iteration 300 / 1500: loss 2.082975\n",
      "iteration 400 / 1500: loss 2.096377\n",
      "iteration 500 / 1500: loss 2.079570\n",
      "iteration 600 / 1500: loss 2.131054\n",
      "iteration 700 / 1500: loss 2.193380\n",
      "iteration 800 / 1500: loss 2.043674\n",
      "iteration 900 / 1500: loss 2.143879\n",
      "iteration 1000 / 1500: loss 2.071395\n",
      "iteration 1100 / 1500: loss 2.094781\n",
      "iteration 1200 / 1500: loss 2.164666\n",
      "iteration 1300 / 1500: loss 2.096752\n",
      "iteration 1400 / 1500: loss 2.039154\n",
      "iteration 0 / 1500: loss 1011.381712\n",
      "iteration 100 / 1500: loss 2.072202\n",
      "iteration 200 / 1500: loss 2.048507\n",
      "iteration 300 / 1500: loss 2.058588\n",
      "iteration 400 / 1500: loss 2.137860\n",
      "iteration 500 / 1500: loss 2.186339\n",
      "iteration 600 / 1500: loss 2.086657\n",
      "iteration 700 / 1500: loss 2.097745\n",
      "iteration 800 / 1500: loss 2.105154\n",
      "iteration 900 / 1500: loss 2.089002\n",
      "iteration 1000 / 1500: loss 2.071690\n",
      "iteration 1100 / 1500: loss 2.142353\n",
      "iteration 1200 / 1500: loss 2.077497\n",
      "iteration 1300 / 1500: loss 2.129875\n",
      "iteration 1400 / 1500: loss 2.092605\n",
      "iteration 0 / 1500: loss 1053.020033\n",
      "iteration 100 / 1500: loss 2.158167\n",
      "iteration 200 / 1500: loss 2.108668\n",
      "iteration 300 / 1500: loss 2.124870\n",
      "iteration 400 / 1500: loss 2.073026\n",
      "iteration 500 / 1500: loss 2.080900\n",
      "iteration 600 / 1500: loss 2.146196\n",
      "iteration 700 / 1500: loss 2.102178\n",
      "iteration 800 / 1500: loss 2.083193\n",
      "iteration 900 / 1500: loss 2.095952\n",
      "iteration 1000 / 1500: loss 2.043712\n",
      "iteration 1100 / 1500: loss 2.089729\n",
      "iteration 1200 / 1500: loss 2.091359\n",
      "iteration 1300 / 1500: loss 2.122613\n",
      "iteration 1400 / 1500: loss 2.109458\n",
      "iteration 0 / 1500: loss 1085.416618\n",
      "iteration 100 / 1500: loss 2.102245\n",
      "iteration 200 / 1500: loss 2.087074\n",
      "iteration 300 / 1500: loss 2.115063\n",
      "iteration 400 / 1500: loss 2.101130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500 / 1500: loss 2.076836\n",
      "iteration 600 / 1500: loss 2.102124\n",
      "iteration 700 / 1500: loss 2.137757\n",
      "iteration 800 / 1500: loss 2.066278\n",
      "iteration 900 / 1500: loss 2.146115\n",
      "iteration 1000 / 1500: loss 2.095233\n",
      "iteration 1100 / 1500: loss 2.090098\n",
      "iteration 1200 / 1500: loss 2.059613\n",
      "iteration 1300 / 1500: loss 2.132248\n",
      "iteration 1400 / 1500: loss 2.093829\n",
      "iteration 0 / 1500: loss 1116.178067\n",
      "iteration 100 / 1500: loss 2.168599\n",
      "iteration 200 / 1500: loss 2.109952\n",
      "iteration 300 / 1500: loss 2.132668\n",
      "iteration 400 / 1500: loss 2.060647\n",
      "iteration 500 / 1500: loss 2.117182\n",
      "iteration 600 / 1500: loss 2.093932\n",
      "iteration 700 / 1500: loss 2.083627\n",
      "iteration 800 / 1500: loss 2.069742\n",
      "iteration 900 / 1500: loss 2.092045\n",
      "iteration 1000 / 1500: loss 2.121687\n",
      "iteration 1100 / 1500: loss 2.102334\n",
      "iteration 1200 / 1500: loss 2.085053\n",
      "iteration 1300 / 1500: loss 2.099085\n",
      "iteration 1400 / 1500: loss 2.112008\n",
      "iteration 0 / 1500: loss 1136.853918\n",
      "iteration 100 / 1500: loss 2.167407\n",
      "iteration 200 / 1500: loss 1.998917\n",
      "iteration 300 / 1500: loss 2.110388\n",
      "iteration 400 / 1500: loss 2.103738\n",
      "iteration 500 / 1500: loss 2.075751\n",
      "iteration 600 / 1500: loss 2.114927\n",
      "iteration 700 / 1500: loss 2.110514\n",
      "iteration 800 / 1500: loss 2.147668\n",
      "iteration 900 / 1500: loss 2.117026\n",
      "iteration 1000 / 1500: loss 2.102550\n",
      "iteration 1100 / 1500: loss 2.077530\n",
      "iteration 1200 / 1500: loss 2.156902\n",
      "iteration 1300 / 1500: loss 2.130999\n",
      "iteration 1400 / 1500: loss 2.065867\n",
      "iteration 0 / 1500: loss 1168.578420\n",
      "iteration 100 / 1500: loss 2.137847\n",
      "iteration 200 / 1500: loss 2.136977\n",
      "iteration 300 / 1500: loss 2.109554\n",
      "iteration 400 / 1500: loss 2.107398\n",
      "iteration 500 / 1500: loss 2.157649\n",
      "iteration 600 / 1500: loss 2.173855\n",
      "iteration 700 / 1500: loss 2.073126\n",
      "iteration 800 / 1500: loss 2.144542\n",
      "iteration 900 / 1500: loss 2.116196\n",
      "iteration 1000 / 1500: loss 2.104369\n",
      "iteration 1100 / 1500: loss 2.174937\n",
      "iteration 1200 / 1500: loss 2.084886\n",
      "iteration 1300 / 1500: loss 2.136325\n",
      "iteration 1400 / 1500: loss 2.097450\n",
      "iteration 0 / 1500: loss 1201.794860\n",
      "iteration 100 / 1500: loss 2.122878\n",
      "iteration 200 / 1500: loss 2.134718\n",
      "iteration 300 / 1500: loss 2.115641\n",
      "iteration 400 / 1500: loss 2.063745\n",
      "iteration 500 / 1500: loss 2.118033\n",
      "iteration 600 / 1500: loss 2.121529\n",
      "iteration 700 / 1500: loss 2.063276\n",
      "iteration 800 / 1500: loss 2.133432\n",
      "iteration 900 / 1500: loss 2.130501\n",
      "iteration 1000 / 1500: loss 2.147366\n",
      "iteration 1100 / 1500: loss 2.079284\n",
      "iteration 1200 / 1500: loss 2.114188\n",
      "iteration 1300 / 1500: loss 2.069809\n",
      "iteration 1400 / 1500: loss 2.124991\n",
      "iteration 0 / 1500: loss 1227.888718\n",
      "iteration 100 / 1500: loss 2.088979\n",
      "iteration 200 / 1500: loss 2.165748\n",
      "iteration 300 / 1500: loss 2.194059\n",
      "iteration 400 / 1500: loss 2.083160\n",
      "iteration 500 / 1500: loss 2.093547\n",
      "iteration 600 / 1500: loss 2.068924\n",
      "iteration 700 / 1500: loss 2.157828\n",
      "iteration 800 / 1500: loss 2.097390\n",
      "iteration 900 / 1500: loss 2.084736\n",
      "iteration 1000 / 1500: loss 2.125832\n",
      "iteration 1100 / 1500: loss 2.079019\n",
      "iteration 1200 / 1500: loss 2.114577\n",
      "iteration 1300 / 1500: loss 2.121107\n",
      "iteration 1400 / 1500: loss 2.100589\n",
      "iteration 0 / 1500: loss 1274.785416\n",
      "iteration 100 / 1500: loss 2.108582\n",
      "iteration 200 / 1500: loss 2.091396\n",
      "iteration 300 / 1500: loss 2.073367\n",
      "iteration 400 / 1500: loss 2.144751\n",
      "iteration 500 / 1500: loss 2.101782\n",
      "iteration 600 / 1500: loss 2.107546\n",
      "iteration 700 / 1500: loss 2.091580\n",
      "iteration 800 / 1500: loss 2.118934\n",
      "iteration 900 / 1500: loss 2.150660\n",
      "iteration 1000 / 1500: loss 2.091536\n",
      "iteration 1100 / 1500: loss 2.143171\n",
      "iteration 1200 / 1500: loss 2.123638\n",
      "iteration 1300 / 1500: loss 2.092929\n",
      "iteration 1400 / 1500: loss 2.144974\n",
      "iteration 0 / 1500: loss 1307.453813\n",
      "iteration 100 / 1500: loss 2.105151\n",
      "iteration 200 / 1500: loss 2.104616\n",
      "iteration 300 / 1500: loss 2.099175\n",
      "iteration 400 / 1500: loss 2.092055\n",
      "iteration 500 / 1500: loss 2.108876\n",
      "iteration 600 / 1500: loss 2.083347\n",
      "iteration 700 / 1500: loss 2.086282\n",
      "iteration 800 / 1500: loss 2.076213\n",
      "iteration 900 / 1500: loss 2.153941\n",
      "iteration 1000 / 1500: loss 2.121009\n",
      "iteration 1100 / 1500: loss 2.109535\n",
      "iteration 1200 / 1500: loss 2.100000\n",
      "iteration 1300 / 1500: loss 2.155941\n",
      "iteration 1400 / 1500: loss 2.097962\n",
      "iteration 0 / 1500: loss 1332.579291\n",
      "iteration 100 / 1500: loss 2.198273\n",
      "iteration 200 / 1500: loss 2.170191\n",
      "iteration 300 / 1500: loss 2.145511\n",
      "iteration 400 / 1500: loss 2.110180\n",
      "iteration 500 / 1500: loss 2.094547\n",
      "iteration 600 / 1500: loss 2.102245\n",
      "iteration 700 / 1500: loss 2.151617\n",
      "iteration 800 / 1500: loss 2.095172\n",
      "iteration 900 / 1500: loss 2.116926\n",
      "iteration 1000 / 1500: loss 2.038646\n",
      "iteration 1100 / 1500: loss 2.114888\n",
      "iteration 1200 / 1500: loss 2.140771\n",
      "iteration 1300 / 1500: loss 2.114841\n",
      "iteration 1400 / 1500: loss 2.091762\n",
      "iteration 0 / 1500: loss 1354.223606\n",
      "iteration 100 / 1500: loss 2.096218\n",
      "iteration 200 / 1500: loss 2.149101\n",
      "iteration 300 / 1500: loss 2.052257\n",
      "iteration 400 / 1500: loss 2.143042\n",
      "iteration 500 / 1500: loss 2.113279\n",
      "iteration 600 / 1500: loss 2.120316\n",
      "iteration 700 / 1500: loss 2.077879\n",
      "iteration 800 / 1500: loss 2.119536\n",
      "iteration 900 / 1500: loss 2.139378\n",
      "iteration 1000 / 1500: loss 2.134626\n",
      "iteration 1100 / 1500: loss 2.168778\n",
      "iteration 1200 / 1500: loss 2.089363\n",
      "iteration 1300 / 1500: loss 2.153749\n",
      "iteration 1400 / 1500: loss 2.107543\n",
      "iteration 0 / 1500: loss 1370.895108\n",
      "iteration 100 / 1500: loss 2.046217\n",
      "iteration 200 / 1500: loss 2.115898\n",
      "iteration 300 / 1500: loss 2.162094\n",
      "iteration 400 / 1500: loss 2.120237\n",
      "iteration 500 / 1500: loss 2.134278\n",
      "iteration 600 / 1500: loss 2.114191\n",
      "iteration 700 / 1500: loss 2.125502\n",
      "iteration 800 / 1500: loss 2.090700\n",
      "iteration 900 / 1500: loss 2.162910\n",
      "iteration 1000 / 1500: loss 2.078063\n",
      "iteration 1100 / 1500: loss 2.145666\n",
      "iteration 1200 / 1500: loss 2.113948\n",
      "iteration 1300 / 1500: loss 2.189668\n",
      "iteration 1400 / 1500: loss 2.155616\n",
      "iteration 0 / 1500: loss 1427.696021\n",
      "iteration 100 / 1500: loss 2.133675\n",
      "iteration 200 / 1500: loss 2.186075\n",
      "iteration 300 / 1500: loss 2.109672\n",
      "iteration 400 / 1500: loss 2.080701\n",
      "iteration 500 / 1500: loss 2.118007\n",
      "iteration 600 / 1500: loss 2.089204\n",
      "iteration 700 / 1500: loss 2.129129\n",
      "iteration 800 / 1500: loss 2.164048\n",
      "iteration 900 / 1500: loss 2.124282\n",
      "iteration 1000 / 1500: loss 2.138309\n",
      "iteration 1100 / 1500: loss 2.090720\n",
      "iteration 1200 / 1500: loss 2.174814\n",
      "iteration 1300 / 1500: loss 2.132249\n",
      "iteration 1400 / 1500: loss 2.136453\n",
      "iteration 0 / 1500: loss 1459.297857\n",
      "iteration 100 / 1500: loss 2.078924\n",
      "iteration 200 / 1500: loss 2.169491\n",
      "iteration 300 / 1500: loss 2.093440\n",
      "iteration 400 / 1500: loss 2.117016\n",
      "iteration 500 / 1500: loss 2.117532\n",
      "iteration 600 / 1500: loss 2.135279\n",
      "iteration 700 / 1500: loss 2.145662\n",
      "iteration 800 / 1500: loss 2.097924\n",
      "iteration 900 / 1500: loss 2.142279\n",
      "iteration 1000 / 1500: loss 2.170626\n",
      "iteration 1100 / 1500: loss 2.124316\n",
      "iteration 1200 / 1500: loss 2.091493\n",
      "iteration 1300 / 1500: loss 2.160914\n",
      "iteration 1400 / 1500: loss 2.108590\n",
      "iteration 0 / 1500: loss 1480.365607\n",
      "iteration 100 / 1500: loss 2.153258\n",
      "iteration 200 / 1500: loss 2.137928\n",
      "iteration 300 / 1500: loss 2.120666\n",
      "iteration 400 / 1500: loss 2.189524\n",
      "iteration 500 / 1500: loss 2.142475\n",
      "iteration 600 / 1500: loss 2.122956\n",
      "iteration 700 / 1500: loss 2.130650\n",
      "iteration 800 / 1500: loss 2.147064\n",
      "iteration 900 / 1500: loss 2.137062\n",
      "iteration 1000 / 1500: loss 2.111326\n",
      "iteration 1100 / 1500: loss 2.185046\n",
      "iteration 1200 / 1500: loss 2.088727\n",
      "iteration 1300 / 1500: loss 2.078615\n",
      "iteration 1400 / 1500: loss 2.176501\n",
      "iteration 0 / 1500: loss 1494.437174\n",
      "iteration 100 / 1500: loss 2.128829\n",
      "iteration 200 / 1500: loss 2.129513\n",
      "iteration 300 / 1500: loss 2.130929\n",
      "iteration 400 / 1500: loss 2.147719\n",
      "iteration 500 / 1500: loss 2.146114\n",
      "iteration 600 / 1500: loss 2.103458\n",
      "iteration 700 / 1500: loss 2.144801\n",
      "iteration 800 / 1500: loss 2.104372\n",
      "iteration 900 / 1500: loss 2.089776\n",
      "iteration 1000 / 1500: loss 2.102057\n",
      "iteration 1100 / 1500: loss 2.113569\n",
      "iteration 1200 / 1500: loss 2.163932\n",
      "iteration 1300 / 1500: loss 2.192661\n",
      "iteration 1400 / 1500: loss 2.169210\n",
      "iteration 0 / 1500: loss 1558.889949\n",
      "iteration 100 / 1500: loss 2.139740\n",
      "iteration 200 / 1500: loss 2.162483\n",
      "iteration 300 / 1500: loss 2.140504\n",
      "iteration 400 / 1500: loss 2.111423\n",
      "iteration 500 / 1500: loss 2.129493\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 1500: loss 2.094808\n",
      "iteration 700 / 1500: loss 2.105898\n",
      "iteration 800 / 1500: loss 2.090692\n",
      "iteration 900 / 1500: loss 2.108972\n",
      "iteration 1000 / 1500: loss 2.144097\n",
      "iteration 1100 / 1500: loss 2.160369\n",
      "iteration 1200 / 1500: loss 2.127268\n",
      "iteration 1300 / 1500: loss 2.179392\n",
      "iteration 1400 / 1500: loss 2.130556\n",
      "iteration 0 / 1500: loss 779.486778\n",
      "iteration 100 / 1500: loss 2.084523\n",
      "iteration 200 / 1500: loss 2.093780\n",
      "iteration 300 / 1500: loss 2.037772\n",
      "iteration 400 / 1500: loss 2.017268\n",
      "iteration 500 / 1500: loss 2.074439\n",
      "iteration 600 / 1500: loss 2.064946\n",
      "iteration 700 / 1500: loss 2.043904\n",
      "iteration 800 / 1500: loss 2.031662\n",
      "iteration 900 / 1500: loss 2.078649\n",
      "iteration 1000 / 1500: loss 2.067106\n",
      "iteration 1100 / 1500: loss 1.993262\n",
      "iteration 1200 / 1500: loss 2.028587\n",
      "iteration 1300 / 1500: loss 2.064994\n",
      "iteration 1400 / 1500: loss 2.042780\n",
      "iteration 0 / 1500: loss 801.702975\n",
      "iteration 100 / 1500: loss 2.042228\n",
      "iteration 200 / 1500: loss 2.066406\n",
      "iteration 300 / 1500: loss 2.034442\n",
      "iteration 400 / 1500: loss 2.112497\n",
      "iteration 500 / 1500: loss 2.091044\n",
      "iteration 600 / 1500: loss 2.072897\n",
      "iteration 700 / 1500: loss 2.130973\n",
      "iteration 800 / 1500: loss 2.074309\n",
      "iteration 900 / 1500: loss 2.082997\n",
      "iteration 1000 / 1500: loss 1.991454\n",
      "iteration 1100 / 1500: loss 2.108685\n",
      "iteration 1200 / 1500: loss 2.092446\n",
      "iteration 1300 / 1500: loss 2.078996\n",
      "iteration 1400 / 1500: loss 2.104611\n",
      "iteration 0 / 1500: loss 839.463868\n",
      "iteration 100 / 1500: loss 2.094128\n",
      "iteration 200 / 1500: loss 2.079283\n",
      "iteration 300 / 1500: loss 2.078469\n",
      "iteration 400 / 1500: loss 2.121792\n",
      "iteration 500 / 1500: loss 2.044823\n",
      "iteration 600 / 1500: loss 2.065073\n",
      "iteration 700 / 1500: loss 2.082501\n",
      "iteration 800 / 1500: loss 2.118320\n",
      "iteration 900 / 1500: loss 2.053348\n",
      "iteration 1000 / 1500: loss 2.086557\n",
      "iteration 1100 / 1500: loss 2.068698\n",
      "iteration 1200 / 1500: loss 2.119517\n",
      "iteration 1300 / 1500: loss 2.187412\n",
      "iteration 1400 / 1500: loss 2.089574\n",
      "iteration 0 / 1500: loss 869.477463\n",
      "iteration 100 / 1500: loss 2.082689\n",
      "iteration 200 / 1500: loss 2.048130\n",
      "iteration 300 / 1500: loss 2.021573\n",
      "iteration 400 / 1500: loss 2.114039\n",
      "iteration 500 / 1500: loss 2.111836\n",
      "iteration 600 / 1500: loss 2.122391\n",
      "iteration 700 / 1500: loss 2.089825\n",
      "iteration 800 / 1500: loss 2.053364\n",
      "iteration 900 / 1500: loss 2.057079\n",
      "iteration 1000 / 1500: loss 2.053520\n",
      "iteration 1100 / 1500: loss 2.130559\n",
      "iteration 1200 / 1500: loss 2.039794\n",
      "iteration 1300 / 1500: loss 2.082774\n",
      "iteration 1400 / 1500: loss 2.040946\n",
      "iteration 0 / 1500: loss 902.586229\n",
      "iteration 100 / 1500: loss 2.106584\n",
      "iteration 200 / 1500: loss 2.079651\n",
      "iteration 300 / 1500: loss 2.101505\n",
      "iteration 400 / 1500: loss 2.122072\n",
      "iteration 500 / 1500: loss 2.080063\n",
      "iteration 600 / 1500: loss 2.055016\n",
      "iteration 700 / 1500: loss 2.117597\n",
      "iteration 800 / 1500: loss 2.101246\n",
      "iteration 900 / 1500: loss 2.052071\n",
      "iteration 1000 / 1500: loss 2.100729\n",
      "iteration 1100 / 1500: loss 2.092390\n",
      "iteration 1200 / 1500: loss 2.113949\n",
      "iteration 1300 / 1500: loss 2.116814\n",
      "iteration 1400 / 1500: loss 2.071104\n",
      "iteration 0 / 1500: loss 935.084106\n",
      "iteration 100 / 1500: loss 2.079411\n",
      "iteration 200 / 1500: loss 2.093575\n",
      "iteration 300 / 1500: loss 2.106443\n",
      "iteration 400 / 1500: loss 2.121273\n",
      "iteration 500 / 1500: loss 2.127105\n",
      "iteration 600 / 1500: loss 2.134133\n",
      "iteration 700 / 1500: loss 2.093345\n",
      "iteration 800 / 1500: loss 2.076841\n",
      "iteration 900 / 1500: loss 2.079894\n",
      "iteration 1000 / 1500: loss 2.058427\n",
      "iteration 1100 / 1500: loss 2.083047\n",
      "iteration 1200 / 1500: loss 2.101351\n",
      "iteration 1300 / 1500: loss 2.084551\n",
      "iteration 1400 / 1500: loss 2.100266\n",
      "iteration 0 / 1500: loss 973.960869\n",
      "iteration 100 / 1500: loss 2.130943\n",
      "iteration 200 / 1500: loss 2.127077\n",
      "iteration 300 / 1500: loss 2.095389\n",
      "iteration 400 / 1500: loss 2.086031\n",
      "iteration 500 / 1500: loss 2.068328\n",
      "iteration 600 / 1500: loss 2.083548\n",
      "iteration 700 / 1500: loss 2.059183\n",
      "iteration 800 / 1500: loss 2.068256\n",
      "iteration 900 / 1500: loss 2.046902\n",
      "iteration 1000 / 1500: loss 2.087182\n",
      "iteration 1100 / 1500: loss 2.098530\n",
      "iteration 1200 / 1500: loss 2.054409\n",
      "iteration 1300 / 1500: loss 2.126352\n",
      "iteration 1400 / 1500: loss 2.063066\n",
      "iteration 0 / 1500: loss 994.674615\n",
      "iteration 100 / 1500: loss 2.047266\n",
      "iteration 200 / 1500: loss 2.080846\n",
      "iteration 300 / 1500: loss 2.114165\n",
      "iteration 400 / 1500: loss 2.065876\n",
      "iteration 500 / 1500: loss 2.102890\n",
      "iteration 600 / 1500: loss 2.075315\n",
      "iteration 700 / 1500: loss 2.109097\n",
      "iteration 800 / 1500: loss 2.094187\n",
      "iteration 900 / 1500: loss 2.095055\n",
      "iteration 1000 / 1500: loss 2.045775\n",
      "iteration 1100 / 1500: loss 2.094212\n",
      "iteration 1200 / 1500: loss 2.113956\n",
      "iteration 1300 / 1500: loss 2.040914\n",
      "iteration 1400 / 1500: loss 2.060594\n",
      "iteration 0 / 1500: loss 1010.639375\n",
      "iteration 100 / 1500: loss 2.023377\n",
      "iteration 200 / 1500: loss 2.159632\n",
      "iteration 300 / 1500: loss 2.098989\n",
      "iteration 400 / 1500: loss 2.091916\n",
      "iteration 500 / 1500: loss 2.116214\n",
      "iteration 600 / 1500: loss 2.126833\n",
      "iteration 700 / 1500: loss 2.082613\n",
      "iteration 800 / 1500: loss 2.101224\n",
      "iteration 900 / 1500: loss 2.142209\n",
      "iteration 1000 / 1500: loss 2.092579\n",
      "iteration 1100 / 1500: loss 2.102599\n",
      "iteration 1200 / 1500: loss 2.130100\n",
      "iteration 1300 / 1500: loss 2.162507\n",
      "iteration 1400 / 1500: loss 2.157625\n",
      "iteration 0 / 1500: loss 1062.240355\n",
      "iteration 100 / 1500: loss 2.094464\n",
      "iteration 200 / 1500: loss 2.167983\n",
      "iteration 300 / 1500: loss 2.189603\n",
      "iteration 400 / 1500: loss 2.137665\n",
      "iteration 500 / 1500: loss 2.143645\n",
      "iteration 600 / 1500: loss 2.119522\n",
      "iteration 700 / 1500: loss 2.077208\n",
      "iteration 800 / 1500: loss 2.116317\n",
      "iteration 900 / 1500: loss 2.084458\n",
      "iteration 1000 / 1500: loss 2.041505\n",
      "iteration 1100 / 1500: loss 2.124911\n",
      "iteration 1200 / 1500: loss 2.064146\n",
      "iteration 1300 / 1500: loss 2.093848\n",
      "iteration 1400 / 1500: loss 2.153365\n",
      "iteration 0 / 1500: loss 1073.698328\n",
      "iteration 100 / 1500: loss 2.087326\n",
      "iteration 200 / 1500: loss 2.115056\n",
      "iteration 300 / 1500: loss 2.080683\n",
      "iteration 400 / 1500: loss 2.110930\n",
      "iteration 500 / 1500: loss 2.099535\n",
      "iteration 600 / 1500: loss 2.096630\n",
      "iteration 700 / 1500: loss 2.067419\n",
      "iteration 800 / 1500: loss 2.094572\n",
      "iteration 900 / 1500: loss 2.151688\n",
      "iteration 1000 / 1500: loss 2.105222\n",
      "iteration 1100 / 1500: loss 2.068704\n",
      "iteration 1200 / 1500: loss 2.075250\n",
      "iteration 1300 / 1500: loss 2.103566\n",
      "iteration 1400 / 1500: loss 2.141262\n",
      "iteration 0 / 1500: loss 1106.873645\n",
      "iteration 100 / 1500: loss 2.113029\n",
      "iteration 200 / 1500: loss 2.136753\n",
      "iteration 300 / 1500: loss 2.131217\n",
      "iteration 400 / 1500: loss 2.120632\n",
      "iteration 500 / 1500: loss 2.145305\n",
      "iteration 600 / 1500: loss 2.102067\n",
      "iteration 700 / 1500: loss 2.085778\n",
      "iteration 800 / 1500: loss 2.096198\n",
      "iteration 900 / 1500: loss 2.008234\n",
      "iteration 1000 / 1500: loss 2.133519\n",
      "iteration 1100 / 1500: loss 2.065637\n",
      "iteration 1200 / 1500: loss 2.073601\n",
      "iteration 1300 / 1500: loss 2.096662\n",
      "iteration 1400 / 1500: loss 2.121306\n",
      "iteration 0 / 1500: loss 1139.143649\n",
      "iteration 100 / 1500: loss 2.119018\n",
      "iteration 200 / 1500: loss 2.046823\n",
      "iteration 300 / 1500: loss 2.130248\n",
      "iteration 400 / 1500: loss 2.123777\n",
      "iteration 500 / 1500: loss 2.077361\n",
      "iteration 600 / 1500: loss 2.095123\n",
      "iteration 700 / 1500: loss 2.120056\n",
      "iteration 800 / 1500: loss 2.151178\n",
      "iteration 900 / 1500: loss 2.066638\n",
      "iteration 1000 / 1500: loss 2.086874\n",
      "iteration 1100 / 1500: loss 2.113121\n",
      "iteration 1200 / 1500: loss 2.119865\n",
      "iteration 1300 / 1500: loss 2.053824\n",
      "iteration 1400 / 1500: loss 2.117759\n",
      "iteration 0 / 1500: loss 1164.938707\n",
      "iteration 100 / 1500: loss 2.075158\n",
      "iteration 200 / 1500: loss 2.163962\n",
      "iteration 300 / 1500: loss 2.079954\n",
      "iteration 400 / 1500: loss 2.134742\n",
      "iteration 500 / 1500: loss 2.136293\n",
      "iteration 600 / 1500: loss 2.101409\n",
      "iteration 700 / 1500: loss 2.110713\n",
      "iteration 800 / 1500: loss 2.131065\n",
      "iteration 900 / 1500: loss 2.037300\n",
      "iteration 1000 / 1500: loss 2.093085\n",
      "iteration 1100 / 1500: loss 2.070014\n",
      "iteration 1200 / 1500: loss 2.117605\n",
      "iteration 1300 / 1500: loss 2.101603\n",
      "iteration 1400 / 1500: loss 2.121796\n",
      "iteration 0 / 1500: loss 1202.676838\n",
      "iteration 100 / 1500: loss 2.165219\n",
      "iteration 200 / 1500: loss 2.147864\n",
      "iteration 300 / 1500: loss 2.134025\n",
      "iteration 400 / 1500: loss 2.100818\n",
      "iteration 500 / 1500: loss 2.084327\n",
      "iteration 600 / 1500: loss 2.184376\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 1500: loss 2.107375\n",
      "iteration 800 / 1500: loss 2.151632\n",
      "iteration 900 / 1500: loss 2.138572\n",
      "iteration 1000 / 1500: loss 2.125206\n",
      "iteration 1100 / 1500: loss 2.061047\n",
      "iteration 1200 / 1500: loss 2.145163\n",
      "iteration 1300 / 1500: loss 2.072191\n",
      "iteration 1400 / 1500: loss 2.091669\n",
      "iteration 0 / 1500: loss 1221.637935\n",
      "iteration 100 / 1500: loss 2.141549\n",
      "iteration 200 / 1500: loss 2.103185\n",
      "iteration 300 / 1500: loss 2.044687\n",
      "iteration 400 / 1500: loss 2.076009\n",
      "iteration 500 / 1500: loss 2.064063\n",
      "iteration 600 / 1500: loss 2.093305\n",
      "iteration 700 / 1500: loss 2.113810\n",
      "iteration 800 / 1500: loss 2.139520\n",
      "iteration 900 / 1500: loss 2.100620\n",
      "iteration 1000 / 1500: loss 2.129048\n",
      "iteration 1100 / 1500: loss 2.102508\n",
      "iteration 1200 / 1500: loss 2.133716\n",
      "iteration 1300 / 1500: loss 2.096568\n",
      "iteration 1400 / 1500: loss 2.105253\n",
      "iteration 0 / 1500: loss 1251.081064\n",
      "iteration 100 / 1500: loss 2.108458\n",
      "iteration 200 / 1500: loss 2.041011\n",
      "iteration 300 / 1500: loss 2.125294\n",
      "iteration 400 / 1500: loss 2.131603\n",
      "iteration 500 / 1500: loss 2.094380\n",
      "iteration 600 / 1500: loss 2.062629\n",
      "iteration 700 / 1500: loss 2.105396\n",
      "iteration 800 / 1500: loss 2.170075\n",
      "iteration 900 / 1500: loss 2.113388\n",
      "iteration 1000 / 1500: loss 2.157223\n",
      "iteration 1100 / 1500: loss 2.069958\n",
      "iteration 1200 / 1500: loss 2.068432\n",
      "iteration 1300 / 1500: loss 2.073424\n",
      "iteration 1400 / 1500: loss 2.133071\n",
      "iteration 0 / 1500: loss 1284.023174\n",
      "iteration 100 / 1500: loss 2.112726\n",
      "iteration 200 / 1500: loss 2.169149\n",
      "iteration 300 / 1500: loss 2.114275\n",
      "iteration 400 / 1500: loss 2.092993\n",
      "iteration 500 / 1500: loss 2.145868\n",
      "iteration 600 / 1500: loss 2.129912\n",
      "iteration 700 / 1500: loss 2.119255\n",
      "iteration 800 / 1500: loss 2.100177\n",
      "iteration 900 / 1500: loss 2.099170\n",
      "iteration 1000 / 1500: loss 2.125882\n",
      "iteration 1100 / 1500: loss 2.113045\n",
      "iteration 1200 / 1500: loss 2.130497\n",
      "iteration 1300 / 1500: loss 2.136049\n",
      "iteration 1400 / 1500: loss 2.100260\n",
      "iteration 0 / 1500: loss 1337.483315\n",
      "iteration 100 / 1500: loss 2.144012\n",
      "iteration 200 / 1500: loss 2.219816\n",
      "iteration 300 / 1500: loss 2.142982\n",
      "iteration 400 / 1500: loss 2.201758\n",
      "iteration 500 / 1500: loss 2.124085\n",
      "iteration 600 / 1500: loss 2.063550\n",
      "iteration 700 / 1500: loss 2.140642\n",
      "iteration 800 / 1500: loss 2.171156\n",
      "iteration 900 / 1500: loss 2.113544\n",
      "iteration 1000 / 1500: loss 2.073849\n",
      "iteration 1100 / 1500: loss 2.144062\n",
      "iteration 1200 / 1500: loss 2.124832\n",
      "iteration 1300 / 1500: loss 2.074917\n",
      "iteration 1400 / 1500: loss 2.118330\n",
      "iteration 0 / 1500: loss 1360.009905\n",
      "iteration 100 / 1500: loss 2.165361\n",
      "iteration 200 / 1500: loss 2.129742\n",
      "iteration 300 / 1500: loss 2.072805\n",
      "iteration 400 / 1500: loss 2.067889\n",
      "iteration 500 / 1500: loss 2.118469\n",
      "iteration 600 / 1500: loss 2.124677\n",
      "iteration 700 / 1500: loss 2.150122\n",
      "iteration 800 / 1500: loss 2.151109\n",
      "iteration 900 / 1500: loss 2.074107\n",
      "iteration 1000 / 1500: loss 2.108014\n",
      "iteration 1100 / 1500: loss 2.138842\n",
      "iteration 1200 / 1500: loss 2.179537\n",
      "iteration 1300 / 1500: loss 2.107178\n",
      "iteration 1400 / 1500: loss 2.137274\n",
      "iteration 0 / 1500: loss 1405.775556\n",
      "iteration 100 / 1500: loss 2.144544\n",
      "iteration 200 / 1500: loss 2.121455\n",
      "iteration 300 / 1500: loss 2.141265\n",
      "iteration 400 / 1500: loss 2.135990\n",
      "iteration 500 / 1500: loss 2.121238\n",
      "iteration 600 / 1500: loss 2.105642\n",
      "iteration 700 / 1500: loss 2.115402\n",
      "iteration 800 / 1500: loss 2.116552\n",
      "iteration 900 / 1500: loss 2.149378\n",
      "iteration 1000 / 1500: loss 2.107394\n",
      "iteration 1100 / 1500: loss 2.119308\n",
      "iteration 1200 / 1500: loss 2.137264\n",
      "iteration 1300 / 1500: loss 2.187202\n",
      "iteration 1400 / 1500: loss 2.131356\n",
      "iteration 0 / 1500: loss 1427.497667\n",
      "iteration 100 / 1500: loss 2.109887\n",
      "iteration 200 / 1500: loss 2.120675\n",
      "iteration 300 / 1500: loss 2.145880\n",
      "iteration 400 / 1500: loss 2.139926\n",
      "iteration 500 / 1500: loss 2.125634\n",
      "iteration 600 / 1500: loss 2.162838\n",
      "iteration 700 / 1500: loss 2.166317\n",
      "iteration 800 / 1500: loss 2.076074\n",
      "iteration 900 / 1500: loss 2.090673\n",
      "iteration 1000 / 1500: loss 2.134419\n",
      "iteration 1100 / 1500: loss 2.122106\n",
      "iteration 1200 / 1500: loss 2.163003\n",
      "iteration 1300 / 1500: loss 2.103201\n",
      "iteration 1400 / 1500: loss 2.111298\n",
      "iteration 0 / 1500: loss 1453.671281\n",
      "iteration 100 / 1500: loss 2.097293\n",
      "iteration 200 / 1500: loss 2.087495\n",
      "iteration 300 / 1500: loss 2.087812\n",
      "iteration 400 / 1500: loss 2.134667\n",
      "iteration 500 / 1500: loss 2.117840\n",
      "iteration 600 / 1500: loss 2.138346\n",
      "iteration 700 / 1500: loss 2.140630\n",
      "iteration 800 / 1500: loss 2.138080\n",
      "iteration 900 / 1500: loss 2.092973\n",
      "iteration 1000 / 1500: loss 2.067692\n",
      "iteration 1100 / 1500: loss 2.115995\n",
      "iteration 1200 / 1500: loss 2.111221\n",
      "iteration 1300 / 1500: loss 2.094979\n",
      "iteration 1400 / 1500: loss 2.156959\n",
      "iteration 0 / 1500: loss 1485.208102\n",
      "iteration 100 / 1500: loss 2.155741\n",
      "iteration 200 / 1500: loss 2.157791\n",
      "iteration 300 / 1500: loss 2.117187\n",
      "iteration 400 / 1500: loss 2.159373\n",
      "iteration 500 / 1500: loss 2.162817\n",
      "iteration 600 / 1500: loss 2.140042\n",
      "iteration 700 / 1500: loss 2.152572\n",
      "iteration 800 / 1500: loss 2.144110\n",
      "iteration 900 / 1500: loss 2.133185\n",
      "iteration 1000 / 1500: loss 2.132585\n",
      "iteration 1100 / 1500: loss 2.102120\n",
      "iteration 1200 / 1500: loss 2.178323\n",
      "iteration 1300 / 1500: loss 2.162541\n",
      "iteration 1400 / 1500: loss 2.141621\n",
      "iteration 0 / 1500: loss 1501.078057\n",
      "iteration 100 / 1500: loss 2.132363\n",
      "iteration 200 / 1500: loss 2.181852\n",
      "iteration 300 / 1500: loss 2.146941\n",
      "iteration 400 / 1500: loss 2.120380\n",
      "iteration 500 / 1500: loss 2.146152\n",
      "iteration 600 / 1500: loss 2.093647\n",
      "iteration 700 / 1500: loss 2.112453\n",
      "iteration 800 / 1500: loss 2.154067\n",
      "iteration 900 / 1500: loss 2.130655\n",
      "iteration 1000 / 1500: loss 2.166378\n",
      "iteration 1100 / 1500: loss 2.139299\n",
      "iteration 1200 / 1500: loss 2.158535\n",
      "iteration 1300 / 1500: loss 2.146154\n",
      "iteration 1400 / 1500: loss 2.151482\n",
      "iteration 0 / 1500: loss 1550.864041\n",
      "iteration 100 / 1500: loss 2.137219\n",
      "iteration 200 / 1500: loss 2.108417\n",
      "iteration 300 / 1500: loss 2.173172\n",
      "iteration 400 / 1500: loss 2.138229\n",
      "iteration 500 / 1500: loss 2.110548\n",
      "iteration 600 / 1500: loss 2.134520\n",
      "iteration 700 / 1500: loss 2.148201\n",
      "iteration 800 / 1500: loss 2.136391\n",
      "iteration 900 / 1500: loss 2.137155\n",
      "iteration 1000 / 1500: loss 2.136269\n",
      "iteration 1100 / 1500: loss 2.100210\n",
      "iteration 1200 / 1500: loss 2.086112\n",
      "iteration 1300 / 1500: loss 2.134834\n",
      "iteration 1400 / 1500: loss 2.076274\n",
      "iteration 0 / 1500: loss 778.121195\n",
      "iteration 100 / 1500: loss 2.094973\n",
      "iteration 200 / 1500: loss 2.139997\n",
      "iteration 300 / 1500: loss 1.989187\n",
      "iteration 400 / 1500: loss 2.060131\n",
      "iteration 500 / 1500: loss 2.122950\n",
      "iteration 600 / 1500: loss 2.101864\n",
      "iteration 700 / 1500: loss 2.062684\n",
      "iteration 800 / 1500: loss 2.122215\n",
      "iteration 900 / 1500: loss 2.082621\n",
      "iteration 1000 / 1500: loss 2.108237\n",
      "iteration 1100 / 1500: loss 2.111930\n",
      "iteration 1200 / 1500: loss 2.059933\n",
      "iteration 1300 / 1500: loss 2.036738\n",
      "iteration 1400 / 1500: loss 2.065114\n",
      "iteration 0 / 1500: loss 804.605752\n",
      "iteration 100 / 1500: loss 2.027103\n",
      "iteration 200 / 1500: loss 2.072794\n",
      "iteration 300 / 1500: loss 2.077251\n",
      "iteration 400 / 1500: loss 2.048261\n",
      "iteration 500 / 1500: loss 2.090726\n",
      "iteration 600 / 1500: loss 2.126018\n",
      "iteration 700 / 1500: loss 2.017519\n",
      "iteration 800 / 1500: loss 2.143906\n",
      "iteration 900 / 1500: loss 2.090290\n",
      "iteration 1000 / 1500: loss 2.067793\n",
      "iteration 1100 / 1500: loss 2.159017\n",
      "iteration 1200 / 1500: loss 2.073841\n",
      "iteration 1300 / 1500: loss 2.101549\n",
      "iteration 1400 / 1500: loss 2.013957\n",
      "iteration 0 / 1500: loss 835.723588\n",
      "iteration 100 / 1500: loss 2.064726\n",
      "iteration 200 / 1500: loss 2.093431\n",
      "iteration 300 / 1500: loss 2.094450\n",
      "iteration 400 / 1500: loss 2.068114\n",
      "iteration 500 / 1500: loss 2.092700\n",
      "iteration 600 / 1500: loss 2.112768\n",
      "iteration 700 / 1500: loss 2.040104\n",
      "iteration 800 / 1500: loss 2.142036\n",
      "iteration 900 / 1500: loss 2.120540\n",
      "iteration 1000 / 1500: loss 2.068280\n",
      "iteration 1100 / 1500: loss 2.137633\n",
      "iteration 1200 / 1500: loss 2.110026\n",
      "iteration 1300 / 1500: loss 2.110366\n",
      "iteration 1400 / 1500: loss 2.152559\n",
      "iteration 0 / 1500: loss 859.795669\n",
      "iteration 100 / 1500: loss 2.056502\n",
      "iteration 200 / 1500: loss 2.103294\n",
      "iteration 300 / 1500: loss 2.077261\n",
      "iteration 400 / 1500: loss 2.129812\n",
      "iteration 500 / 1500: loss 2.059774\n",
      "iteration 600 / 1500: loss 2.049939\n",
      "iteration 700 / 1500: loss 2.143656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 1500: loss 2.113115\n",
      "iteration 900 / 1500: loss 2.023087\n",
      "iteration 1000 / 1500: loss 2.074000\n",
      "iteration 1100 / 1500: loss 2.112387\n",
      "iteration 1200 / 1500: loss 2.049065\n",
      "iteration 1300 / 1500: loss 2.096265\n",
      "iteration 1400 / 1500: loss 2.084742\n",
      "iteration 0 / 1500: loss 891.182136\n",
      "iteration 100 / 1500: loss 2.142716\n",
      "iteration 200 / 1500: loss 2.136052\n",
      "iteration 300 / 1500: loss 2.108011\n",
      "iteration 400 / 1500: loss 2.024011\n",
      "iteration 500 / 1500: loss 2.089246\n",
      "iteration 600 / 1500: loss 2.103468\n",
      "iteration 700 / 1500: loss 2.112873\n",
      "iteration 800 / 1500: loss 2.069741\n",
      "iteration 900 / 1500: loss 2.080563\n",
      "iteration 1000 / 1500: loss 2.053396\n",
      "iteration 1100 / 1500: loss 2.112546\n",
      "iteration 1200 / 1500: loss 2.079234\n",
      "iteration 1300 / 1500: loss 2.049694\n",
      "iteration 1400 / 1500: loss 2.054802\n",
      "iteration 0 / 1500: loss 916.126397\n",
      "iteration 100 / 1500: loss 2.058418\n",
      "iteration 200 / 1500: loss 2.044038\n",
      "iteration 300 / 1500: loss 2.087746\n",
      "iteration 400 / 1500: loss 2.066742\n",
      "iteration 500 / 1500: loss 2.035121\n",
      "iteration 600 / 1500: loss 2.121315\n",
      "iteration 700 / 1500: loss 2.136059\n",
      "iteration 800 / 1500: loss 2.120288\n",
      "iteration 900 / 1500: loss 2.141623\n",
      "iteration 1000 / 1500: loss 2.068486\n",
      "iteration 1100 / 1500: loss 2.150114\n",
      "iteration 1200 / 1500: loss 2.098817\n",
      "iteration 1300 / 1500: loss 2.115350\n",
      "iteration 1400 / 1500: loss 2.077100\n",
      "iteration 0 / 1500: loss 968.140396\n",
      "iteration 100 / 1500: loss 2.098453\n",
      "iteration 200 / 1500: loss 2.042960\n",
      "iteration 300 / 1500: loss 2.077181\n",
      "iteration 400 / 1500: loss 2.077894\n",
      "iteration 500 / 1500: loss 2.073634\n",
      "iteration 600 / 1500: loss 2.079553\n",
      "iteration 700 / 1500: loss 2.154667\n",
      "iteration 800 / 1500: loss 2.117497\n",
      "iteration 900 / 1500: loss 2.111337\n",
      "iteration 1000 / 1500: loss 2.128937\n",
      "iteration 1100 / 1500: loss 2.094018\n",
      "iteration 1200 / 1500: loss 2.066843\n",
      "iteration 1300 / 1500: loss 2.089508\n",
      "iteration 1400 / 1500: loss 2.119046\n",
      "iteration 0 / 1500: loss 991.497075\n",
      "iteration 100 / 1500: loss 2.108784\n",
      "iteration 200 / 1500: loss 2.120971\n",
      "iteration 300 / 1500: loss 2.122864\n",
      "iteration 400 / 1500: loss 2.114363\n",
      "iteration 500 / 1500: loss 2.124226\n",
      "iteration 600 / 1500: loss 2.154308\n",
      "iteration 700 / 1500: loss 2.161204\n",
      "iteration 800 / 1500: loss 2.076444\n",
      "iteration 900 / 1500: loss 2.097251\n",
      "iteration 1000 / 1500: loss 2.119743\n",
      "iteration 1100 / 1500: loss 2.106439\n",
      "iteration 1200 / 1500: loss 2.160567\n",
      "iteration 1300 / 1500: loss 2.093502\n",
      "iteration 1400 / 1500: loss 2.076971\n",
      "iteration 0 / 1500: loss 1035.219725\n",
      "iteration 100 / 1500: loss 2.087105\n",
      "iteration 200 / 1500: loss 2.079030\n",
      "iteration 300 / 1500: loss 2.073782\n",
      "iteration 400 / 1500: loss 2.118969\n",
      "iteration 500 / 1500: loss 2.088152\n",
      "iteration 600 / 1500: loss 2.110937\n",
      "iteration 700 / 1500: loss 2.075304\n",
      "iteration 800 / 1500: loss 2.080189\n",
      "iteration 900 / 1500: loss 2.101760\n",
      "iteration 1000 / 1500: loss 2.143383\n",
      "iteration 1100 / 1500: loss 2.099895\n",
      "iteration 1200 / 1500: loss 2.117066\n",
      "iteration 1300 / 1500: loss 2.126113\n",
      "iteration 1400 / 1500: loss 2.096948\n",
      "iteration 0 / 1500: loss 1051.936202\n",
      "iteration 100 / 1500: loss 2.201809\n",
      "iteration 200 / 1500: loss 2.077178\n",
      "iteration 300 / 1500: loss 2.085416\n",
      "iteration 400 / 1500: loss 2.134780\n",
      "iteration 500 / 1500: loss 2.155149\n",
      "iteration 600 / 1500: loss 2.148558\n",
      "iteration 700 / 1500: loss 2.121045\n",
      "iteration 800 / 1500: loss 2.109166\n",
      "iteration 900 / 1500: loss 2.098604\n",
      "iteration 1000 / 1500: loss 2.028477\n",
      "iteration 1100 / 1500: loss 2.141064\n",
      "iteration 1200 / 1500: loss 2.030965\n",
      "iteration 1300 / 1500: loss 2.107170\n",
      "iteration 1400 / 1500: loss 2.085621\n",
      "iteration 0 / 1500: loss 1065.957417\n",
      "iteration 100 / 1500: loss 2.141113\n",
      "iteration 200 / 1500: loss 2.094523\n",
      "iteration 300 / 1500: loss 2.140598\n",
      "iteration 400 / 1500: loss 2.084220\n",
      "iteration 500 / 1500: loss 2.114647\n",
      "iteration 600 / 1500: loss 2.134430\n",
      "iteration 700 / 1500: loss 2.108137\n",
      "iteration 800 / 1500: loss 2.050035\n",
      "iteration 900 / 1500: loss 2.109993\n",
      "iteration 1000 / 1500: loss 2.089102\n",
      "iteration 1100 / 1500: loss 2.151385\n",
      "iteration 1200 / 1500: loss 2.118578\n",
      "iteration 1300 / 1500: loss 2.159713\n",
      "iteration 1400 / 1500: loss 2.118718\n",
      "iteration 0 / 1500: loss 1115.970403\n",
      "iteration 100 / 1500: loss 2.103456\n",
      "iteration 200 / 1500: loss 2.128940\n",
      "iteration 300 / 1500: loss 2.121471\n",
      "iteration 400 / 1500: loss 2.083458\n",
      "iteration 500 / 1500: loss 2.080059\n",
      "iteration 600 / 1500: loss 2.012998\n",
      "iteration 700 / 1500: loss 2.107280\n",
      "iteration 800 / 1500: loss 2.107329\n",
      "iteration 900 / 1500: loss 2.075884\n",
      "iteration 1000 / 1500: loss 2.132972\n",
      "iteration 1100 / 1500: loss 2.119842\n",
      "iteration 1200 / 1500: loss 2.113402\n",
      "iteration 1300 / 1500: loss 2.111271\n",
      "iteration 1400 / 1500: loss 2.082912\n",
      "iteration 0 / 1500: loss 1138.949386\n",
      "iteration 100 / 1500: loss 2.115581\n",
      "iteration 200 / 1500: loss 2.122106\n",
      "iteration 300 / 1500: loss 2.127752\n",
      "iteration 400 / 1500: loss 2.102412\n",
      "iteration 500 / 1500: loss 2.133468\n",
      "iteration 600 / 1500: loss 2.148201\n",
      "iteration 700 / 1500: loss 2.122821\n",
      "iteration 800 / 1500: loss 2.154087\n",
      "iteration 900 / 1500: loss 2.070463\n",
      "iteration 1000 / 1500: loss 2.112543\n",
      "iteration 1100 / 1500: loss 2.109669\n",
      "iteration 1200 / 1500: loss 2.103289\n",
      "iteration 1300 / 1500: loss 2.103743\n",
      "iteration 1400 / 1500: loss 2.096120\n",
      "iteration 0 / 1500: loss 1158.881051\n",
      "iteration 100 / 1500: loss 2.122436\n",
      "iteration 200 / 1500: loss 2.106648\n",
      "iteration 300 / 1500: loss 2.132127\n",
      "iteration 400 / 1500: loss 2.131169\n",
      "iteration 500 / 1500: loss 2.081630\n",
      "iteration 600 / 1500: loss 2.101102\n",
      "iteration 700 / 1500: loss 2.127858\n",
      "iteration 800 / 1500: loss 2.132832\n",
      "iteration 900 / 1500: loss 2.085643\n",
      "iteration 1000 / 1500: loss 2.106186\n",
      "iteration 1100 / 1500: loss 2.121561\n",
      "iteration 1200 / 1500: loss 2.121379\n",
      "iteration 1300 / 1500: loss 2.171375\n",
      "iteration 1400 / 1500: loss 2.177000\n",
      "iteration 0 / 1500: loss 1203.256884\n",
      "iteration 100 / 1500: loss 2.148559\n",
      "iteration 200 / 1500: loss 2.175895\n",
      "iteration 300 / 1500: loss 2.104687\n",
      "iteration 400 / 1500: loss 2.130666\n",
      "iteration 500 / 1500: loss 2.128609\n",
      "iteration 600 / 1500: loss 2.103572\n",
      "iteration 700 / 1500: loss 2.097179\n",
      "iteration 800 / 1500: loss 2.085909\n",
      "iteration 900 / 1500: loss 2.134468\n",
      "iteration 1000 / 1500: loss 2.123446\n",
      "iteration 1100 / 1500: loss 2.147213\n",
      "iteration 1200 / 1500: loss 2.150039\n",
      "iteration 1300 / 1500: loss 2.223298\n",
      "iteration 1400 / 1500: loss 2.084384\n",
      "iteration 0 / 1500: loss 1253.806947\n",
      "iteration 100 / 1500: loss 2.115531\n",
      "iteration 200 / 1500: loss 2.130400\n",
      "iteration 300 / 1500: loss 2.120520\n",
      "iteration 400 / 1500: loss 2.114960\n",
      "iteration 500 / 1500: loss 2.167982\n",
      "iteration 600 / 1500: loss 2.120939\n",
      "iteration 700 / 1500: loss 2.140011\n",
      "iteration 800 / 1500: loss 2.111033\n",
      "iteration 900 / 1500: loss 2.137925\n",
      "iteration 1000 / 1500: loss 2.134623\n",
      "iteration 1100 / 1500: loss 2.133655\n",
      "iteration 1200 / 1500: loss 2.135284\n",
      "iteration 1300 / 1500: loss 2.112047\n",
      "iteration 1400 / 1500: loss 2.062640\n",
      "iteration 0 / 1500: loss 1273.398014\n",
      "iteration 100 / 1500: loss 2.159073\n",
      "iteration 200 / 1500: loss 2.110527\n",
      "iteration 300 / 1500: loss 2.084510\n",
      "iteration 400 / 1500: loss 2.110602\n",
      "iteration 500 / 1500: loss 2.127566\n",
      "iteration 600 / 1500: loss 2.079083\n",
      "iteration 700 / 1500: loss 2.122888\n",
      "iteration 800 / 1500: loss 2.167047\n",
      "iteration 900 / 1500: loss 2.108623\n",
      "iteration 1000 / 1500: loss 2.087782\n",
      "iteration 1100 / 1500: loss 2.056354\n",
      "iteration 1200 / 1500: loss 2.133345\n",
      "iteration 1300 / 1500: loss 2.078924\n",
      "iteration 1400 / 1500: loss 2.112833\n",
      "iteration 0 / 1500: loss 1283.045955\n",
      "iteration 100 / 1500: loss 2.128037\n",
      "iteration 200 / 1500: loss 2.150229\n",
      "iteration 300 / 1500: loss 2.073849\n",
      "iteration 400 / 1500: loss 2.103140\n",
      "iteration 500 / 1500: loss 2.118339\n",
      "iteration 600 / 1500: loss 2.133951\n",
      "iteration 700 / 1500: loss 2.159427\n",
      "iteration 800 / 1500: loss 2.132713\n",
      "iteration 900 / 1500: loss 2.164000\n",
      "iteration 1000 / 1500: loss 2.136484\n",
      "iteration 1100 / 1500: loss 2.177429\n",
      "iteration 1200 / 1500: loss 2.186080\n",
      "iteration 1300 / 1500: loss 2.086981\n",
      "iteration 1400 / 1500: loss 2.090379\n",
      "iteration 0 / 1500: loss 1320.673209\n",
      "iteration 100 / 1500: loss 2.156155\n",
      "iteration 200 / 1500: loss 2.130813\n",
      "iteration 300 / 1500: loss 2.105135\n",
      "iteration 400 / 1500: loss 2.141541\n",
      "iteration 500 / 1500: loss 2.126422\n",
      "iteration 600 / 1500: loss 2.156472\n",
      "iteration 700 / 1500: loss 2.125357\n",
      "iteration 800 / 1500: loss 2.139306\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 1500: loss 2.080604\n",
      "iteration 1000 / 1500: loss 2.129810\n",
      "iteration 1100 / 1500: loss 2.122698\n",
      "iteration 1200 / 1500: loss 2.127352\n",
      "iteration 1300 / 1500: loss 2.103958\n",
      "iteration 1400 / 1500: loss 2.177549\n",
      "iteration 0 / 1500: loss 1370.307307\n",
      "iteration 100 / 1500: loss 2.122979\n",
      "iteration 200 / 1500: loss 2.144230\n",
      "iteration 300 / 1500: loss 2.109000\n",
      "iteration 400 / 1500: loss 2.133653\n",
      "iteration 500 / 1500: loss 2.077046\n",
      "iteration 600 / 1500: loss 2.130199\n",
      "iteration 700 / 1500: loss 2.126755\n",
      "iteration 800 / 1500: loss 2.150070\n",
      "iteration 900 / 1500: loss 2.220974\n",
      "iteration 1000 / 1500: loss 2.121882\n",
      "iteration 1100 / 1500: loss 2.141543\n",
      "iteration 1200 / 1500: loss 2.143362\n",
      "iteration 1300 / 1500: loss 2.158054\n",
      "iteration 1400 / 1500: loss 2.136178\n",
      "iteration 0 / 1500: loss 1372.638042\n",
      "iteration 100 / 1500: loss 2.124164\n",
      "iteration 200 / 1500: loss 2.144843\n",
      "iteration 300 / 1500: loss 2.122237\n",
      "iteration 400 / 1500: loss 2.139737\n",
      "iteration 500 / 1500: loss 2.178662\n",
      "iteration 600 / 1500: loss 2.180498\n",
      "iteration 700 / 1500: loss 2.172744\n",
      "iteration 800 / 1500: loss 2.129245\n",
      "iteration 900 / 1500: loss 2.177085\n",
      "iteration 1000 / 1500: loss 2.093483\n",
      "iteration 1100 / 1500: loss 2.137597\n",
      "iteration 1200 / 1500: loss 2.074335\n",
      "iteration 1300 / 1500: loss 2.156374\n",
      "iteration 1400 / 1500: loss 2.115980\n",
      "iteration 0 / 1500: loss 1420.272496\n",
      "iteration 100 / 1500: loss 2.120733\n",
      "iteration 200 / 1500: loss 2.142149\n",
      "iteration 300 / 1500: loss 2.149612\n",
      "iteration 400 / 1500: loss 2.168844\n",
      "iteration 500 / 1500: loss 2.155669\n",
      "iteration 600 / 1500: loss 2.127379\n",
      "iteration 700 / 1500: loss 2.090597\n",
      "iteration 800 / 1500: loss 2.157802\n",
      "iteration 900 / 1500: loss 2.144267\n",
      "iteration 1000 / 1500: loss 2.132690\n",
      "iteration 1100 / 1500: loss 2.178082\n",
      "iteration 1200 / 1500: loss 2.116760\n",
      "iteration 1300 / 1500: loss 2.141842\n",
      "iteration 1400 / 1500: loss 2.118227\n",
      "iteration 0 / 1500: loss 1435.803004\n",
      "iteration 100 / 1500: loss 2.134094\n",
      "iteration 200 / 1500: loss 2.102014\n",
      "iteration 300 / 1500: loss 2.189611\n",
      "iteration 400 / 1500: loss 2.193233\n",
      "iteration 500 / 1500: loss 2.182614\n",
      "iteration 600 / 1500: loss 2.126649\n",
      "iteration 700 / 1500: loss 2.121330\n",
      "iteration 800 / 1500: loss 2.092894\n",
      "iteration 900 / 1500: loss 2.137484\n",
      "iteration 1000 / 1500: loss 2.107986\n",
      "iteration 1100 / 1500: loss 2.126064\n",
      "iteration 1200 / 1500: loss 2.117796\n",
      "iteration 1300 / 1500: loss 2.140762\n",
      "iteration 1400 / 1500: loss 2.210631\n",
      "iteration 0 / 1500: loss 1475.713359\n",
      "iteration 100 / 1500: loss 2.114381\n",
      "iteration 200 / 1500: loss 2.076916\n",
      "iteration 300 / 1500: loss 2.167641\n",
      "iteration 400 / 1500: loss 2.149882\n",
      "iteration 500 / 1500: loss 2.130561\n",
      "iteration 600 / 1500: loss 2.150640\n",
      "iteration 700 / 1500: loss 2.152450\n",
      "iteration 800 / 1500: loss 2.145279\n",
      "iteration 900 / 1500: loss 2.128080\n",
      "iteration 1000 / 1500: loss 2.112080\n",
      "iteration 1100 / 1500: loss 2.099412\n",
      "iteration 1200 / 1500: loss 2.158539\n",
      "iteration 1300 / 1500: loss 2.142509\n",
      "iteration 1400 / 1500: loss 2.148425\n",
      "iteration 0 / 1500: loss 1493.191705\n",
      "iteration 100 / 1500: loss 2.157589\n",
      "iteration 200 / 1500: loss 2.137320\n",
      "iteration 300 / 1500: loss 2.114013\n",
      "iteration 400 / 1500: loss 2.197433\n",
      "iteration 500 / 1500: loss 2.122130\n",
      "iteration 600 / 1500: loss 2.137417\n",
      "iteration 700 / 1500: loss 2.152566\n",
      "iteration 800 / 1500: loss 2.109083\n",
      "iteration 900 / 1500: loss 2.146325\n",
      "iteration 1000 / 1500: loss 2.104142\n",
      "iteration 1100 / 1500: loss 2.102383\n",
      "iteration 1200 / 1500: loss 2.173433\n",
      "iteration 1300 / 1500: loss 2.258929\n",
      "iteration 1400 / 1500: loss 2.163183\n",
      "iteration 0 / 1500: loss 1541.524672\n",
      "iteration 100 / 1500: loss 2.110900\n",
      "iteration 200 / 1500: loss 2.087720\n",
      "iteration 300 / 1500: loss 2.137406\n",
      "iteration 400 / 1500: loss 2.137942\n",
      "iteration 500 / 1500: loss 2.199399\n",
      "iteration 600 / 1500: loss 2.116283\n",
      "iteration 700 / 1500: loss 2.133563\n",
      "iteration 800 / 1500: loss 2.167648\n",
      "iteration 900 / 1500: loss 2.119260\n",
      "iteration 1000 / 1500: loss 2.150639\n",
      "iteration 1100 / 1500: loss 2.209454\n",
      "iteration 1200 / 1500: loss 2.116613\n",
      "iteration 1300 / 1500: loss 2.164977\n",
      "iteration 1400 / 1500: loss 2.146383\n",
      "iteration 0 / 1500: loss 785.303445\n",
      "iteration 100 / 1500: loss 2.080894\n",
      "iteration 200 / 1500: loss 2.065895\n",
      "iteration 300 / 1500: loss 2.155036\n",
      "iteration 400 / 1500: loss 2.089821\n",
      "iteration 500 / 1500: loss 2.147460\n",
      "iteration 600 / 1500: loss 2.115160\n",
      "iteration 700 / 1500: loss 2.139318\n",
      "iteration 800 / 1500: loss 2.099608\n",
      "iteration 900 / 1500: loss 2.026022\n",
      "iteration 1000 / 1500: loss 1.991320\n",
      "iteration 1100 / 1500: loss 2.065117\n",
      "iteration 1200 / 1500: loss 2.068433\n",
      "iteration 1300 / 1500: loss 2.116994\n",
      "iteration 1400 / 1500: loss 2.076766\n",
      "iteration 0 / 1500: loss 806.580156\n",
      "iteration 100 / 1500: loss 2.132388\n",
      "iteration 200 / 1500: loss 2.090445\n",
      "iteration 300 / 1500: loss 2.056027\n",
      "iteration 400 / 1500: loss 2.090314\n",
      "iteration 500 / 1500: loss 2.090604\n",
      "iteration 600 / 1500: loss 2.086565\n",
      "iteration 700 / 1500: loss 2.091116\n",
      "iteration 800 / 1500: loss 2.069892\n",
      "iteration 900 / 1500: loss 2.087927\n",
      "iteration 1000 / 1500: loss 2.105046\n",
      "iteration 1100 / 1500: loss 2.128754\n",
      "iteration 1200 / 1500: loss 2.086262\n",
      "iteration 1300 / 1500: loss 2.105735\n",
      "iteration 1400 / 1500: loss 2.140764\n",
      "iteration 0 / 1500: loss 819.276031\n",
      "iteration 100 / 1500: loss 2.074639\n",
      "iteration 200 / 1500: loss 2.114884\n",
      "iteration 300 / 1500: loss 2.067616\n",
      "iteration 400 / 1500: loss 2.089116\n",
      "iteration 500 / 1500: loss 2.102903\n",
      "iteration 600 / 1500: loss 2.075101\n",
      "iteration 700 / 1500: loss 2.129081\n",
      "iteration 800 / 1500: loss 2.101113\n",
      "iteration 900 / 1500: loss 2.091523\n",
      "iteration 1000 / 1500: loss 2.161330\n",
      "iteration 1100 / 1500: loss 2.091587\n",
      "iteration 1200 / 1500: loss 2.059289\n",
      "iteration 1300 / 1500: loss 2.117426\n",
      "iteration 1400 / 1500: loss 2.066973\n",
      "iteration 0 / 1500: loss 873.379914\n",
      "iteration 100 / 1500: loss 2.157830\n",
      "iteration 200 / 1500: loss 2.095568\n",
      "iteration 300 / 1500: loss 2.056397\n",
      "iteration 400 / 1500: loss 2.090825\n",
      "iteration 500 / 1500: loss 2.067806\n",
      "iteration 600 / 1500: loss 2.160741\n",
      "iteration 700 / 1500: loss 2.084874\n",
      "iteration 800 / 1500: loss 2.204212\n",
      "iteration 900 / 1500: loss 2.127488\n",
      "iteration 1000 / 1500: loss 2.042530\n",
      "iteration 1100 / 1500: loss 2.038227\n",
      "iteration 1200 / 1500: loss 2.094126\n",
      "iteration 1300 / 1500: loss 2.140532\n",
      "iteration 1400 / 1500: loss 1.983271\n",
      "iteration 0 / 1500: loss 905.389251\n",
      "iteration 100 / 1500: loss 2.181331\n",
      "iteration 200 / 1500: loss 2.129192\n",
      "iteration 300 / 1500: loss 2.061371\n",
      "iteration 400 / 1500: loss 2.137139\n",
      "iteration 500 / 1500: loss 2.148232\n",
      "iteration 600 / 1500: loss 2.078014\n",
      "iteration 700 / 1500: loss 2.137067\n",
      "iteration 800 / 1500: loss 2.130093\n",
      "iteration 900 / 1500: loss 2.225654\n",
      "iteration 1000 / 1500: loss 2.118802\n",
      "iteration 1100 / 1500: loss 2.074001\n",
      "iteration 1200 / 1500: loss 2.072561\n",
      "iteration 1300 / 1500: loss 2.172376\n",
      "iteration 1400 / 1500: loss 2.112390\n",
      "iteration 0 / 1500: loss 917.062055\n",
      "iteration 100 / 1500: loss 2.140950\n",
      "iteration 200 / 1500: loss 2.097703\n",
      "iteration 300 / 1500: loss 2.070463\n",
      "iteration 400 / 1500: loss 2.126558\n",
      "iteration 500 / 1500: loss 2.133702\n",
      "iteration 600 / 1500: loss 2.120105\n",
      "iteration 700 / 1500: loss 2.045103\n",
      "iteration 800 / 1500: loss 2.081023\n",
      "iteration 900 / 1500: loss 2.116283\n",
      "iteration 1000 / 1500: loss 2.051276\n",
      "iteration 1100 / 1500: loss 2.140762\n",
      "iteration 1200 / 1500: loss 2.080610\n",
      "iteration 1300 / 1500: loss 2.070936\n",
      "iteration 1400 / 1500: loss 2.132085\n",
      "iteration 0 / 1500: loss 967.760271\n",
      "iteration 100 / 1500: loss 2.088652\n",
      "iteration 200 / 1500: loss 2.112697\n",
      "iteration 300 / 1500: loss 2.123904\n",
      "iteration 400 / 1500: loss 2.075755\n",
      "iteration 500 / 1500: loss 2.053137\n",
      "iteration 600 / 1500: loss 2.150063\n",
      "iteration 700 / 1500: loss 2.106376\n",
      "iteration 800 / 1500: loss 2.087177\n",
      "iteration 900 / 1500: loss 2.127046\n",
      "iteration 1000 / 1500: loss 2.095401\n",
      "iteration 1100 / 1500: loss 2.201378\n",
      "iteration 1200 / 1500: loss 2.059495\n",
      "iteration 1300 / 1500: loss 2.109236\n",
      "iteration 1400 / 1500: loss 2.131172\n",
      "iteration 0 / 1500: loss 984.955482\n",
      "iteration 100 / 1500: loss 2.133882\n",
      "iteration 200 / 1500: loss 2.118501\n",
      "iteration 300 / 1500: loss 2.047496\n",
      "iteration 400 / 1500: loss 2.181551\n",
      "iteration 500 / 1500: loss 2.087810\n",
      "iteration 600 / 1500: loss 2.098386\n",
      "iteration 700 / 1500: loss 2.090004\n",
      "iteration 800 / 1500: loss 2.101716\n",
      "iteration 900 / 1500: loss 2.075744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 1500: loss 2.078057\n",
      "iteration 1100 / 1500: loss 2.180492\n",
      "iteration 1200 / 1500: loss 2.131513\n",
      "iteration 1300 / 1500: loss 2.078961\n",
      "iteration 1400 / 1500: loss 2.116283\n",
      "iteration 0 / 1500: loss 1015.837184\n",
      "iteration 100 / 1500: loss 2.114269\n",
      "iteration 200 / 1500: loss 2.057954\n",
      "iteration 300 / 1500: loss 2.136565\n",
      "iteration 400 / 1500: loss 2.122551\n",
      "iteration 500 / 1500: loss 2.140531\n",
      "iteration 600 / 1500: loss 2.088353\n",
      "iteration 700 / 1500: loss 2.098041\n",
      "iteration 800 / 1500: loss 2.095725\n",
      "iteration 900 / 1500: loss 2.088775\n",
      "iteration 1000 / 1500: loss 2.196147\n",
      "iteration 1100 / 1500: loss 2.124128\n",
      "iteration 1200 / 1500: loss 2.151551\n",
      "iteration 1300 / 1500: loss 2.071622\n",
      "iteration 1400 / 1500: loss 2.067774\n",
      "iteration 0 / 1500: loss 1053.387442\n",
      "iteration 100 / 1500: loss 2.125642\n",
      "iteration 200 / 1500: loss 2.121718\n",
      "iteration 300 / 1500: loss 2.128837\n",
      "iteration 400 / 1500: loss 2.119305\n",
      "iteration 500 / 1500: loss 2.089274\n",
      "iteration 600 / 1500: loss 2.101470\n",
      "iteration 700 / 1500: loss 2.219379\n",
      "iteration 800 / 1500: loss 2.134315\n",
      "iteration 900 / 1500: loss 2.106147\n",
      "iteration 1000 / 1500: loss 2.109491\n",
      "iteration 1100 / 1500: loss 2.107976\n",
      "iteration 1200 / 1500: loss 2.101544\n",
      "iteration 1300 / 1500: loss 2.060855\n",
      "iteration 1400 / 1500: loss 2.064052\n",
      "iteration 0 / 1500: loss 1066.985778\n",
      "iteration 100 / 1500: loss 2.189939\n",
      "iteration 200 / 1500: loss 2.180406\n",
      "iteration 300 / 1500: loss 2.132011\n",
      "iteration 400 / 1500: loss 2.129127\n",
      "iteration 500 / 1500: loss 2.146569\n",
      "iteration 600 / 1500: loss 2.077753\n",
      "iteration 700 / 1500: loss 2.075964\n",
      "iteration 800 / 1500: loss 2.066073\n",
      "iteration 900 / 1500: loss 2.162398\n",
      "iteration 1000 / 1500: loss 2.079364\n",
      "iteration 1100 / 1500: loss 2.172340\n",
      "iteration 1200 / 1500: loss 2.123464\n",
      "iteration 1300 / 1500: loss 2.096273\n",
      "iteration 1400 / 1500: loss 2.063138\n",
      "iteration 0 / 1500: loss 1115.353913\n",
      "iteration 100 / 1500: loss 2.058819\n",
      "iteration 200 / 1500: loss 2.059223\n",
      "iteration 300 / 1500: loss 2.164276\n",
      "iteration 400 / 1500: loss 2.092724\n",
      "iteration 500 / 1500: loss 2.142585\n",
      "iteration 600 / 1500: loss 2.139281\n",
      "iteration 700 / 1500: loss 2.095228\n",
      "iteration 800 / 1500: loss 2.132843\n",
      "iteration 900 / 1500: loss 2.125418\n",
      "iteration 1000 / 1500: loss 2.064130\n",
      "iteration 1100 / 1500: loss 2.089150\n",
      "iteration 1200 / 1500: loss 2.135001\n",
      "iteration 1300 / 1500: loss 2.113307\n",
      "iteration 1400 / 1500: loss 2.030464\n",
      "iteration 0 / 1500: loss 1138.027689\n",
      "iteration 100 / 1500: loss 2.146219\n",
      "iteration 200 / 1500: loss 2.122564\n",
      "iteration 300 / 1500: loss 2.086915\n",
      "iteration 400 / 1500: loss 2.149039\n",
      "iteration 500 / 1500: loss 2.100448\n",
      "iteration 600 / 1500: loss 2.107145\n",
      "iteration 700 / 1500: loss 2.180328\n",
      "iteration 800 / 1500: loss 2.047629\n",
      "iteration 900 / 1500: loss 2.120725\n",
      "iteration 1000 / 1500: loss 2.080888\n",
      "iteration 1100 / 1500: loss 2.123023\n",
      "iteration 1200 / 1500: loss 2.091524\n",
      "iteration 1300 / 1500: loss 2.137319\n",
      "iteration 1400 / 1500: loss 2.155161\n",
      "iteration 0 / 1500: loss 1181.023686\n",
      "iteration 100 / 1500: loss 2.147645\n",
      "iteration 200 / 1500: loss 2.117894\n",
      "iteration 300 / 1500: loss 2.140502\n",
      "iteration 400 / 1500: loss 2.179860\n",
      "iteration 500 / 1500: loss 2.118778\n",
      "iteration 600 / 1500: loss 2.159765\n",
      "iteration 700 / 1500: loss 2.159450\n",
      "iteration 800 / 1500: loss 2.076806\n",
      "iteration 900 / 1500: loss 2.106501\n",
      "iteration 1000 / 1500: loss 2.108316\n",
      "iteration 1100 / 1500: loss 2.095696\n",
      "iteration 1200 / 1500: loss 2.115124\n",
      "iteration 1300 / 1500: loss 2.175786\n",
      "iteration 1400 / 1500: loss 2.128483\n",
      "iteration 0 / 1500: loss 1200.007628\n",
      "iteration 100 / 1500: loss 2.129866\n",
      "iteration 200 / 1500: loss 2.183138\n",
      "iteration 300 / 1500: loss 2.124150\n",
      "iteration 400 / 1500: loss 2.137918\n",
      "iteration 500 / 1500: loss 2.065238\n",
      "iteration 600 / 1500: loss 2.117547\n",
      "iteration 700 / 1500: loss 2.164873\n",
      "iteration 800 / 1500: loss 2.075712\n",
      "iteration 900 / 1500: loss 2.159459\n",
      "iteration 1000 / 1500: loss 2.112916\n",
      "iteration 1100 / 1500: loss 2.186362\n",
      "iteration 1200 / 1500: loss 2.141891\n",
      "iteration 1300 / 1500: loss 2.090916\n",
      "iteration 1400 / 1500: loss 2.173363\n",
      "iteration 0 / 1500: loss 1247.433583\n",
      "iteration 100 / 1500: loss 2.133935\n",
      "iteration 200 / 1500: loss 2.199953\n",
      "iteration 300 / 1500: loss 2.128641\n",
      "iteration 400 / 1500: loss 2.112298\n",
      "iteration 500 / 1500: loss 2.128659\n",
      "iteration 600 / 1500: loss 2.129666\n",
      "iteration 700 / 1500: loss 2.124210\n",
      "iteration 800 / 1500: loss 2.152032\n",
      "iteration 900 / 1500: loss 2.086056\n",
      "iteration 1000 / 1500: loss 2.114646\n",
      "iteration 1100 / 1500: loss 2.106563\n",
      "iteration 1200 / 1500: loss 2.144236\n",
      "iteration 1300 / 1500: loss 2.081025\n",
      "iteration 1400 / 1500: loss 2.102403\n",
      "iteration 0 / 1500: loss 1263.073252\n",
      "iteration 100 / 1500: loss 2.155375\n",
      "iteration 200 / 1500: loss 2.113262\n",
      "iteration 300 / 1500: loss 2.098816\n",
      "iteration 400 / 1500: loss 2.094901\n",
      "iteration 500 / 1500: loss 2.113370\n",
      "iteration 600 / 1500: loss 2.175435\n",
      "iteration 700 / 1500: loss 2.104538\n",
      "iteration 800 / 1500: loss 2.180706\n",
      "iteration 900 / 1500: loss 2.181125\n",
      "iteration 1000 / 1500: loss 2.184208\n",
      "iteration 1100 / 1500: loss 2.109780\n",
      "iteration 1200 / 1500: loss 2.135078\n",
      "iteration 1300 / 1500: loss 2.123534\n",
      "iteration 1400 / 1500: loss 2.124721\n",
      "iteration 0 / 1500: loss 1318.591799\n",
      "iteration 100 / 1500: loss 2.133259\n",
      "iteration 200 / 1500: loss 2.189993\n",
      "iteration 300 / 1500: loss 2.092225\n",
      "iteration 400 / 1500: loss 2.146636\n",
      "iteration 500 / 1500: loss 2.165788\n",
      "iteration 600 / 1500: loss 2.139797\n",
      "iteration 700 / 1500: loss 2.074816\n",
      "iteration 800 / 1500: loss 2.114339\n",
      "iteration 900 / 1500: loss 2.158485\n",
      "iteration 1000 / 1500: loss 2.190426\n",
      "iteration 1100 / 1500: loss 2.092762\n",
      "iteration 1200 / 1500: loss 2.141531\n",
      "iteration 1300 / 1500: loss 2.152864\n",
      "iteration 1400 / 1500: loss 2.167767\n",
      "iteration 0 / 1500: loss 1330.992345\n",
      "iteration 100 / 1500: loss 2.141689\n",
      "iteration 200 / 1500: loss 2.167900\n",
      "iteration 300 / 1500: loss 2.141808\n",
      "iteration 400 / 1500: loss 2.149263\n",
      "iteration 500 / 1500: loss 2.116557\n",
      "iteration 600 / 1500: loss 2.111894\n",
      "iteration 700 / 1500: loss 2.091268\n",
      "iteration 800 / 1500: loss 2.159410\n",
      "iteration 900 / 1500: loss 2.169205\n",
      "iteration 1000 / 1500: loss 2.127792\n",
      "iteration 1100 / 1500: loss 2.192849\n",
      "iteration 1200 / 1500: loss 2.154697\n",
      "iteration 1300 / 1500: loss 2.119141\n",
      "iteration 1400 / 1500: loss 2.122498\n",
      "iteration 0 / 1500: loss 1378.085108\n",
      "iteration 100 / 1500: loss 2.161123\n",
      "iteration 200 / 1500: loss 2.110891\n",
      "iteration 300 / 1500: loss 2.190668\n",
      "iteration 400 / 1500: loss 2.098245\n",
      "iteration 500 / 1500: loss 2.106197\n",
      "iteration 600 / 1500: loss 2.094812\n",
      "iteration 700 / 1500: loss 2.121694\n",
      "iteration 800 / 1500: loss 2.105632\n",
      "iteration 900 / 1500: loss 2.138923\n",
      "iteration 1000 / 1500: loss 2.134500\n",
      "iteration 1100 / 1500: loss 2.143063\n",
      "iteration 1200 / 1500: loss 2.150374\n",
      "iteration 1300 / 1500: loss 2.151937\n",
      "iteration 1400 / 1500: loss 2.121425\n",
      "iteration 0 / 1500: loss 1389.630647\n",
      "iteration 100 / 1500: loss 2.148262\n",
      "iteration 200 / 1500: loss 2.117325\n",
      "iteration 300 / 1500: loss 2.162767\n",
      "iteration 400 / 1500: loss 2.102370\n",
      "iteration 500 / 1500: loss 2.136318\n",
      "iteration 600 / 1500: loss 2.175463\n",
      "iteration 700 / 1500: loss 2.142447\n",
      "iteration 800 / 1500: loss 2.128272\n",
      "iteration 900 / 1500: loss 2.102039\n",
      "iteration 1000 / 1500: loss 2.157009\n",
      "iteration 1100 / 1500: loss 2.098654\n",
      "iteration 1200 / 1500: loss 2.118409\n",
      "iteration 1300 / 1500: loss 2.130474\n",
      "iteration 1400 / 1500: loss 2.196982\n",
      "iteration 0 / 1500: loss 1404.648265\n",
      "iteration 100 / 1500: loss 2.194604\n",
      "iteration 200 / 1500: loss 2.161069\n",
      "iteration 300 / 1500: loss 2.123076\n",
      "iteration 400 / 1500: loss 2.162705\n",
      "iteration 500 / 1500: loss 2.184288\n",
      "iteration 600 / 1500: loss 2.226083\n",
      "iteration 700 / 1500: loss 2.109507\n",
      "iteration 800 / 1500: loss 2.128016\n",
      "iteration 900 / 1500: loss 2.138350\n",
      "iteration 1000 / 1500: loss 2.148792\n",
      "iteration 1100 / 1500: loss 2.153586\n",
      "iteration 1200 / 1500: loss 2.123679\n",
      "iteration 1300 / 1500: loss 2.115564\n",
      "iteration 1400 / 1500: loss 2.170170\n",
      "iteration 0 / 1500: loss 1464.549094\n",
      "iteration 100 / 1500: loss 2.113689\n",
      "iteration 200 / 1500: loss 2.139148\n",
      "iteration 300 / 1500: loss 2.093608\n",
      "iteration 400 / 1500: loss 2.153502\n",
      "iteration 500 / 1500: loss 2.105227\n",
      "iteration 600 / 1500: loss 2.178462\n",
      "iteration 700 / 1500: loss 2.104517\n",
      "iteration 800 / 1500: loss 2.151142\n",
      "iteration 900 / 1500: loss 2.100939\n",
      "iteration 1000 / 1500: loss 2.176873\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 1500: loss 2.135706\n",
      "iteration 1200 / 1500: loss 2.181920\n",
      "iteration 1300 / 1500: loss 2.141433\n",
      "iteration 1400 / 1500: loss 2.177561\n",
      "iteration 0 / 1500: loss 1485.771565\n",
      "iteration 100 / 1500: loss 2.108668\n",
      "iteration 200 / 1500: loss 2.177953\n",
      "iteration 300 / 1500: loss 2.148658\n",
      "iteration 400 / 1500: loss 2.163408\n",
      "iteration 500 / 1500: loss 2.113890\n",
      "iteration 600 / 1500: loss 2.118553\n",
      "iteration 700 / 1500: loss 2.196651\n",
      "iteration 800 / 1500: loss 2.141994\n",
      "iteration 900 / 1500: loss 2.123999\n",
      "iteration 1000 / 1500: loss 2.128402\n",
      "iteration 1100 / 1500: loss 2.176741\n",
      "iteration 1200 / 1500: loss 2.129885\n",
      "iteration 1300 / 1500: loss 2.152969\n",
      "iteration 1400 / 1500: loss 2.122128\n",
      "iteration 0 / 1500: loss 1513.172861\n",
      "iteration 100 / 1500: loss 2.138902\n",
      "iteration 200 / 1500: loss 2.174003\n",
      "iteration 300 / 1500: loss 2.172454\n",
      "iteration 400 / 1500: loss 2.180696\n",
      "iteration 500 / 1500: loss 2.171683\n",
      "iteration 600 / 1500: loss 2.207414\n",
      "iteration 700 / 1500: loss 2.130727\n",
      "iteration 800 / 1500: loss 2.117856\n",
      "iteration 900 / 1500: loss 2.192989\n",
      "iteration 1000 / 1500: loss 2.103643\n",
      "iteration 1100 / 1500: loss 2.155555\n",
      "iteration 1200 / 1500: loss 2.148817\n",
      "iteration 1300 / 1500: loss 2.095496\n",
      "iteration 1400 / 1500: loss 2.132887\n",
      "iteration 0 / 1500: loss 1536.733080\n",
      "iteration 100 / 1500: loss 2.167980\n",
      "iteration 200 / 1500: loss 2.134713\n",
      "iteration 300 / 1500: loss 2.152762\n",
      "iteration 400 / 1500: loss 2.144710\n",
      "iteration 500 / 1500: loss 2.124756\n",
      "iteration 600 / 1500: loss 2.186017\n",
      "iteration 700 / 1500: loss 2.133714\n",
      "iteration 800 / 1500: loss 2.140701\n",
      "iteration 900 / 1500: loss 2.132275\n",
      "iteration 1000 / 1500: loss 2.127609\n",
      "iteration 1100 / 1500: loss 2.170971\n",
      "iteration 1200 / 1500: loss 2.188430\n",
      "iteration 1300 / 1500: loss 2.172621\n",
      "iteration 1400 / 1500: loss 2.183581\n",
      "iteration 0 / 1500: loss 772.193970\n",
      "iteration 100 / 1500: loss 2.011153\n",
      "iteration 200 / 1500: loss 2.103502\n",
      "iteration 300 / 1500: loss 2.104130\n",
      "iteration 400 / 1500: loss 2.105412\n",
      "iteration 500 / 1500: loss 2.141404\n",
      "iteration 600 / 1500: loss 2.185347\n",
      "iteration 700 / 1500: loss 2.117955\n",
      "iteration 800 / 1500: loss 2.086409\n",
      "iteration 900 / 1500: loss 2.113846\n",
      "iteration 1000 / 1500: loss 2.149949\n",
      "iteration 1100 / 1500: loss 2.095323\n",
      "iteration 1200 / 1500: loss 1.985299\n",
      "iteration 1300 / 1500: loss 2.066475\n",
      "iteration 1400 / 1500: loss 2.138961\n",
      "iteration 0 / 1500: loss 810.208479\n",
      "iteration 100 / 1500: loss 2.083233\n",
      "iteration 200 / 1500: loss 2.139582\n",
      "iteration 300 / 1500: loss 2.042289\n",
      "iteration 400 / 1500: loss 2.113040\n",
      "iteration 500 / 1500: loss 2.072165\n",
      "iteration 600 / 1500: loss 2.163518\n",
      "iteration 700 / 1500: loss 2.144397\n",
      "iteration 800 / 1500: loss 2.137110\n",
      "iteration 900 / 1500: loss 2.163381\n",
      "iteration 1000 / 1500: loss 2.122514\n",
      "iteration 1100 / 1500: loss 2.100437\n",
      "iteration 1200 / 1500: loss 2.111660\n",
      "iteration 1300 / 1500: loss 2.161727\n",
      "iteration 1400 / 1500: loss 2.056351\n",
      "iteration 0 / 1500: loss 833.471947\n",
      "iteration 100 / 1500: loss 2.098204\n",
      "iteration 200 / 1500: loss 2.112145\n",
      "iteration 300 / 1500: loss 2.111757\n",
      "iteration 400 / 1500: loss 2.140810\n",
      "iteration 500 / 1500: loss 2.071894\n",
      "iteration 600 / 1500: loss 2.145820\n",
      "iteration 700 / 1500: loss 2.051838\n",
      "iteration 800 / 1500: loss 2.149216\n",
      "iteration 900 / 1500: loss 2.104231\n",
      "iteration 1000 / 1500: loss 2.084799\n",
      "iteration 1100 / 1500: loss 2.107476\n",
      "iteration 1200 / 1500: loss 2.071985\n",
      "iteration 1300 / 1500: loss 2.103072\n",
      "iteration 1400 / 1500: loss 2.200973\n",
      "iteration 0 / 1500: loss 864.685310\n",
      "iteration 100 / 1500: loss 2.077438\n",
      "iteration 200 / 1500: loss 2.105540\n",
      "iteration 300 / 1500: loss 2.132463\n",
      "iteration 400 / 1500: loss 2.128912\n",
      "iteration 500 / 1500: loss 2.123090\n",
      "iteration 600 / 1500: loss 2.131084\n",
      "iteration 700 / 1500: loss 2.137975\n",
      "iteration 800 / 1500: loss 2.113065\n",
      "iteration 900 / 1500: loss 2.150685\n",
      "iteration 1000 / 1500: loss 2.170459\n",
      "iteration 1100 / 1500: loss 2.132246\n",
      "iteration 1200 / 1500: loss 2.172356\n",
      "iteration 1300 / 1500: loss 2.142804\n",
      "iteration 1400 / 1500: loss 2.117617\n",
      "iteration 0 / 1500: loss 890.949382\n",
      "iteration 100 / 1500: loss 2.163857\n",
      "iteration 200 / 1500: loss 2.134405\n",
      "iteration 300 / 1500: loss 2.132865\n",
      "iteration 400 / 1500: loss 2.169143\n",
      "iteration 500 / 1500: loss 2.095905\n",
      "iteration 600 / 1500: loss 2.138234\n",
      "iteration 700 / 1500: loss 2.107230\n",
      "iteration 800 / 1500: loss 2.100891\n",
      "iteration 900 / 1500: loss 2.080603\n",
      "iteration 1000 / 1500: loss 2.115921\n",
      "iteration 1100 / 1500: loss 2.094800\n",
      "iteration 1200 / 1500: loss 2.112395\n",
      "iteration 1300 / 1500: loss 2.138325\n",
      "iteration 1400 / 1500: loss 2.078777\n",
      "iteration 0 / 1500: loss 924.317437\n",
      "iteration 100 / 1500: loss 2.091014\n",
      "iteration 200 / 1500: loss 2.094023\n",
      "iteration 300 / 1500: loss 2.141796\n",
      "iteration 400 / 1500: loss 2.147148\n",
      "iteration 500 / 1500: loss 2.095814\n",
      "iteration 600 / 1500: loss 2.151046\n",
      "iteration 700 / 1500: loss 2.031121\n",
      "iteration 800 / 1500: loss 2.162431\n",
      "iteration 900 / 1500: loss 2.163564\n",
      "iteration 1000 / 1500: loss 2.113134\n",
      "iteration 1100 / 1500: loss 2.122415\n",
      "iteration 1200 / 1500: loss 2.154636\n",
      "iteration 1300 / 1500: loss 2.168439\n",
      "iteration 1400 / 1500: loss 2.170365\n",
      "iteration 0 / 1500: loss 958.667271\n",
      "iteration 100 / 1500: loss 2.093373\n",
      "iteration 200 / 1500: loss 2.166189\n",
      "iteration 300 / 1500: loss 2.107566\n",
      "iteration 400 / 1500: loss 2.124740\n",
      "iteration 500 / 1500: loss 2.105734\n",
      "iteration 600 / 1500: loss 2.146940\n",
      "iteration 700 / 1500: loss 2.119811\n",
      "iteration 800 / 1500: loss 2.067133\n",
      "iteration 900 / 1500: loss 2.209998\n",
      "iteration 1000 / 1500: loss 2.138275\n",
      "iteration 1100 / 1500: loss 2.109335\n",
      "iteration 1200 / 1500: loss 2.050721\n",
      "iteration 1300 / 1500: loss 2.101842\n",
      "iteration 1400 / 1500: loss 2.098101\n",
      "iteration 0 / 1500: loss 981.871445\n",
      "iteration 100 / 1500: loss 2.096793\n",
      "iteration 200 / 1500: loss 2.145023\n",
      "iteration 300 / 1500: loss 2.078834\n",
      "iteration 400 / 1500: loss 2.087541\n",
      "iteration 500 / 1500: loss 2.144283\n",
      "iteration 600 / 1500: loss 2.148908\n",
      "iteration 700 / 1500: loss 2.015624\n",
      "iteration 800 / 1500: loss 2.024693\n",
      "iteration 900 / 1500: loss 2.157186\n",
      "iteration 1000 / 1500: loss 2.153847\n",
      "iteration 1100 / 1500: loss 2.088689\n",
      "iteration 1200 / 1500: loss 2.113233\n",
      "iteration 1300 / 1500: loss 2.183926\n",
      "iteration 1400 / 1500: loss 2.171978\n",
      "iteration 0 / 1500: loss 1020.787931\n",
      "iteration 100 / 1500: loss 2.150706\n",
      "iteration 200 / 1500: loss 2.141129\n",
      "iteration 300 / 1500: loss 2.123369\n",
      "iteration 400 / 1500: loss 2.100411\n",
      "iteration 500 / 1500: loss 2.099188\n",
      "iteration 600 / 1500: loss 2.084442\n",
      "iteration 700 / 1500: loss 2.091066\n",
      "iteration 800 / 1500: loss 2.128524\n",
      "iteration 900 / 1500: loss 2.174387\n",
      "iteration 1000 / 1500: loss 2.181492\n",
      "iteration 1100 / 1500: loss 2.131015\n",
      "iteration 1200 / 1500: loss 2.224925\n",
      "iteration 1300 / 1500: loss 2.160079\n",
      "iteration 1400 / 1500: loss 2.080904\n",
      "iteration 0 / 1500: loss 1056.079176\n",
      "iteration 100 / 1500: loss 2.129649\n",
      "iteration 200 / 1500: loss 2.124190\n",
      "iteration 300 / 1500: loss 2.132645\n",
      "iteration 400 / 1500: loss 2.113329\n",
      "iteration 500 / 1500: loss 2.082034\n",
      "iteration 600 / 1500: loss 2.092554\n",
      "iteration 700 / 1500: loss 2.077977\n",
      "iteration 800 / 1500: loss 2.139616\n",
      "iteration 900 / 1500: loss 2.106552\n",
      "iteration 1000 / 1500: loss 2.082260\n",
      "iteration 1100 / 1500: loss 2.087560\n",
      "iteration 1200 / 1500: loss 2.163177\n",
      "iteration 1300 / 1500: loss 2.155611\n",
      "iteration 1400 / 1500: loss 2.061788\n",
      "iteration 0 / 1500: loss 1077.573279\n",
      "iteration 100 / 1500: loss 2.114437\n",
      "iteration 200 / 1500: loss 2.125423\n",
      "iteration 300 / 1500: loss 2.145774\n",
      "iteration 400 / 1500: loss 2.106056\n",
      "iteration 500 / 1500: loss 2.186851\n",
      "iteration 600 / 1500: loss 2.143880\n",
      "iteration 700 / 1500: loss 2.136299\n",
      "iteration 800 / 1500: loss 2.126869\n",
      "iteration 900 / 1500: loss 2.127561\n",
      "iteration 1000 / 1500: loss 2.112536\n",
      "iteration 1100 / 1500: loss 2.047562\n",
      "iteration 1200 / 1500: loss 2.087674\n",
      "iteration 1300 / 1500: loss 2.164159\n",
      "iteration 1400 / 1500: loss 2.085569\n",
      "iteration 0 / 1500: loss 1097.863287\n",
      "iteration 100 / 1500: loss 2.123141\n",
      "iteration 200 / 1500: loss 2.055130\n",
      "iteration 300 / 1500: loss 2.111202\n",
      "iteration 400 / 1500: loss 2.094583\n",
      "iteration 500 / 1500: loss 2.139946\n",
      "iteration 600 / 1500: loss 2.114716\n",
      "iteration 700 / 1500: loss 2.068034\n",
      "iteration 800 / 1500: loss 2.179765\n",
      "iteration 900 / 1500: loss 2.137600\n",
      "iteration 1000 / 1500: loss 2.123181\n",
      "iteration 1100 / 1500: loss 2.094026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 1500: loss 2.105860\n",
      "iteration 1300 / 1500: loss 2.195764\n",
      "iteration 1400 / 1500: loss 2.142576\n",
      "iteration 0 / 1500: loss 1143.554300\n",
      "iteration 100 / 1500: loss 2.212955\n",
      "iteration 200 / 1500: loss 2.147314\n",
      "iteration 300 / 1500: loss 2.101932\n",
      "iteration 400 / 1500: loss 2.087938\n",
      "iteration 500 / 1500: loss 2.132756\n",
      "iteration 600 / 1500: loss 2.064258\n",
      "iteration 700 / 1500: loss 2.131661\n",
      "iteration 800 / 1500: loss 2.120519\n",
      "iteration 900 / 1500: loss 2.105023\n",
      "iteration 1000 / 1500: loss 2.091696\n",
      "iteration 1100 / 1500: loss 2.134973\n",
      "iteration 1200 / 1500: loss 2.123523\n",
      "iteration 1300 / 1500: loss 2.166489\n",
      "iteration 1400 / 1500: loss 2.115806\n",
      "iteration 0 / 1500: loss 1162.845660\n",
      "iteration 100 / 1500: loss 2.181727\n",
      "iteration 200 / 1500: loss 2.136005\n",
      "iteration 300 / 1500: loss 2.155271\n",
      "iteration 400 / 1500: loss 2.141408\n",
      "iteration 500 / 1500: loss 2.070612\n",
      "iteration 600 / 1500: loss 2.127031\n",
      "iteration 700 / 1500: loss 2.205844\n",
      "iteration 800 / 1500: loss 2.199432\n",
      "iteration 900 / 1500: loss 2.125847\n",
      "iteration 1000 / 1500: loss 2.114791\n",
      "iteration 1100 / 1500: loss 2.164058\n",
      "iteration 1200 / 1500: loss 2.153999\n",
      "iteration 1300 / 1500: loss 2.175280\n",
      "iteration 1400 / 1500: loss 2.098623\n",
      "iteration 0 / 1500: loss 1206.883105\n",
      "iteration 100 / 1500: loss 2.142770\n",
      "iteration 200 / 1500: loss 2.180508\n",
      "iteration 300 / 1500: loss 2.133251\n",
      "iteration 400 / 1500: loss 2.153393\n",
      "iteration 500 / 1500: loss 2.097871\n",
      "iteration 600 / 1500: loss 2.125337\n",
      "iteration 700 / 1500: loss 2.169473\n",
      "iteration 800 / 1500: loss 2.194099\n",
      "iteration 900 / 1500: loss 2.143143\n",
      "iteration 1000 / 1500: loss 2.130585\n",
      "iteration 1100 / 1500: loss 2.119935\n",
      "iteration 1200 / 1500: loss 2.169808\n",
      "iteration 1300 / 1500: loss 2.169783\n",
      "iteration 1400 / 1500: loss 2.132810\n",
      "iteration 0 / 1500: loss 1215.474552\n",
      "iteration 100 / 1500: loss 2.166023\n",
      "iteration 200 / 1500: loss 2.191583\n",
      "iteration 300 / 1500: loss 2.190116\n",
      "iteration 400 / 1500: loss 2.086488\n",
      "iteration 500 / 1500: loss 2.153957\n",
      "iteration 600 / 1500: loss 2.160109\n",
      "iteration 700 / 1500: loss 2.124457\n",
      "iteration 800 / 1500: loss 2.118886\n",
      "iteration 900 / 1500: loss 2.168968\n",
      "iteration 1000 / 1500: loss 2.126521\n",
      "iteration 1100 / 1500: loss 2.144696\n",
      "iteration 1200 / 1500: loss 2.126900\n",
      "iteration 1300 / 1500: loss 2.155870\n",
      "iteration 1400 / 1500: loss 2.134671\n",
      "iteration 0 / 1500: loss 1266.452543\n",
      "iteration 100 / 1500: loss 2.108938\n",
      "iteration 200 / 1500: loss 2.158519\n",
      "iteration 300 / 1500: loss 2.159617\n",
      "iteration 400 / 1500: loss 2.105937\n",
      "iteration 500 / 1500: loss 2.141760\n",
      "iteration 600 / 1500: loss 2.138861\n",
      "iteration 700 / 1500: loss 2.103726\n",
      "iteration 800 / 1500: loss 2.170281\n",
      "iteration 900 / 1500: loss 2.166231\n",
      "iteration 1000 / 1500: loss 2.078818\n",
      "iteration 1100 / 1500: loss 2.121855\n",
      "iteration 1200 / 1500: loss 2.155550\n",
      "iteration 1300 / 1500: loss 2.146544\n",
      "iteration 1400 / 1500: loss 2.116189\n",
      "iteration 0 / 1500: loss 1277.871947\n",
      "iteration 100 / 1500: loss 2.138862\n",
      "iteration 200 / 1500: loss 2.125833\n",
      "iteration 300 / 1500: loss 2.107705\n",
      "iteration 400 / 1500: loss 2.182304\n",
      "iteration 500 / 1500: loss 2.122523\n",
      "iteration 600 / 1500: loss 2.140815\n",
      "iteration 700 / 1500: loss 2.183543\n",
      "iteration 800 / 1500: loss 2.137665\n",
      "iteration 900 / 1500: loss 2.150062\n",
      "iteration 1000 / 1500: loss 2.174465\n",
      "iteration 1100 / 1500: loss 2.121723\n",
      "iteration 1200 / 1500: loss 2.167988\n",
      "iteration 1300 / 1500: loss 2.110189\n",
      "iteration 1400 / 1500: loss 2.151410\n",
      "iteration 0 / 1500: loss 1340.760330\n",
      "iteration 100 / 1500: loss 2.193795\n",
      "iteration 200 / 1500: loss 2.116535\n",
      "iteration 300 / 1500: loss 2.223952\n",
      "iteration 400 / 1500: loss 2.137072\n",
      "iteration 500 / 1500: loss 2.069171\n",
      "iteration 600 / 1500: loss 2.123075\n",
      "iteration 700 / 1500: loss 2.133864\n",
      "iteration 800 / 1500: loss 2.139861\n",
      "iteration 900 / 1500: loss 2.125343\n",
      "iteration 1000 / 1500: loss 2.140889\n",
      "iteration 1100 / 1500: loss 2.176722\n",
      "iteration 1200 / 1500: loss 2.148919\n",
      "iteration 1300 / 1500: loss 2.175566\n",
      "iteration 1400 / 1500: loss 2.165530\n",
      "iteration 0 / 1500: loss 1334.687149\n",
      "iteration 100 / 1500: loss 2.173794\n",
      "iteration 200 / 1500: loss 2.172782\n",
      "iteration 300 / 1500: loss 2.146253\n",
      "iteration 400 / 1500: loss 2.123576\n",
      "iteration 500 / 1500: loss 2.169160\n",
      "iteration 600 / 1500: loss 2.133346\n",
      "iteration 700 / 1500: loss 2.169370\n",
      "iteration 800 / 1500: loss 2.178468\n",
      "iteration 900 / 1500: loss 2.101926\n",
      "iteration 1000 / 1500: loss 2.178037\n",
      "iteration 1100 / 1500: loss 2.134655\n",
      "iteration 1200 / 1500: loss 2.162334\n",
      "iteration 1300 / 1500: loss 2.203784\n",
      "iteration 1400 / 1500: loss 2.164824\n",
      "iteration 0 / 1500: loss 1415.276589\n",
      "iteration 100 / 1500: loss 2.128896\n",
      "iteration 200 / 1500: loss 2.163777\n",
      "iteration 300 / 1500: loss 2.159473\n",
      "iteration 400 / 1500: loss 2.197047\n",
      "iteration 500 / 1500: loss 2.122017\n",
      "iteration 600 / 1500: loss 2.108910\n",
      "iteration 700 / 1500: loss 2.188933\n",
      "iteration 800 / 1500: loss 2.263653\n",
      "iteration 900 / 1500: loss 2.121297\n",
      "iteration 1000 / 1500: loss 2.210529\n",
      "iteration 1100 / 1500: loss 2.131163\n",
      "iteration 1200 / 1500: loss 2.195125\n",
      "iteration 1300 / 1500: loss 2.205855\n",
      "iteration 1400 / 1500: loss 2.174276\n",
      "iteration 0 / 1500: loss 1405.461207\n",
      "iteration 100 / 1500: loss 2.184288\n",
      "iteration 200 / 1500: loss 2.163834\n",
      "iteration 300 / 1500: loss 2.128322\n",
      "iteration 400 / 1500: loss 2.146661\n",
      "iteration 500 / 1500: loss 2.172874\n",
      "iteration 600 / 1500: loss 2.176194\n",
      "iteration 700 / 1500: loss 2.145822\n",
      "iteration 800 / 1500: loss 2.126087\n",
      "iteration 900 / 1500: loss 2.214259\n",
      "iteration 1000 / 1500: loss 2.123412\n",
      "iteration 1100 / 1500: loss 2.185281\n",
      "iteration 1200 / 1500: loss 2.147956\n",
      "iteration 1300 / 1500: loss 2.200120\n",
      "iteration 1400 / 1500: loss 2.247030\n",
      "iteration 0 / 1500: loss 1459.093096\n",
      "iteration 100 / 1500: loss 2.125632\n",
      "iteration 200 / 1500: loss 2.137325\n",
      "iteration 300 / 1500: loss 2.183811\n",
      "iteration 400 / 1500: loss 2.175360\n",
      "iteration 500 / 1500: loss 2.151580\n",
      "iteration 600 / 1500: loss 2.213606\n",
      "iteration 700 / 1500: loss 2.150680\n",
      "iteration 800 / 1500: loss 2.139107\n",
      "iteration 900 / 1500: loss 2.214096\n",
      "iteration 1000 / 1500: loss 2.179805\n",
      "iteration 1100 / 1500: loss 2.189978\n",
      "iteration 1200 / 1500: loss 2.153280\n",
      "iteration 1300 / 1500: loss 2.234870\n",
      "iteration 1400 / 1500: loss 2.242371\n",
      "iteration 0 / 1500: loss 1466.164127\n",
      "iteration 100 / 1500: loss 2.100551\n",
      "iteration 200 / 1500: loss 2.192205\n",
      "iteration 300 / 1500: loss 2.259374\n",
      "iteration 400 / 1500: loss 2.165718\n",
      "iteration 500 / 1500: loss 2.168980\n",
      "iteration 600 / 1500: loss 2.120617\n",
      "iteration 700 / 1500: loss 2.159841\n",
      "iteration 800 / 1500: loss 2.167033\n",
      "iteration 900 / 1500: loss 2.202809\n",
      "iteration 1000 / 1500: loss 2.181329\n",
      "iteration 1100 / 1500: loss 2.221619\n",
      "iteration 1200 / 1500: loss 2.126847\n",
      "iteration 1300 / 1500: loss 2.136036\n",
      "iteration 1400 / 1500: loss 2.107375\n",
      "iteration 0 / 1500: loss 1506.743188\n",
      "iteration 100 / 1500: loss 2.165054\n",
      "iteration 200 / 1500: loss 2.147543\n",
      "iteration 300 / 1500: loss 2.171465\n",
      "iteration 400 / 1500: loss 2.235584\n",
      "iteration 500 / 1500: loss 2.203083\n",
      "iteration 600 / 1500: loss 2.186955\n",
      "iteration 700 / 1500: loss 2.192880\n",
      "iteration 800 / 1500: loss 2.156781\n",
      "iteration 900 / 1500: loss 2.196050\n",
      "iteration 1000 / 1500: loss 2.213515\n",
      "iteration 1100 / 1500: loss 2.214595\n",
      "iteration 1200 / 1500: loss 2.205593\n",
      "iteration 1300 / 1500: loss 2.128175\n",
      "iteration 1400 / 1500: loss 2.172573\n",
      "iteration 0 / 1500: loss 1546.835179\n",
      "iteration 100 / 1500: loss 2.175117\n",
      "iteration 200 / 1500: loss 2.108982\n",
      "iteration 300 / 1500: loss 2.187170\n",
      "iteration 400 / 1500: loss 2.135539\n",
      "iteration 500 / 1500: loss 2.199570\n",
      "iteration 600 / 1500: loss 2.222054\n",
      "iteration 700 / 1500: loss 2.171367\n",
      "iteration 800 / 1500: loss 2.187243\n",
      "iteration 900 / 1500: loss 2.152118\n",
      "iteration 1000 / 1500: loss 2.155430\n",
      "iteration 1100 / 1500: loss 2.181824\n",
      "iteration 1200 / 1500: loss 2.180160\n",
      "iteration 1300 / 1500: loss 2.157289\n",
      "iteration 1400 / 1500: loss 2.144036\n",
      "iteration 0 / 1500: loss 776.671545\n",
      "iteration 100 / 1500: loss 2.106467\n",
      "iteration 200 / 1500: loss 2.081129\n",
      "iteration 300 / 1500: loss 2.163850\n",
      "iteration 400 / 1500: loss 2.112873\n",
      "iteration 500 / 1500: loss 2.137097\n",
      "iteration 600 / 1500: loss 2.079215\n",
      "iteration 700 / 1500: loss 2.122582\n",
      "iteration 800 / 1500: loss 2.056958\n",
      "iteration 900 / 1500: loss 2.139856\n",
      "iteration 1000 / 1500: loss 2.141402\n",
      "iteration 1100 / 1500: loss 2.124459\n",
      "iteration 1200 / 1500: loss 2.160839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300 / 1500: loss 2.164867\n",
      "iteration 1400 / 1500: loss 2.167014\n",
      "iteration 0 / 1500: loss 797.519393\n",
      "iteration 100 / 1500: loss 2.126964\n",
      "iteration 200 / 1500: loss 2.180810\n",
      "iteration 300 / 1500: loss 2.203995\n",
      "iteration 400 / 1500: loss 2.242200\n",
      "iteration 500 / 1500: loss 2.112521\n",
      "iteration 600 / 1500: loss 2.148484\n",
      "iteration 700 / 1500: loss 2.136032\n",
      "iteration 800 / 1500: loss 2.150134\n",
      "iteration 900 / 1500: loss 2.066378\n",
      "iteration 1000 / 1500: loss 2.133008\n",
      "iteration 1100 / 1500: loss 2.113470\n",
      "iteration 1200 / 1500: loss 2.191812\n",
      "iteration 1300 / 1500: loss 2.139467\n",
      "iteration 1400 / 1500: loss 2.100416\n",
      "iteration 0 / 1500: loss 836.351597\n",
      "iteration 100 / 1500: loss 2.119550\n",
      "iteration 200 / 1500: loss 2.093696\n",
      "iteration 300 / 1500: loss 2.116319\n",
      "iteration 400 / 1500: loss 2.113499\n",
      "iteration 500 / 1500: loss 2.145031\n",
      "iteration 600 / 1500: loss 2.134449\n",
      "iteration 700 / 1500: loss 2.108643\n",
      "iteration 800 / 1500: loss 2.140744\n",
      "iteration 900 / 1500: loss 2.118519\n",
      "iteration 1000 / 1500: loss 2.131990\n",
      "iteration 1100 / 1500: loss 2.164400\n",
      "iteration 1200 / 1500: loss 2.045306\n",
      "iteration 1300 / 1500: loss 2.147079\n",
      "iteration 1400 / 1500: loss 2.114907\n",
      "iteration 0 / 1500: loss 864.938183\n",
      "iteration 100 / 1500: loss 2.092231\n",
      "iteration 200 / 1500: loss 2.087904\n",
      "iteration 300 / 1500: loss 2.088672\n",
      "iteration 400 / 1500: loss 2.088740\n",
      "iteration 500 / 1500: loss 2.065056\n",
      "iteration 600 / 1500: loss 2.127620\n",
      "iteration 700 / 1500: loss 2.143906\n",
      "iteration 800 / 1500: loss 2.067198\n",
      "iteration 900 / 1500: loss 2.149886\n",
      "iteration 1000 / 1500: loss 2.207508\n",
      "iteration 1100 / 1500: loss 2.170076\n",
      "iteration 1200 / 1500: loss 2.118628\n",
      "iteration 1300 / 1500: loss 2.181974\n",
      "iteration 1400 / 1500: loss 2.107664\n",
      "iteration 0 / 1500: loss 896.579955\n",
      "iteration 100 / 1500: loss 2.136186\n",
      "iteration 200 / 1500: loss 2.088581\n",
      "iteration 300 / 1500: loss 2.092265\n",
      "iteration 400 / 1500: loss 2.178816\n",
      "iteration 500 / 1500: loss 2.086368\n",
      "iteration 600 / 1500: loss 2.084974\n",
      "iteration 700 / 1500: loss 2.101002\n",
      "iteration 800 / 1500: loss 2.125968\n",
      "iteration 900 / 1500: loss 2.135267\n",
      "iteration 1000 / 1500: loss 2.136162\n",
      "iteration 1100 / 1500: loss 2.156328\n",
      "iteration 1200 / 1500: loss 2.157851\n",
      "iteration 1300 / 1500: loss 2.099452\n",
      "iteration 1400 / 1500: loss 2.180893\n",
      "iteration 0 / 1500: loss 926.756524\n",
      "iteration 100 / 1500: loss 2.152117\n",
      "iteration 200 / 1500: loss 2.130613\n",
      "iteration 300 / 1500: loss 2.149497\n",
      "iteration 400 / 1500: loss 2.168425\n",
      "iteration 500 / 1500: loss 2.126342\n",
      "iteration 600 / 1500: loss 2.168022\n",
      "iteration 700 / 1500: loss 2.220977\n",
      "iteration 800 / 1500: loss 2.138028\n",
      "iteration 900 / 1500: loss 2.109834\n",
      "iteration 1000 / 1500: loss 2.138304\n",
      "iteration 1100 / 1500: loss 2.179188\n",
      "iteration 1200 / 1500: loss 2.186500\n",
      "iteration 1300 / 1500: loss 2.115850\n",
      "iteration 1400 / 1500: loss 2.117649\n",
      "iteration 0 / 1500: loss 953.558793\n",
      "iteration 100 / 1500: loss 2.164536\n",
      "iteration 200 / 1500: loss 2.107632\n",
      "iteration 300 / 1500: loss 2.148681\n",
      "iteration 400 / 1500: loss 2.118496\n",
      "iteration 500 / 1500: loss 2.195253\n",
      "iteration 600 / 1500: loss 2.202450\n",
      "iteration 700 / 1500: loss 2.118239\n",
      "iteration 800 / 1500: loss 2.114324\n",
      "iteration 900 / 1500: loss 2.135000\n",
      "iteration 1000 / 1500: loss 2.095384\n",
      "iteration 1100 / 1500: loss 2.236840\n",
      "iteration 1200 / 1500: loss 2.135398\n",
      "iteration 1300 / 1500: loss 2.088045\n",
      "iteration 1400 / 1500: loss 2.141275\n",
      "iteration 0 / 1500: loss 988.289644\n",
      "iteration 100 / 1500: loss 2.099361\n",
      "iteration 200 / 1500: loss 2.085437\n",
      "iteration 300 / 1500: loss 2.115827\n",
      "iteration 400 / 1500: loss 2.144571\n",
      "iteration 500 / 1500: loss 2.225852\n",
      "iteration 600 / 1500: loss 2.190825\n",
      "iteration 700 / 1500: loss 2.218113\n",
      "iteration 800 / 1500: loss 2.107297\n",
      "iteration 900 / 1500: loss 2.198878\n",
      "iteration 1000 / 1500: loss 2.139671\n",
      "iteration 1100 / 1500: loss 2.198107\n",
      "iteration 1200 / 1500: loss 2.158738\n",
      "iteration 1300 / 1500: loss 2.099186\n",
      "iteration 1400 / 1500: loss 2.196569\n",
      "iteration 0 / 1500: loss 1013.164972\n",
      "iteration 100 / 1500: loss 2.185131\n",
      "iteration 200 / 1500: loss 2.110951\n",
      "iteration 300 / 1500: loss 2.117789\n",
      "iteration 400 / 1500: loss 2.085018\n",
      "iteration 500 / 1500: loss 2.191706\n",
      "iteration 600 / 1500: loss 2.123434\n",
      "iteration 700 / 1500: loss 2.203505\n",
      "iteration 800 / 1500: loss 2.162501\n",
      "iteration 900 / 1500: loss 2.129067\n",
      "iteration 1000 / 1500: loss 2.180322\n",
      "iteration 1100 / 1500: loss 2.140427\n",
      "iteration 1200 / 1500: loss 2.205700\n",
      "iteration 1300 / 1500: loss 2.067876\n",
      "iteration 1400 / 1500: loss 2.112431\n",
      "iteration 0 / 1500: loss 1054.701976\n",
      "iteration 100 / 1500: loss 2.215915\n",
      "iteration 200 / 1500: loss 2.129968\n",
      "iteration 300 / 1500: loss 2.100215\n",
      "iteration 400 / 1500: loss 2.230553\n",
      "iteration 500 / 1500: loss 2.266448\n",
      "iteration 600 / 1500: loss 2.161093\n",
      "iteration 700 / 1500: loss 2.136332\n",
      "iteration 800 / 1500: loss 2.082075\n",
      "iteration 900 / 1500: loss 2.227866\n",
      "iteration 1000 / 1500: loss 2.162397\n",
      "iteration 1100 / 1500: loss 2.123853\n",
      "iteration 1200 / 1500: loss 2.245069\n",
      "iteration 1300 / 1500: loss 2.124839\n",
      "iteration 1400 / 1500: loss 2.159043\n",
      "iteration 0 / 1500: loss 1065.875748\n",
      "iteration 100 / 1500: loss 2.171290\n",
      "iteration 200 / 1500: loss 2.151517\n",
      "iteration 300 / 1500: loss 2.144391\n",
      "iteration 400 / 1500: loss 2.178295\n",
      "iteration 500 / 1500: loss 2.109744\n",
      "iteration 600 / 1500: loss 2.226540\n",
      "iteration 700 / 1500: loss 2.162147\n",
      "iteration 800 / 1500: loss 2.165581\n",
      "iteration 900 / 1500: loss 2.177054\n",
      "iteration 1000 / 1500: loss 2.148027\n",
      "iteration 1100 / 1500: loss 2.163268\n",
      "iteration 1200 / 1500: loss 2.161040\n",
      "iteration 1300 / 1500: loss 2.133480\n",
      "iteration 1400 / 1500: loss 2.149503\n",
      "iteration 0 / 1500: loss 1118.389401\n",
      "iteration 100 / 1500: loss 2.157190\n",
      "iteration 200 / 1500: loss 2.180127\n",
      "iteration 300 / 1500: loss 2.092491\n",
      "iteration 400 / 1500: loss 2.086906\n",
      "iteration 500 / 1500: loss 2.160751\n",
      "iteration 600 / 1500: loss 2.141408\n",
      "iteration 700 / 1500: loss 2.214723\n",
      "iteration 800 / 1500: loss 2.105010\n",
      "iteration 900 / 1500: loss 2.114687\n",
      "iteration 1000 / 1500: loss 2.063843\n",
      "iteration 1100 / 1500: loss 2.175893\n",
      "iteration 1200 / 1500: loss 2.201283\n",
      "iteration 1300 / 1500: loss 2.166048\n",
      "iteration 1400 / 1500: loss 2.144702\n",
      "iteration 0 / 1500: loss 1118.512293\n",
      "iteration 100 / 1500: loss 2.151023\n",
      "iteration 200 / 1500: loss 2.132393\n",
      "iteration 300 / 1500: loss 2.071915\n",
      "iteration 400 / 1500: loss 2.197036\n",
      "iteration 500 / 1500: loss 2.205199\n",
      "iteration 600 / 1500: loss 2.136020\n",
      "iteration 700 / 1500: loss 2.082077\n",
      "iteration 800 / 1500: loss 2.180475\n",
      "iteration 900 / 1500: loss 2.136377\n",
      "iteration 1000 / 1500: loss 2.158625\n",
      "iteration 1100 / 1500: loss 2.172862\n",
      "iteration 1200 / 1500: loss 2.217181\n",
      "iteration 1300 / 1500: loss 2.170098\n",
      "iteration 1400 / 1500: loss 2.173189\n",
      "iteration 0 / 1500: loss 1174.198797\n",
      "iteration 100 / 1500: loss 2.152863\n",
      "iteration 200 / 1500: loss 2.124599\n",
      "iteration 300 / 1500: loss 2.126951\n",
      "iteration 400 / 1500: loss 2.116663\n",
      "iteration 500 / 1500: loss 2.169542\n",
      "iteration 600 / 1500: loss 2.072470\n",
      "iteration 700 / 1500: loss 2.154202\n",
      "iteration 800 / 1500: loss 2.148357\n",
      "iteration 900 / 1500: loss 2.162261\n",
      "iteration 1000 / 1500: loss 2.104170\n",
      "iteration 1100 / 1500: loss 2.138032\n",
      "iteration 1200 / 1500: loss 2.223177\n",
      "iteration 1300 / 1500: loss 2.170826\n",
      "iteration 1400 / 1500: loss 2.036571\n",
      "iteration 0 / 1500: loss 1206.486272\n",
      "iteration 100 / 1500: loss 2.105555\n",
      "iteration 200 / 1500: loss 2.191710\n",
      "iteration 300 / 1500: loss 2.136127\n",
      "iteration 400 / 1500: loss 2.173973\n",
      "iteration 500 / 1500: loss 2.162380\n",
      "iteration 600 / 1500: loss 2.166180\n",
      "iteration 700 / 1500: loss 2.093165\n",
      "iteration 800 / 1500: loss 2.194349\n",
      "iteration 900 / 1500: loss 2.205634\n",
      "iteration 1000 / 1500: loss 2.165002\n",
      "iteration 1100 / 1500: loss 2.140193\n",
      "iteration 1200 / 1500: loss 2.224855\n",
      "iteration 1300 / 1500: loss 2.209846\n",
      "iteration 1400 / 1500: loss 2.148595\n",
      "iteration 0 / 1500: loss 1254.421139\n",
      "iteration 100 / 1500: loss 2.156122\n",
      "iteration 200 / 1500: loss 2.153662\n",
      "iteration 300 / 1500: loss 2.222394\n",
      "iteration 400 / 1500: loss 2.190191\n",
      "iteration 500 / 1500: loss 2.212340\n",
      "iteration 600 / 1500: loss 2.189560\n",
      "iteration 700 / 1500: loss 2.221788\n",
      "iteration 800 / 1500: loss 2.224466\n",
      "iteration 900 / 1500: loss 2.141483\n",
      "iteration 1000 / 1500: loss 2.201909\n",
      "iteration 1100 / 1500: loss 2.146223\n",
      "iteration 1200 / 1500: loss 2.182532\n",
      "iteration 1300 / 1500: loss 2.073976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 1500: loss 2.190594\n",
      "iteration 0 / 1500: loss 1251.696303\n",
      "iteration 100 / 1500: loss 2.178671\n",
      "iteration 200 / 1500: loss 2.236766\n",
      "iteration 300 / 1500: loss 2.171338\n",
      "iteration 400 / 1500: loss 2.274525\n",
      "iteration 500 / 1500: loss 2.197699\n",
      "iteration 600 / 1500: loss 2.198666\n",
      "iteration 700 / 1500: loss 2.184570\n",
      "iteration 800 / 1500: loss 2.178428\n",
      "iteration 900 / 1500: loss 2.214847\n",
      "iteration 1000 / 1500: loss 2.175837\n",
      "iteration 1100 / 1500: loss 2.180507\n",
      "iteration 1200 / 1500: loss 2.157935\n",
      "iteration 1300 / 1500: loss 2.150952\n",
      "iteration 1400 / 1500: loss 2.115979\n",
      "iteration 0 / 1500: loss 1305.716024\n",
      "iteration 100 / 1500: loss 2.176550\n",
      "iteration 200 / 1500: loss 2.126187\n",
      "iteration 300 / 1500: loss 2.107588\n",
      "iteration 400 / 1500: loss 2.139171\n",
      "iteration 500 / 1500: loss 2.111727\n",
      "iteration 600 / 1500: loss 2.218452\n",
      "iteration 700 / 1500: loss 2.174531\n",
      "iteration 800 / 1500: loss 2.202490\n",
      "iteration 900 / 1500: loss 2.137332\n",
      "iteration 1000 / 1500: loss 2.138710\n",
      "iteration 1100 / 1500: loss 2.135806\n",
      "iteration 1200 / 1500: loss 2.209120\n",
      "iteration 1300 / 1500: loss 2.189479\n",
      "iteration 1400 / 1500: loss 2.134867\n",
      "iteration 0 / 1500: loss 1330.611813\n",
      "iteration 100 / 1500: loss 2.229684\n",
      "iteration 200 / 1500: loss 2.169033\n",
      "iteration 300 / 1500: loss 2.160995\n",
      "iteration 400 / 1500: loss 2.147635\n",
      "iteration 500 / 1500: loss 2.218374\n",
      "iteration 600 / 1500: loss 2.176347\n",
      "iteration 700 / 1500: loss 2.193299\n",
      "iteration 800 / 1500: loss 2.155077\n",
      "iteration 900 / 1500: loss 2.151987\n",
      "iteration 1000 / 1500: loss 2.158867\n",
      "iteration 1100 / 1500: loss 2.150355\n",
      "iteration 1200 / 1500: loss 2.141618\n",
      "iteration 1300 / 1500: loss 2.281002\n",
      "iteration 1400 / 1500: loss 2.187100\n",
      "iteration 0 / 1500: loss 1355.741095\n",
      "iteration 100 / 1500: loss 2.170992\n",
      "iteration 200 / 1500: loss 2.157980\n",
      "iteration 300 / 1500: loss 2.213873\n",
      "iteration 400 / 1500: loss 2.142834\n",
      "iteration 500 / 1500: loss 2.228605\n",
      "iteration 600 / 1500: loss 2.124582\n",
      "iteration 700 / 1500: loss 2.199326\n",
      "iteration 800 / 1500: loss 2.240475\n",
      "iteration 900 / 1500: loss 2.182799\n",
      "iteration 1000 / 1500: loss 2.165114\n",
      "iteration 1100 / 1500: loss 2.229960\n",
      "iteration 1200 / 1500: loss 2.161662\n",
      "iteration 1300 / 1500: loss 2.214899\n",
      "iteration 1400 / 1500: loss 2.310362\n",
      "iteration 0 / 1500: loss 1394.432801\n",
      "iteration 100 / 1500: loss 2.164060\n",
      "iteration 200 / 1500: loss 2.191308\n",
      "iteration 300 / 1500: loss 2.233148\n",
      "iteration 400 / 1500: loss 2.168440\n",
      "iteration 500 / 1500: loss 2.157900\n",
      "iteration 600 / 1500: loss 2.093391\n",
      "iteration 700 / 1500: loss 2.157812\n",
      "iteration 800 / 1500: loss 2.148688\n",
      "iteration 900 / 1500: loss 2.212705\n",
      "iteration 1000 / 1500: loss 2.242321\n",
      "iteration 1100 / 1500: loss 2.108087\n",
      "iteration 1200 / 1500: loss 2.196290\n",
      "iteration 1300 / 1500: loss 2.116592\n",
      "iteration 1400 / 1500: loss 2.151755\n",
      "iteration 0 / 1500: loss 1414.120463\n",
      "iteration 100 / 1500: loss 2.242247\n",
      "iteration 200 / 1500: loss 2.308694\n",
      "iteration 300 / 1500: loss 2.163851\n",
      "iteration 400 / 1500: loss 2.170299\n",
      "iteration 500 / 1500: loss 2.182503\n",
      "iteration 600 / 1500: loss 2.219799\n",
      "iteration 700 / 1500: loss 2.279862\n",
      "iteration 800 / 1500: loss 2.188543\n",
      "iteration 900 / 1500: loss 2.181247\n",
      "iteration 1000 / 1500: loss 2.148921\n",
      "iteration 1100 / 1500: loss 2.180568\n",
      "iteration 1200 / 1500: loss 2.126709\n",
      "iteration 1300 / 1500: loss 2.160719\n",
      "iteration 1400 / 1500: loss 2.117137\n",
      "iteration 0 / 1500: loss 1445.916901\n",
      "iteration 100 / 1500: loss 2.215343\n",
      "iteration 200 / 1500: loss 2.237131\n",
      "iteration 300 / 1500: loss 2.257842\n",
      "iteration 400 / 1500: loss 2.129889\n",
      "iteration 500 / 1500: loss 2.180505\n",
      "iteration 600 / 1500: loss 2.151430\n",
      "iteration 700 / 1500: loss 2.199156\n",
      "iteration 800 / 1500: loss 2.182910\n",
      "iteration 900 / 1500: loss 2.131191\n",
      "iteration 1000 / 1500: loss 2.190876\n",
      "iteration 1100 / 1500: loss 2.106213\n",
      "iteration 1200 / 1500: loss 2.148506\n",
      "iteration 1300 / 1500: loss 2.148537\n",
      "iteration 1400 / 1500: loss 2.195311\n",
      "iteration 0 / 1500: loss 1468.855911\n",
      "iteration 100 / 1500: loss 2.100009\n",
      "iteration 200 / 1500: loss 2.292558\n",
      "iteration 300 / 1500: loss 2.242250\n",
      "iteration 400 / 1500: loss 2.204202\n",
      "iteration 500 / 1500: loss 2.195740\n",
      "iteration 600 / 1500: loss 2.166893\n",
      "iteration 700 / 1500: loss 2.283643\n",
      "iteration 800 / 1500: loss 2.094158\n",
      "iteration 900 / 1500: loss 2.170715\n",
      "iteration 1000 / 1500: loss 2.280453\n",
      "iteration 1100 / 1500: loss 2.148425\n",
      "iteration 1200 / 1500: loss 2.192561\n",
      "iteration 1300 / 1500: loss 2.134979\n",
      "iteration 1400 / 1500: loss 2.195684\n",
      "iteration 0 / 1500: loss 1501.173490\n",
      "iteration 100 / 1500: loss 2.234390\n",
      "iteration 200 / 1500: loss 2.159490\n",
      "iteration 300 / 1500: loss 2.129053\n",
      "iteration 400 / 1500: loss 2.192755\n",
      "iteration 500 / 1500: loss 2.191074\n",
      "iteration 600 / 1500: loss 2.159885\n",
      "iteration 700 / 1500: loss 2.164918\n",
      "iteration 800 / 1500: loss 2.185475\n",
      "iteration 900 / 1500: loss 2.187498\n",
      "iteration 1000 / 1500: loss 2.170440\n",
      "iteration 1100 / 1500: loss 2.181409\n",
      "iteration 1200 / 1500: loss 2.208649\n",
      "iteration 1300 / 1500: loss 2.154567\n",
      "iteration 1400 / 1500: loss 2.239233\n",
      "iteration 0 / 1500: loss 1521.751728\n",
      "iteration 100 / 1500: loss 2.187215\n",
      "iteration 200 / 1500: loss 2.159478\n",
      "iteration 300 / 1500: loss 2.190141\n",
      "iteration 400 / 1500: loss 2.252270\n",
      "iteration 500 / 1500: loss 2.161386\n",
      "iteration 600 / 1500: loss 2.237656\n",
      "iteration 700 / 1500: loss 2.175204\n",
      "iteration 800 / 1500: loss 2.180191\n",
      "iteration 900 / 1500: loss 2.174048\n",
      "iteration 1000 / 1500: loss 2.284836\n",
      "iteration 1100 / 1500: loss 2.163296\n",
      "iteration 1200 / 1500: loss 2.191733\n",
      "iteration 1300 / 1500: loss 2.180274\n",
      "iteration 1400 / 1500: loss 2.212111\n",
      "iteration 0 / 1500: loss 778.858101\n",
      "iteration 100 / 1500: loss 2.163575\n",
      "iteration 200 / 1500: loss 2.149638\n",
      "iteration 300 / 1500: loss 2.121254\n",
      "iteration 400 / 1500: loss 2.313251\n",
      "iteration 500 / 1500: loss 2.174179\n",
      "iteration 600 / 1500: loss 2.061001\n",
      "iteration 700 / 1500: loss 2.182639\n",
      "iteration 800 / 1500: loss 2.188228\n",
      "iteration 900 / 1500: loss 2.241364\n",
      "iteration 1000 / 1500: loss 2.121824\n",
      "iteration 1100 / 1500: loss 2.197879\n",
      "iteration 1200 / 1500: loss 2.218168\n",
      "iteration 1300 / 1500: loss 2.232088\n",
      "iteration 1400 / 1500: loss 2.208816\n",
      "iteration 0 / 1500: loss 807.945371\n",
      "iteration 100 / 1500: loss 2.202373\n",
      "iteration 200 / 1500: loss 2.187732\n",
      "iteration 300 / 1500: loss 2.108831\n",
      "iteration 400 / 1500: loss 2.138500\n",
      "iteration 500 / 1500: loss 2.176633\n",
      "iteration 600 / 1500: loss 2.076798\n",
      "iteration 700 / 1500: loss 2.192870\n",
      "iteration 800 / 1500: loss 2.223537\n",
      "iteration 900 / 1500: loss 2.229124\n",
      "iteration 1000 / 1500: loss 2.121498\n",
      "iteration 1100 / 1500: loss 2.264384\n",
      "iteration 1200 / 1500: loss 2.133496\n",
      "iteration 1300 / 1500: loss 2.114612\n",
      "iteration 1400 / 1500: loss 2.079341\n",
      "iteration 0 / 1500: loss 844.962677\n",
      "iteration 100 / 1500: loss 2.076280\n",
      "iteration 200 / 1500: loss 2.148703\n",
      "iteration 300 / 1500: loss 2.193444\n",
      "iteration 400 / 1500: loss 2.085282\n",
      "iteration 500 / 1500: loss 2.078787\n",
      "iteration 600 / 1500: loss 2.117361\n",
      "iteration 700 / 1500: loss 2.186132\n",
      "iteration 800 / 1500: loss 2.230153\n",
      "iteration 900 / 1500: loss 2.069554\n",
      "iteration 1000 / 1500: loss 2.047823\n",
      "iteration 1100 / 1500: loss 2.171418\n",
      "iteration 1200 / 1500: loss 2.153875\n",
      "iteration 1300 / 1500: loss 2.150067\n",
      "iteration 1400 / 1500: loss 2.199704\n",
      "iteration 0 / 1500: loss 862.835788\n",
      "iteration 100 / 1500: loss 2.140479\n",
      "iteration 200 / 1500: loss 2.208296\n",
      "iteration 300 / 1500: loss 2.086646\n",
      "iteration 400 / 1500: loss 2.183104\n",
      "iteration 500 / 1500: loss 2.141661\n",
      "iteration 600 / 1500: loss 2.181425\n",
      "iteration 700 / 1500: loss 2.215069\n",
      "iteration 800 / 1500: loss 2.210920\n",
      "iteration 900 / 1500: loss 2.268428\n",
      "iteration 1000 / 1500: loss 2.245784\n",
      "iteration 1100 / 1500: loss 2.164536\n",
      "iteration 1200 / 1500: loss 2.145623\n",
      "iteration 1300 / 1500: loss 2.197993\n",
      "iteration 1400 / 1500: loss 2.134093\n",
      "iteration 0 / 1500: loss 891.328108\n",
      "iteration 100 / 1500: loss 2.086137\n",
      "iteration 200 / 1500: loss 2.165971\n",
      "iteration 300 / 1500: loss 2.177156\n",
      "iteration 400 / 1500: loss 2.166111\n",
      "iteration 500 / 1500: loss 2.178195\n",
      "iteration 600 / 1500: loss 2.255845\n",
      "iteration 700 / 1500: loss 2.112774\n",
      "iteration 800 / 1500: loss 2.174413\n",
      "iteration 900 / 1500: loss 2.277161\n",
      "iteration 1000 / 1500: loss 2.189548\n",
      "iteration 1100 / 1500: loss 2.202531\n",
      "iteration 1200 / 1500: loss 2.120729\n",
      "iteration 1300 / 1500: loss 2.121102\n",
      "iteration 1400 / 1500: loss 2.100687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 1500: loss 931.861024\n",
      "iteration 100 / 1500: loss 2.267174\n",
      "iteration 200 / 1500: loss 2.229631\n",
      "iteration 300 / 1500: loss 2.194152\n",
      "iteration 400 / 1500: loss 2.202582\n",
      "iteration 500 / 1500: loss 2.254668\n",
      "iteration 600 / 1500: loss 2.200458\n",
      "iteration 700 / 1500: loss 2.083297\n",
      "iteration 800 / 1500: loss 2.146153\n",
      "iteration 900 / 1500: loss 2.246307\n",
      "iteration 1000 / 1500: loss 2.124079\n",
      "iteration 1100 / 1500: loss 2.195254\n",
      "iteration 1200 / 1500: loss 2.145868\n",
      "iteration 1300 / 1500: loss 2.216405\n",
      "iteration 1400 / 1500: loss 2.269842\n",
      "iteration 0 / 1500: loss 978.725326\n",
      "iteration 100 / 1500: loss 2.167467\n",
      "iteration 200 / 1500: loss 2.153701\n",
      "iteration 300 / 1500: loss 2.157902\n",
      "iteration 400 / 1500: loss 2.180989\n",
      "iteration 500 / 1500: loss 2.154651\n",
      "iteration 600 / 1500: loss 2.227866\n",
      "iteration 700 / 1500: loss 2.197251\n",
      "iteration 800 / 1500: loss 2.193031\n",
      "iteration 900 / 1500: loss 2.231830\n",
      "iteration 1000 / 1500: loss 2.168955\n",
      "iteration 1100 / 1500: loss 2.136755\n",
      "iteration 1200 / 1500: loss 2.220819\n",
      "iteration 1300 / 1500: loss 2.163234\n",
      "iteration 1400 / 1500: loss 2.235609\n",
      "iteration 0 / 1500: loss 981.559434\n",
      "iteration 100 / 1500: loss 2.353128\n",
      "iteration 200 / 1500: loss 2.218022\n",
      "iteration 300 / 1500: loss 2.190871\n",
      "iteration 400 / 1500: loss 2.246537\n",
      "iteration 500 / 1500: loss 2.248278\n",
      "iteration 600 / 1500: loss 2.277211\n",
      "iteration 700 / 1500: loss 2.223667\n",
      "iteration 800 / 1500: loss 2.221294\n",
      "iteration 900 / 1500: loss 2.203996\n",
      "iteration 1000 / 1500: loss 2.177556\n",
      "iteration 1100 / 1500: loss 2.339609\n",
      "iteration 1200 / 1500: loss 2.193519\n",
      "iteration 1300 / 1500: loss 2.286943\n",
      "iteration 1400 / 1500: loss 2.194171\n",
      "iteration 0 / 1500: loss 1021.981514\n",
      "iteration 100 / 1500: loss 2.180151\n",
      "iteration 200 / 1500: loss 2.169438\n",
      "iteration 300 / 1500: loss 2.184963\n",
      "iteration 400 / 1500: loss 2.260926\n",
      "iteration 500 / 1500: loss 2.104390\n",
      "iteration 600 / 1500: loss 2.232414\n",
      "iteration 700 / 1500: loss 2.142482\n",
      "iteration 800 / 1500: loss 2.115176\n",
      "iteration 900 / 1500: loss 2.230445\n",
      "iteration 1000 / 1500: loss 2.347074\n",
      "iteration 1100 / 1500: loss 2.287251\n",
      "iteration 1200 / 1500: loss 2.204028\n",
      "iteration 1300 / 1500: loss 2.200749\n",
      "iteration 1400 / 1500: loss 2.194682\n",
      "iteration 0 / 1500: loss 1058.168013\n",
      "iteration 100 / 1500: loss 2.292235\n",
      "iteration 200 / 1500: loss 2.158899\n",
      "iteration 300 / 1500: loss 2.189493\n",
      "iteration 400 / 1500: loss 2.179689\n",
      "iteration 500 / 1500: loss 2.193013\n",
      "iteration 600 / 1500: loss 2.254985\n",
      "iteration 700 / 1500: loss 2.150427\n",
      "iteration 800 / 1500: loss 2.247866\n",
      "iteration 900 / 1500: loss 2.295537\n",
      "iteration 1000 / 1500: loss 2.308204\n",
      "iteration 1100 / 1500: loss 2.084595\n",
      "iteration 1200 / 1500: loss 2.165940\n",
      "iteration 1300 / 1500: loss 2.289125\n",
      "iteration 1400 / 1500: loss 2.197036\n",
      "iteration 0 / 1500: loss 1088.936253\n",
      "iteration 100 / 1500: loss 2.338171\n",
      "iteration 200 / 1500: loss 2.240324\n",
      "iteration 300 / 1500: loss 2.338904\n",
      "iteration 400 / 1500: loss 2.203438\n",
      "iteration 500 / 1500: loss 2.221251\n",
      "iteration 600 / 1500: loss 2.196929\n",
      "iteration 700 / 1500: loss 2.262164\n",
      "iteration 800 / 1500: loss 2.265734\n",
      "iteration 900 / 1500: loss 2.258146\n",
      "iteration 1000 / 1500: loss 2.239400\n",
      "iteration 1100 / 1500: loss 2.221002\n",
      "iteration 1200 / 1500: loss 2.281381\n",
      "iteration 1300 / 1500: loss 2.173013\n",
      "iteration 1400 / 1500: loss 2.108659\n",
      "iteration 0 / 1500: loss 1107.129758\n",
      "iteration 100 / 1500: loss 2.463591\n",
      "iteration 200 / 1500: loss 2.150750\n",
      "iteration 300 / 1500: loss 2.249589\n",
      "iteration 400 / 1500: loss 2.192859\n",
      "iteration 500 / 1500: loss 2.370485\n",
      "iteration 600 / 1500: loss 2.477042\n",
      "iteration 700 / 1500: loss 2.197240\n",
      "iteration 800 / 1500: loss 2.267010\n",
      "iteration 900 / 1500: loss 2.241541\n",
      "iteration 1000 / 1500: loss 2.183689\n",
      "iteration 1100 / 1500: loss 2.308510\n",
      "iteration 1200 / 1500: loss 2.243223\n",
      "iteration 1300 / 1500: loss 2.135883\n",
      "iteration 1400 / 1500: loss 2.261478\n",
      "iteration 0 / 1500: loss 1143.591523\n",
      "iteration 100 / 1500: loss 2.239177\n",
      "iteration 200 / 1500: loss 2.190521\n",
      "iteration 300 / 1500: loss 2.285124\n",
      "iteration 400 / 1500: loss 2.126150\n",
      "iteration 500 / 1500: loss 2.309487\n",
      "iteration 600 / 1500: loss 2.161188\n",
      "iteration 700 / 1500: loss 2.282455\n",
      "iteration 800 / 1500: loss 2.124269\n",
      "iteration 900 / 1500: loss 2.187472\n",
      "iteration 1000 / 1500: loss 2.227512\n",
      "iteration 1100 / 1500: loss 2.493133\n",
      "iteration 1200 / 1500: loss 2.225015\n",
      "iteration 1300 / 1500: loss 2.184408\n",
      "iteration 1400 / 1500: loss 2.235108\n",
      "iteration 0 / 1500: loss 1177.220136\n",
      "iteration 100 / 1500: loss 2.254033\n",
      "iteration 200 / 1500: loss 2.160821\n",
      "iteration 300 / 1500: loss 2.256709\n",
      "iteration 400 / 1500: loss 2.218482\n",
      "iteration 500 / 1500: loss 2.315438\n",
      "iteration 600 / 1500: loss 2.235926\n",
      "iteration 700 / 1500: loss 2.255785\n",
      "iteration 800 / 1500: loss 2.219077\n",
      "iteration 900 / 1500: loss 2.239048\n",
      "iteration 1000 / 1500: loss 2.224608\n",
      "iteration 1100 / 1500: loss 2.100712\n",
      "iteration 1200 / 1500: loss 2.354035\n",
      "iteration 1300 / 1500: loss 2.208555\n",
      "iteration 1400 / 1500: loss 2.295106\n",
      "iteration 0 / 1500: loss 1200.330244\n",
      "iteration 100 / 1500: loss 2.202738\n",
      "iteration 200 / 1500: loss 2.221141\n",
      "iteration 300 / 1500: loss 2.247152\n",
      "iteration 400 / 1500: loss 2.222250\n",
      "iteration 500 / 1500: loss 2.136568\n",
      "iteration 600 / 1500: loss 2.203413\n",
      "iteration 700 / 1500: loss 2.141533\n",
      "iteration 800 / 1500: loss 2.313641\n",
      "iteration 900 / 1500: loss 2.179200\n",
      "iteration 1000 / 1500: loss 2.178779\n",
      "iteration 1100 / 1500: loss 2.325254\n",
      "iteration 1200 / 1500: loss 2.247515\n",
      "iteration 1300 / 1500: loss 2.230984\n",
      "iteration 1400 / 1500: loss 2.321359\n",
      "iteration 0 / 1500: loss 1223.438381\n",
      "iteration 100 / 1500: loss 2.250323\n",
      "iteration 200 / 1500: loss 2.198162\n",
      "iteration 300 / 1500: loss 2.225293\n",
      "iteration 400 / 1500: loss 2.163757\n",
      "iteration 500 / 1500: loss 2.147365\n",
      "iteration 600 / 1500: loss 2.324277\n",
      "iteration 700 / 1500: loss 2.260092\n",
      "iteration 800 / 1500: loss 2.508278\n",
      "iteration 900 / 1500: loss 2.207033\n",
      "iteration 1000 / 1500: loss 2.297480\n",
      "iteration 1100 / 1500: loss 2.196145\n",
      "iteration 1200 / 1500: loss 2.391833\n",
      "iteration 1300 / 1500: loss 2.384541\n",
      "iteration 1400 / 1500: loss 2.279848\n",
      "iteration 0 / 1500: loss 1275.474895\n",
      "iteration 100 / 1500: loss 2.460695\n",
      "iteration 200 / 1500: loss 2.246293\n",
      "iteration 300 / 1500: loss 2.192077\n",
      "iteration 400 / 1500: loss 2.240401\n",
      "iteration 500 / 1500: loss 2.269460\n",
      "iteration 600 / 1500: loss 2.220134\n",
      "iteration 700 / 1500: loss 2.235332\n",
      "iteration 800 / 1500: loss 2.180619\n",
      "iteration 900 / 1500: loss 2.396735\n",
      "iteration 1000 / 1500: loss 2.189853\n",
      "iteration 1100 / 1500: loss 2.267225\n",
      "iteration 1200 / 1500: loss 2.230095\n",
      "iteration 1300 / 1500: loss 2.236806\n",
      "iteration 1400 / 1500: loss 2.112617\n",
      "iteration 0 / 1500: loss 1308.859747\n",
      "iteration 100 / 1500: loss 2.309909\n",
      "iteration 200 / 1500: loss 2.158723\n",
      "iteration 300 / 1500: loss 2.252681\n",
      "iteration 400 / 1500: loss 2.217067\n",
      "iteration 500 / 1500: loss 2.194296\n",
      "iteration 600 / 1500: loss 2.380246\n",
      "iteration 700 / 1500: loss 2.206275\n",
      "iteration 800 / 1500: loss 2.198231\n",
      "iteration 900 / 1500: loss 2.320853\n",
      "iteration 1000 / 1500: loss 2.272295\n",
      "iteration 1100 / 1500: loss 2.223492\n",
      "iteration 1200 / 1500: loss 2.238293\n",
      "iteration 1300 / 1500: loss 2.350124\n",
      "iteration 1400 / 1500: loss 2.281498\n",
      "iteration 0 / 1500: loss 1326.163146\n",
      "iteration 100 / 1500: loss 2.258908\n",
      "iteration 200 / 1500: loss 2.238526\n",
      "iteration 300 / 1500: loss 2.260983\n",
      "iteration 400 / 1500: loss 2.222925\n",
      "iteration 500 / 1500: loss 2.216880\n",
      "iteration 600 / 1500: loss 2.250448\n",
      "iteration 700 / 1500: loss 2.236564\n",
      "iteration 800 / 1500: loss 2.314665\n",
      "iteration 900 / 1500: loss 2.225479\n",
      "iteration 1000 / 1500: loss 2.183864\n",
      "iteration 1100 / 1500: loss 2.283991\n",
      "iteration 1200 / 1500: loss 2.203753\n",
      "iteration 1300 / 1500: loss 2.264418\n",
      "iteration 1400 / 1500: loss 2.324500\n",
      "iteration 0 / 1500: loss 1350.862681\n",
      "iteration 100 / 1500: loss 2.271293\n",
      "iteration 200 / 1500: loss 2.233954\n",
      "iteration 300 / 1500: loss 2.268627\n",
      "iteration 400 / 1500: loss 2.238589\n",
      "iteration 500 / 1500: loss 2.260217\n",
      "iteration 600 / 1500: loss 2.277250\n",
      "iteration 700 / 1500: loss 2.283134\n",
      "iteration 800 / 1500: loss 2.380709\n",
      "iteration 900 / 1500: loss 2.297275\n",
      "iteration 1000 / 1500: loss 2.263886\n",
      "iteration 1100 / 1500: loss 2.450938\n",
      "iteration 1200 / 1500: loss 2.264671\n",
      "iteration 1300 / 1500: loss 2.212578\n",
      "iteration 1400 / 1500: loss 2.268889\n",
      "iteration 0 / 1500: loss 1387.349195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 1500: loss 2.301070\n",
      "iteration 200 / 1500: loss 2.260442\n",
      "iteration 300 / 1500: loss 2.279095\n",
      "iteration 400 / 1500: loss 2.271589\n",
      "iteration 500 / 1500: loss 2.211140\n",
      "iteration 600 / 1500: loss 2.200652\n",
      "iteration 700 / 1500: loss 2.407430\n",
      "iteration 800 / 1500: loss 2.236408\n",
      "iteration 900 / 1500: loss 2.310900\n",
      "iteration 1000 / 1500: loss 2.250735\n",
      "iteration 1100 / 1500: loss 2.228134\n",
      "iteration 1200 / 1500: loss 2.167655\n",
      "iteration 1300 / 1500: loss 2.348229\n",
      "iteration 1400 / 1500: loss 2.235986\n",
      "iteration 0 / 1500: loss 1420.800093\n",
      "iteration 100 / 1500: loss 2.260133\n",
      "iteration 200 / 1500: loss 2.205454\n",
      "iteration 300 / 1500: loss 2.548910\n",
      "iteration 400 / 1500: loss 2.317598\n",
      "iteration 500 / 1500: loss 2.305259\n",
      "iteration 600 / 1500: loss 2.404204\n",
      "iteration 700 / 1500: loss 2.176732\n",
      "iteration 800 / 1500: loss 2.218917\n",
      "iteration 900 / 1500: loss 2.220075\n",
      "iteration 1000 / 1500: loss 2.184534\n",
      "iteration 1100 / 1500: loss 2.268883\n",
      "iteration 1200 / 1500: loss 2.371232\n",
      "iteration 1300 / 1500: loss 2.318332\n",
      "iteration 1400 / 1500: loss 2.236597\n",
      "iteration 0 / 1500: loss 1440.454773\n",
      "iteration 100 / 1500: loss 2.283879\n",
      "iteration 200 / 1500: loss 2.213677\n",
      "iteration 300 / 1500: loss 2.234906\n",
      "iteration 400 / 1500: loss 2.205949\n",
      "iteration 500 / 1500: loss 2.237270\n",
      "iteration 600 / 1500: loss 2.223054\n",
      "iteration 700 / 1500: loss 2.378880\n",
      "iteration 800 / 1500: loss 2.278570\n",
      "iteration 900 / 1500: loss 2.148253\n",
      "iteration 1000 / 1500: loss 2.172052\n",
      "iteration 1100 / 1500: loss 2.413456\n",
      "iteration 1200 / 1500: loss 2.226304\n",
      "iteration 1300 / 1500: loss 2.290619\n",
      "iteration 1400 / 1500: loss 2.254379\n",
      "iteration 0 / 1500: loss 1476.410890\n",
      "iteration 100 / 1500: loss 2.364413\n",
      "iteration 200 / 1500: loss 2.315781\n",
      "iteration 300 / 1500: loss 2.247751\n",
      "iteration 400 / 1500: loss 2.456441\n",
      "iteration 500 / 1500: loss 2.288500\n",
      "iteration 600 / 1500: loss 2.136436\n",
      "iteration 700 / 1500: loss 2.132062\n",
      "iteration 800 / 1500: loss 2.091675\n",
      "iteration 900 / 1500: loss 2.253246\n",
      "iteration 1000 / 1500: loss 2.268824\n",
      "iteration 1100 / 1500: loss 2.250794\n",
      "iteration 1200 / 1500: loss 2.300160\n",
      "iteration 1300 / 1500: loss 2.214974\n",
      "iteration 1400 / 1500: loss 2.352716\n",
      "iteration 0 / 1500: loss 1510.795480\n",
      "iteration 100 / 1500: loss 2.513913\n",
      "iteration 200 / 1500: loss 2.300302\n",
      "iteration 300 / 1500: loss 2.391015\n",
      "iteration 400 / 1500: loss 2.308012\n",
      "iteration 500 / 1500: loss 2.273904\n",
      "iteration 600 / 1500: loss 2.245138\n",
      "iteration 700 / 1500: loss 2.368401\n",
      "iteration 800 / 1500: loss 2.233511\n",
      "iteration 900 / 1500: loss 2.445996\n",
      "iteration 1000 / 1500: loss 2.331279\n",
      "iteration 1100 / 1500: loss 2.199157\n",
      "iteration 1200 / 1500: loss 2.378842\n",
      "iteration 1300 / 1500: loss 2.296087\n",
      "iteration 1400 / 1500: loss 2.391359\n",
      "iteration 0 / 1500: loss 1536.389794\n",
      "iteration 100 / 1500: loss 2.303610\n",
      "iteration 200 / 1500: loss 2.180331\n",
      "iteration 300 / 1500: loss 2.251136\n",
      "iteration 400 / 1500: loss 2.438807\n",
      "iteration 500 / 1500: loss 2.412559\n",
      "iteration 600 / 1500: loss 2.303217\n",
      "iteration 700 / 1500: loss 2.349750\n",
      "iteration 800 / 1500: loss 2.273181\n",
      "iteration 900 / 1500: loss 2.222146\n",
      "iteration 1000 / 1500: loss 2.224100\n",
      "iteration 1100 / 1500: loss 2.210808\n",
      "iteration 1200 / 1500: loss 2.199273\n",
      "iteration 1300 / 1500: loss 2.208123\n",
      "iteration 1400 / 1500: loss 2.439085\n",
      "iteration 0 / 1500: loss 779.440603\n",
      "iteration 100 / 1500: loss 2.139014\n",
      "iteration 200 / 1500: loss 2.233154\n",
      "iteration 300 / 1500: loss 2.742144\n",
      "iteration 400 / 1500: loss 2.750694\n",
      "iteration 500 / 1500: loss 2.281942\n",
      "iteration 600 / 1500: loss 2.343020\n",
      "iteration 700 / 1500: loss 2.388744\n",
      "iteration 800 / 1500: loss 3.095110\n",
      "iteration 900 / 1500: loss 2.339453\n",
      "iteration 1000 / 1500: loss 2.689483\n",
      "iteration 1100 / 1500: loss 2.880853\n",
      "iteration 1200 / 1500: loss 2.246299\n",
      "iteration 1300 / 1500: loss 2.405064\n",
      "iteration 1400 / 1500: loss 2.326584\n",
      "iteration 0 / 1500: loss 807.895040\n",
      "iteration 100 / 1500: loss 2.472736\n",
      "iteration 200 / 1500: loss 2.381264\n",
      "iteration 300 / 1500: loss 2.631569\n",
      "iteration 400 / 1500: loss 2.359093\n",
      "iteration 500 / 1500: loss 2.690290\n",
      "iteration 600 / 1500: loss 2.925625\n",
      "iteration 700 / 1500: loss 2.635311\n",
      "iteration 800 / 1500: loss 2.350141\n",
      "iteration 900 / 1500: loss 2.646747\n",
      "iteration 1000 / 1500: loss 2.450804\n",
      "iteration 1100 / 1500: loss 2.292211\n",
      "iteration 1200 / 1500: loss 2.329188\n",
      "iteration 1300 / 1500: loss 2.669270\n",
      "iteration 1400 / 1500: loss 2.497649\n",
      "iteration 0 / 1500: loss 831.597429\n",
      "iteration 100 / 1500: loss 2.313695\n",
      "iteration 200 / 1500: loss 2.506763\n",
      "iteration 300 / 1500: loss 2.334226\n",
      "iteration 400 / 1500: loss 2.884896\n",
      "iteration 500 / 1500: loss 2.582996\n",
      "iteration 600 / 1500: loss 2.367607\n",
      "iteration 700 / 1500: loss 2.174381\n",
      "iteration 800 / 1500: loss 2.792074\n",
      "iteration 900 / 1500: loss 2.672391\n",
      "iteration 1000 / 1500: loss 2.973149\n",
      "iteration 1100 / 1500: loss 2.493242\n",
      "iteration 1200 / 1500: loss 2.632275\n",
      "iteration 1300 / 1500: loss 2.383565\n",
      "iteration 1400 / 1500: loss 2.601241\n",
      "iteration 0 / 1500: loss 864.073948\n",
      "iteration 100 / 1500: loss 2.676460\n",
      "iteration 200 / 1500: loss 2.459929\n",
      "iteration 300 / 1500: loss 2.303682\n",
      "iteration 400 / 1500: loss 3.178272\n",
      "iteration 500 / 1500: loss 2.342945\n",
      "iteration 600 / 1500: loss 2.890484\n",
      "iteration 700 / 1500: loss 2.277718\n",
      "iteration 800 / 1500: loss 2.479192\n",
      "iteration 900 / 1500: loss 2.573960\n",
      "iteration 1000 / 1500: loss 2.280047\n",
      "iteration 1100 / 1500: loss 3.274906\n",
      "iteration 1200 / 1500: loss 2.594961\n",
      "iteration 1300 / 1500: loss 2.345221\n",
      "iteration 1400 / 1500: loss 2.620101\n",
      "iteration 0 / 1500: loss 903.187056\n",
      "iteration 100 / 1500: loss 2.483270\n",
      "iteration 200 / 1500: loss 2.342793\n",
      "iteration 300 / 1500: loss 2.262696\n",
      "iteration 400 / 1500: loss 3.004727\n",
      "iteration 500 / 1500: loss 3.099121\n",
      "iteration 600 / 1500: loss 2.797902\n",
      "iteration 700 / 1500: loss 2.535538\n",
      "iteration 800 / 1500: loss 2.581887\n",
      "iteration 900 / 1500: loss 2.604367\n",
      "iteration 1000 / 1500: loss 2.785840\n",
      "iteration 1100 / 1500: loss 2.515459\n",
      "iteration 1200 / 1500: loss 2.193789\n",
      "iteration 1300 / 1500: loss 2.645597\n",
      "iteration 1400 / 1500: loss 3.147786\n",
      "iteration 0 / 1500: loss 931.840741\n",
      "iteration 100 / 1500: loss 2.640899\n",
      "iteration 200 / 1500: loss 2.585291\n",
      "iteration 300 / 1500: loss 2.713291\n",
      "iteration 400 / 1500: loss 2.653151\n",
      "iteration 500 / 1500: loss 2.474913\n",
      "iteration 600 / 1500: loss 2.777124\n",
      "iteration 700 / 1500: loss 2.775483\n",
      "iteration 800 / 1500: loss 2.694841\n",
      "iteration 900 / 1500: loss 2.808807\n",
      "iteration 1000 / 1500: loss 3.107100\n",
      "iteration 1100 / 1500: loss 2.458164\n",
      "iteration 1200 / 1500: loss 2.792454\n",
      "iteration 1300 / 1500: loss 2.785959\n",
      "iteration 1400 / 1500: loss 2.591801\n",
      "iteration 0 / 1500: loss 952.484982\n",
      "iteration 100 / 1500: loss 2.591695\n",
      "iteration 200 / 1500: loss 2.345284\n",
      "iteration 300 / 1500: loss 2.423780\n",
      "iteration 400 / 1500: loss 2.379229\n",
      "iteration 500 / 1500: loss 2.740808\n",
      "iteration 600 / 1500: loss 2.685514\n",
      "iteration 700 / 1500: loss 2.320570\n",
      "iteration 800 / 1500: loss 3.009442\n",
      "iteration 900 / 1500: loss 3.457341\n",
      "iteration 1000 / 1500: loss 2.885509\n",
      "iteration 1100 / 1500: loss 2.552140\n",
      "iteration 1200 / 1500: loss 2.592324\n",
      "iteration 1300 / 1500: loss 2.374175\n",
      "iteration 1400 / 1500: loss 2.877507\n",
      "iteration 0 / 1500: loss 989.186967\n",
      "iteration 100 / 1500: loss 2.263889\n",
      "iteration 200 / 1500: loss 2.385392\n",
      "iteration 300 / 1500: loss 3.093319\n",
      "iteration 400 / 1500: loss 2.873739\n",
      "iteration 500 / 1500: loss 2.593605\n",
      "iteration 600 / 1500: loss 2.749124\n",
      "iteration 700 / 1500: loss 2.296492\n",
      "iteration 800 / 1500: loss 2.807767\n",
      "iteration 900 / 1500: loss 3.160125\n",
      "iteration 1000 / 1500: loss 3.048746\n",
      "iteration 1100 / 1500: loss 2.841569\n",
      "iteration 1200 / 1500: loss 2.485433\n",
      "iteration 1300 / 1500: loss 2.580387\n",
      "iteration 1400 / 1500: loss 2.304876\n",
      "iteration 0 / 1500: loss 1022.825120\n",
      "iteration 100 / 1500: loss 2.935609\n",
      "iteration 200 / 1500: loss 2.977264\n",
      "iteration 300 / 1500: loss 2.936785\n",
      "iteration 400 / 1500: loss 2.302838\n",
      "iteration 500 / 1500: loss 2.749399\n",
      "iteration 600 / 1500: loss 2.327842\n",
      "iteration 700 / 1500: loss 2.398951\n",
      "iteration 800 / 1500: loss 2.415310\n",
      "iteration 900 / 1500: loss 2.621907\n",
      "iteration 1000 / 1500: loss 2.471368\n",
      "iteration 1100 / 1500: loss 2.379657\n",
      "iteration 1200 / 1500: loss 2.771710\n",
      "iteration 1300 / 1500: loss 3.048083\n",
      "iteration 1400 / 1500: loss 2.879462\n",
      "iteration 0 / 1500: loss 1071.307770\n",
      "iteration 100 / 1500: loss 2.285693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss 3.526621\n",
      "iteration 300 / 1500: loss 2.839230\n",
      "iteration 400 / 1500: loss 2.786524\n",
      "iteration 500 / 1500: loss 2.626534\n",
      "iteration 600 / 1500: loss 2.720526\n",
      "iteration 700 / 1500: loss 3.213883\n",
      "iteration 800 / 1500: loss 2.823851\n",
      "iteration 900 / 1500: loss 2.765424\n",
      "iteration 1000 / 1500: loss 2.662691\n",
      "iteration 1100 / 1500: loss 2.576085\n",
      "iteration 1200 / 1500: loss 2.261780\n",
      "iteration 1300 / 1500: loss 2.358486\n",
      "iteration 1400 / 1500: loss 2.929022\n",
      "iteration 0 / 1500: loss 1087.192535\n",
      "iteration 100 / 1500: loss 2.588221\n",
      "iteration 200 / 1500: loss 2.783782\n",
      "iteration 300 / 1500: loss 2.830132\n",
      "iteration 400 / 1500: loss 2.867786\n",
      "iteration 500 / 1500: loss 2.422898\n",
      "iteration 600 / 1500: loss 2.474709\n",
      "iteration 700 / 1500: loss 2.979669\n",
      "iteration 800 / 1500: loss 2.535797\n",
      "iteration 900 / 1500: loss 2.814518\n",
      "iteration 1000 / 1500: loss 2.654680\n",
      "iteration 1100 / 1500: loss 2.334867\n",
      "iteration 1200 / 1500: loss 3.173964\n",
      "iteration 1300 / 1500: loss 2.406049\n",
      "iteration 1400 / 1500: loss 2.850117\n",
      "iteration 0 / 1500: loss 1102.154026\n",
      "iteration 100 / 1500: loss 2.644009\n",
      "iteration 200 / 1500: loss 2.394814\n",
      "iteration 300 / 1500: loss 2.978985\n",
      "iteration 400 / 1500: loss 2.926794\n",
      "iteration 500 / 1500: loss 2.557128\n",
      "iteration 600 / 1500: loss 2.913449\n",
      "iteration 700 / 1500: loss 2.770209\n",
      "iteration 800 / 1500: loss 2.361565\n",
      "iteration 900 / 1500: loss 3.088535\n",
      "iteration 1000 / 1500: loss 2.601220\n",
      "iteration 1100 / 1500: loss 2.332863\n",
      "iteration 1200 / 1500: loss 3.462493\n",
      "iteration 1300 / 1500: loss 2.691617\n",
      "iteration 1400 / 1500: loss 3.198665\n",
      "iteration 0 / 1500: loss 1145.632961\n",
      "iteration 100 / 1500: loss 2.923604\n",
      "iteration 200 / 1500: loss 3.410133\n",
      "iteration 300 / 1500: loss 2.308094\n",
      "iteration 400 / 1500: loss 2.954317\n",
      "iteration 500 / 1500: loss 3.245070\n",
      "iteration 600 / 1500: loss 3.200391\n",
      "iteration 700 / 1500: loss 2.883698\n",
      "iteration 800 / 1500: loss 3.258856\n",
      "iteration 900 / 1500: loss 2.572632\n",
      "iteration 1000 / 1500: loss 2.548050\n",
      "iteration 1100 / 1500: loss 2.761784\n",
      "iteration 1200 / 1500: loss 2.605019\n",
      "iteration 1300 / 1500: loss 2.885501\n",
      "iteration 1400 / 1500: loss 2.482077\n",
      "iteration 0 / 1500: loss 1177.043405\n",
      "iteration 100 / 1500: loss 2.703266\n",
      "iteration 200 / 1500: loss 2.747393\n",
      "iteration 300 / 1500: loss 2.969145\n",
      "iteration 400 / 1500: loss 2.515494\n",
      "iteration 500 / 1500: loss 2.988806\n",
      "iteration 600 / 1500: loss 2.636778\n",
      "iteration 700 / 1500: loss 2.728397\n",
      "iteration 800 / 1500: loss 3.268981\n",
      "iteration 900 / 1500: loss 2.866747\n",
      "iteration 1000 / 1500: loss 2.669766\n",
      "iteration 1100 / 1500: loss 2.810738\n",
      "iteration 1200 / 1500: loss 3.255710\n",
      "iteration 1300 / 1500: loss 2.526598\n",
      "iteration 1400 / 1500: loss 3.099244\n",
      "iteration 0 / 1500: loss 1214.612751\n",
      "iteration 100 / 1500: loss 2.383349\n",
      "iteration 200 / 1500: loss 2.523331\n",
      "iteration 300 / 1500: loss 2.911659\n",
      "iteration 400 / 1500: loss 3.215093\n",
      "iteration 500 / 1500: loss 3.109711\n",
      "iteration 600 / 1500: loss 2.604708\n",
      "iteration 700 / 1500: loss 3.551379\n",
      "iteration 800 / 1500: loss 3.668863\n",
      "iteration 900 / 1500: loss 3.122677\n",
      "iteration 1000 / 1500: loss 3.711755\n",
      "iteration 1100 / 1500: loss 2.667950\n",
      "iteration 1200 / 1500: loss 2.983528\n",
      "iteration 1300 / 1500: loss 2.704476\n",
      "iteration 1400 / 1500: loss 2.507234\n",
      "iteration 0 / 1500: loss 1243.756529\n",
      "iteration 100 / 1500: loss 2.451346\n",
      "iteration 200 / 1500: loss 2.761182\n",
      "iteration 300 / 1500: loss 2.752108\n",
      "iteration 400 / 1500: loss 3.250642\n",
      "iteration 500 / 1500: loss 2.666483\n",
      "iteration 600 / 1500: loss 2.382688\n",
      "iteration 700 / 1500: loss 3.130072\n",
      "iteration 800 / 1500: loss 3.053756\n",
      "iteration 900 / 1500: loss 3.337434\n",
      "iteration 1000 / 1500: loss 2.891691\n",
      "iteration 1100 / 1500: loss 2.993835\n",
      "iteration 1200 / 1500: loss 2.649489\n",
      "iteration 1300 / 1500: loss 3.113244\n",
      "iteration 1400 / 1500: loss 3.044044\n",
      "iteration 0 / 1500: loss 1275.965347\n",
      "iteration 100 / 1500: loss 2.695718\n",
      "iteration 200 / 1500: loss 3.010149\n",
      "iteration 300 / 1500: loss 3.072065\n",
      "iteration 400 / 1500: loss 2.886050\n",
      "iteration 500 / 1500: loss 3.338820\n",
      "iteration 600 / 1500: loss 2.957643\n",
      "iteration 700 / 1500: loss 2.176530\n",
      "iteration 800 / 1500: loss 3.170419\n",
      "iteration 900 / 1500: loss 3.569129\n",
      "iteration 1000 / 1500: loss 3.196308\n",
      "iteration 1100 / 1500: loss 2.763683\n",
      "iteration 1200 / 1500: loss 3.479143\n",
      "iteration 1300 / 1500: loss 3.114618\n",
      "iteration 1400 / 1500: loss 2.854118\n",
      "iteration 0 / 1500: loss 1286.700734\n",
      "iteration 100 / 1500: loss 3.221311\n",
      "iteration 200 / 1500: loss 3.127428\n",
      "iteration 300 / 1500: loss 2.838445\n",
      "iteration 400 / 1500: loss 2.812670\n",
      "iteration 500 / 1500: loss 3.798477\n",
      "iteration 600 / 1500: loss 3.261571\n",
      "iteration 700 / 1500: loss 3.209914\n",
      "iteration 800 / 1500: loss 3.192803\n",
      "iteration 900 / 1500: loss 3.080649\n",
      "iteration 1000 / 1500: loss 2.807351\n",
      "iteration 1100 / 1500: loss 3.706497\n",
      "iteration 1200 / 1500: loss 3.423537\n",
      "iteration 1300 / 1500: loss 2.850250\n",
      "iteration 1400 / 1500: loss 3.553383\n",
      "iteration 0 / 1500: loss 1325.227903\n",
      "iteration 100 / 1500: loss 3.391609\n",
      "iteration 200 / 1500: loss 2.763626\n",
      "iteration 300 / 1500: loss 2.741964\n",
      "iteration 400 / 1500: loss 2.797409\n",
      "iteration 500 / 1500: loss 3.398606\n",
      "iteration 600 / 1500: loss 3.488171\n",
      "iteration 700 / 1500: loss 2.778195\n",
      "iteration 800 / 1500: loss 3.127739\n",
      "iteration 900 / 1500: loss 3.073832\n",
      "iteration 1000 / 1500: loss 2.792592\n",
      "iteration 1100 / 1500: loss 3.584077\n",
      "iteration 1200 / 1500: loss 2.651638\n",
      "iteration 1300 / 1500: loss 3.615253\n",
      "iteration 1400 / 1500: loss 3.045329\n",
      "iteration 0 / 1500: loss 1367.224637\n",
      "iteration 100 / 1500: loss 3.148725\n",
      "iteration 200 / 1500: loss 2.868813\n",
      "iteration 300 / 1500: loss 2.672228\n",
      "iteration 400 / 1500: loss 2.273137\n",
      "iteration 500 / 1500: loss 2.807786\n",
      "iteration 600 / 1500: loss 3.933653\n",
      "iteration 700 / 1500: loss 2.903292\n",
      "iteration 800 / 1500: loss 2.729167\n",
      "iteration 900 / 1500: loss 3.105269\n",
      "iteration 1000 / 1500: loss 3.907554\n",
      "iteration 1100 / 1500: loss 2.944514\n",
      "iteration 1200 / 1500: loss 3.265183\n",
      "iteration 1300 / 1500: loss 3.206684\n",
      "iteration 1400 / 1500: loss 2.943829\n",
      "iteration 0 / 1500: loss 1374.221158\n",
      "iteration 100 / 1500: loss 3.448978\n",
      "iteration 200 / 1500: loss 2.983796\n",
      "iteration 300 / 1500: loss 2.818976\n",
      "iteration 400 / 1500: loss 2.525976\n",
      "iteration 500 / 1500: loss 2.546396\n",
      "iteration 600 / 1500: loss 2.804758\n",
      "iteration 700 / 1500: loss 3.442144\n",
      "iteration 800 / 1500: loss 3.137521\n",
      "iteration 900 / 1500: loss 2.858798\n",
      "iteration 1000 / 1500: loss 3.116913\n",
      "iteration 1100 / 1500: loss 3.432762\n",
      "iteration 1200 / 1500: loss 3.979452\n",
      "iteration 1300 / 1500: loss 3.578286\n",
      "iteration 1400 / 1500: loss 2.736374\n",
      "iteration 0 / 1500: loss 1417.885340\n",
      "iteration 100 / 1500: loss 2.649458\n",
      "iteration 200 / 1500: loss 3.107613\n",
      "iteration 300 / 1500: loss 2.902058\n",
      "iteration 400 / 1500: loss 3.017241\n",
      "iteration 500 / 1500: loss 3.380392\n",
      "iteration 600 / 1500: loss 2.965627\n",
      "iteration 700 / 1500: loss 3.195775\n",
      "iteration 800 / 1500: loss 3.428688\n",
      "iteration 900 / 1500: loss 2.896071\n",
      "iteration 1000 / 1500: loss 3.437004\n",
      "iteration 1100 / 1500: loss 3.055508\n",
      "iteration 1200 / 1500: loss 3.076757\n",
      "iteration 1300 / 1500: loss 3.864888\n",
      "iteration 1400 / 1500: loss 3.073422\n",
      "iteration 0 / 1500: loss 1459.807837\n",
      "iteration 100 / 1500: loss 2.623640\n",
      "iteration 200 / 1500: loss 3.125707\n",
      "iteration 300 / 1500: loss 3.402997\n",
      "iteration 400 / 1500: loss 2.736943\n",
      "iteration 500 / 1500: loss 3.767900\n",
      "iteration 600 / 1500: loss 3.452639\n",
      "iteration 700 / 1500: loss 3.756116\n",
      "iteration 800 / 1500: loss 3.009559\n",
      "iteration 900 / 1500: loss 3.117631\n",
      "iteration 1000 / 1500: loss 3.633826\n",
      "iteration 1100 / 1500: loss 3.550587\n",
      "iteration 1200 / 1500: loss 2.992489\n",
      "iteration 1300 / 1500: loss 3.357815\n",
      "iteration 1400 / 1500: loss 2.588709\n",
      "iteration 0 / 1500: loss 1476.598972\n",
      "iteration 100 / 1500: loss 3.438655\n",
      "iteration 200 / 1500: loss 2.727992\n",
      "iteration 300 / 1500: loss 4.060735\n",
      "iteration 400 / 1500: loss 3.529290\n",
      "iteration 500 / 1500: loss 2.918236\n",
      "iteration 600 / 1500: loss 3.131150\n",
      "iteration 700 / 1500: loss 3.733816\n",
      "iteration 800 / 1500: loss 3.638570\n",
      "iteration 900 / 1500: loss 3.267167\n",
      "iteration 1000 / 1500: loss 3.828308\n",
      "iteration 1100 / 1500: loss 3.157927\n",
      "iteration 1200 / 1500: loss 4.443538\n",
      "iteration 1300 / 1500: loss 2.761042\n",
      "iteration 1400 / 1500: loss 2.693039\n",
      "iteration 0 / 1500: loss 1510.637037\n",
      "iteration 100 / 1500: loss 3.716737\n",
      "iteration 200 / 1500: loss 3.515095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 1500: loss 3.463012\n",
      "iteration 400 / 1500: loss 3.099133\n",
      "iteration 500 / 1500: loss 2.880813\n",
      "iteration 600 / 1500: loss 3.699340\n",
      "iteration 700 / 1500: loss 3.113022\n",
      "iteration 800 / 1500: loss 3.635825\n",
      "iteration 900 / 1500: loss 3.552800\n",
      "iteration 1000 / 1500: loss 4.092940\n",
      "iteration 1100 / 1500: loss 3.627245\n",
      "iteration 1200 / 1500: loss 3.801445\n",
      "iteration 1300 / 1500: loss 2.817010\n",
      "iteration 1400 / 1500: loss 3.110200\n",
      "iteration 0 / 1500: loss 1543.645371\n",
      "iteration 100 / 1500: loss 2.858967\n",
      "iteration 200 / 1500: loss 2.902917\n",
      "iteration 300 / 1500: loss 3.433632\n",
      "iteration 400 / 1500: loss 3.459247\n",
      "iteration 500 / 1500: loss 3.459338\n",
      "iteration 600 / 1500: loss 3.144806\n",
      "iteration 700 / 1500: loss 4.024355\n",
      "iteration 800 / 1500: loss 3.728151\n",
      "iteration 900 / 1500: loss 3.118240\n",
      "iteration 1000 / 1500: loss 3.712308\n",
      "iteration 1100 / 1500: loss 3.137401\n",
      "iteration 1200 / 1500: loss 3.331842\n",
      "iteration 1300 / 1500: loss 3.205964\n",
      "iteration 1400 / 1500: loss 3.306376\n",
      "iteration 0 / 1500: loss 767.891409\n",
      "iteration 100 / 1500: loss 4.154295\n",
      "iteration 200 / 1500: loss 3.680074\n",
      "iteration 300 / 1500: loss 3.575503\n",
      "iteration 400 / 1500: loss 3.233867\n",
      "iteration 500 / 1500: loss 3.146126\n",
      "iteration 600 / 1500: loss 2.919231\n",
      "iteration 700 / 1500: loss 3.761274\n",
      "iteration 800 / 1500: loss 3.088021\n",
      "iteration 900 / 1500: loss 2.470483\n",
      "iteration 1000 / 1500: loss 3.011049\n",
      "iteration 1100 / 1500: loss 2.591842\n",
      "iteration 1200 / 1500: loss 3.528488\n",
      "iteration 1300 / 1500: loss 3.623101\n",
      "iteration 1400 / 1500: loss 3.435995\n",
      "iteration 0 / 1500: loss 805.485919\n",
      "iteration 100 / 1500: loss 2.590569\n",
      "iteration 200 / 1500: loss 2.993870\n",
      "iteration 300 / 1500: loss 4.563400\n",
      "iteration 400 / 1500: loss 4.515111\n",
      "iteration 500 / 1500: loss 3.744969\n",
      "iteration 600 / 1500: loss 2.950848\n",
      "iteration 700 / 1500: loss 2.770389\n",
      "iteration 800 / 1500: loss 3.738087\n",
      "iteration 900 / 1500: loss 3.314396\n",
      "iteration 1000 / 1500: loss 3.758600\n",
      "iteration 1100 / 1500: loss 3.615936\n",
      "iteration 1200 / 1500: loss 2.857576\n",
      "iteration 1300 / 1500: loss 3.384898\n",
      "iteration 1400 / 1500: loss 3.237433\n",
      "iteration 0 / 1500: loss 832.651463\n",
      "iteration 100 / 1500: loss 2.862281\n",
      "iteration 200 / 1500: loss 3.666529\n",
      "iteration 300 / 1500: loss 3.635050\n",
      "iteration 400 / 1500: loss 5.330599\n",
      "iteration 500 / 1500: loss 2.949460\n",
      "iteration 600 / 1500: loss 3.451071\n",
      "iteration 700 / 1500: loss 3.532746\n",
      "iteration 800 / 1500: loss 3.209891\n",
      "iteration 900 / 1500: loss 3.427214\n",
      "iteration 1000 / 1500: loss 4.816019\n",
      "iteration 1100 / 1500: loss 4.063761\n",
      "iteration 1200 / 1500: loss 3.934762\n",
      "iteration 1300 / 1500: loss 4.096699\n",
      "iteration 1400 / 1500: loss 4.063203\n",
      "iteration 0 / 1500: loss 880.155854\n",
      "iteration 100 / 1500: loss 3.928871\n",
      "iteration 200 / 1500: loss 2.963552\n",
      "iteration 300 / 1500: loss 4.490409\n",
      "iteration 400 / 1500: loss 3.265179\n",
      "iteration 500 / 1500: loss 5.414551\n",
      "iteration 600 / 1500: loss 4.849127\n",
      "iteration 700 / 1500: loss 2.860365\n",
      "iteration 800 / 1500: loss 4.470623\n",
      "iteration 900 / 1500: loss 2.949239\n",
      "iteration 1000 / 1500: loss 3.633562\n",
      "iteration 1100 / 1500: loss 4.534469\n",
      "iteration 1200 / 1500: loss 3.438164\n",
      "iteration 1300 / 1500: loss 3.374322\n",
      "iteration 1400 / 1500: loss 4.083788\n",
      "iteration 0 / 1500: loss 901.773328\n",
      "iteration 100 / 1500: loss 3.320109\n",
      "iteration 200 / 1500: loss 4.784631\n",
      "iteration 300 / 1500: loss 3.830627\n",
      "iteration 400 / 1500: loss 3.927367\n",
      "iteration 500 / 1500: loss 2.934569\n",
      "iteration 600 / 1500: loss 4.779417\n",
      "iteration 700 / 1500: loss 2.902067\n",
      "iteration 800 / 1500: loss 2.709229\n",
      "iteration 900 / 1500: loss 3.490463\n",
      "iteration 1000 / 1500: loss 2.954210\n",
      "iteration 1100 / 1500: loss 4.047505\n",
      "iteration 1200 / 1500: loss 4.430422\n",
      "iteration 1300 / 1500: loss 3.151178\n",
      "iteration 1400 / 1500: loss 4.316847\n",
      "iteration 0 / 1500: loss 921.620957\n",
      "iteration 100 / 1500: loss 3.468276\n",
      "iteration 200 / 1500: loss 3.670002\n",
      "iteration 300 / 1500: loss 4.715759\n",
      "iteration 400 / 1500: loss 3.174232\n",
      "iteration 500 / 1500: loss 2.622554\n",
      "iteration 600 / 1500: loss 3.867852\n",
      "iteration 700 / 1500: loss 3.196731\n",
      "iteration 800 / 1500: loss 2.990509\n",
      "iteration 900 / 1500: loss 3.347439\n",
      "iteration 1000 / 1500: loss 3.667432\n",
      "iteration 1100 / 1500: loss 3.572737\n",
      "iteration 1200 / 1500: loss 4.602178\n",
      "iteration 1300 / 1500: loss 3.768074\n",
      "iteration 1400 / 1500: loss 3.670267\n",
      "iteration 0 / 1500: loss 966.959873\n",
      "iteration 100 / 1500: loss 4.786476\n",
      "iteration 200 / 1500: loss 4.561238\n",
      "iteration 300 / 1500: loss 3.482775\n",
      "iteration 400 / 1500: loss 3.883582\n",
      "iteration 500 / 1500: loss 3.911518\n",
      "iteration 600 / 1500: loss 3.289376\n",
      "iteration 700 / 1500: loss 3.785121\n",
      "iteration 800 / 1500: loss 3.333591\n",
      "iteration 900 / 1500: loss 3.836430\n",
      "iteration 1000 / 1500: loss 3.359790\n",
      "iteration 1100 / 1500: loss 4.456039\n",
      "iteration 1200 / 1500: loss 4.319246\n",
      "iteration 1300 / 1500: loss 4.053973\n",
      "iteration 1400 / 1500: loss 3.149935\n",
      "iteration 0 / 1500: loss 983.868107\n",
      "iteration 100 / 1500: loss 3.336598\n",
      "iteration 200 / 1500: loss 3.012438\n",
      "iteration 300 / 1500: loss 5.310720\n",
      "iteration 400 / 1500: loss 4.037068\n",
      "iteration 500 / 1500: loss 3.656730\n",
      "iteration 600 / 1500: loss 3.848295\n",
      "iteration 700 / 1500: loss 3.595539\n",
      "iteration 800 / 1500: loss 5.663939\n",
      "iteration 900 / 1500: loss 3.696631\n",
      "iteration 1000 / 1500: loss 3.646970\n",
      "iteration 1100 / 1500: loss 3.194564\n",
      "iteration 1200 / 1500: loss 4.007593\n",
      "iteration 1300 / 1500: loss 4.355435\n",
      "iteration 1400 / 1500: loss 4.129636\n",
      "iteration 0 / 1500: loss 1026.673938\n",
      "iteration 100 / 1500: loss 4.132701\n",
      "iteration 200 / 1500: loss 3.510305\n",
      "iteration 300 / 1500: loss 5.098682\n",
      "iteration 400 / 1500: loss 4.056276\n",
      "iteration 500 / 1500: loss 3.822621\n",
      "iteration 600 / 1500: loss 4.313967\n",
      "iteration 700 / 1500: loss 3.674592\n",
      "iteration 800 / 1500: loss 3.097585\n",
      "iteration 900 / 1500: loss 4.679168\n",
      "iteration 1000 / 1500: loss 2.923742\n",
      "iteration 1100 / 1500: loss 3.726236\n",
      "iteration 1200 / 1500: loss 4.449680\n",
      "iteration 1300 / 1500: loss 3.792496\n",
      "iteration 1400 / 1500: loss 3.363553\n",
      "iteration 0 / 1500: loss 1053.013371\n",
      "iteration 100 / 1500: loss 4.084605\n",
      "iteration 200 / 1500: loss 4.197084\n",
      "iteration 300 / 1500: loss 4.272043\n",
      "iteration 400 / 1500: loss 5.758564\n",
      "iteration 500 / 1500: loss 4.564660\n",
      "iteration 600 / 1500: loss 3.159060\n",
      "iteration 700 / 1500: loss 4.278467\n",
      "iteration 800 / 1500: loss 4.830757\n",
      "iteration 900 / 1500: loss 3.848472\n",
      "iteration 1000 / 1500: loss 3.297255\n",
      "iteration 1100 / 1500: loss 4.098463\n",
      "iteration 1200 / 1500: loss 4.714073\n",
      "iteration 1300 / 1500: loss 3.497671\n",
      "iteration 1400 / 1500: loss 4.782824\n",
      "iteration 0 / 1500: loss 1075.196362\n",
      "iteration 100 / 1500: loss 3.675739\n",
      "iteration 200 / 1500: loss 4.205196\n",
      "iteration 300 / 1500: loss 4.923087\n",
      "iteration 400 / 1500: loss 4.083124\n",
      "iteration 500 / 1500: loss 4.629231\n",
      "iteration 600 / 1500: loss 5.337443\n",
      "iteration 700 / 1500: loss 4.219816\n",
      "iteration 800 / 1500: loss 4.716520\n",
      "iteration 900 / 1500: loss 4.461842\n",
      "iteration 1000 / 1500: loss 4.008143\n",
      "iteration 1100 / 1500: loss 3.949618\n",
      "iteration 1200 / 1500: loss 4.526770\n",
      "iteration 1300 / 1500: loss 3.744275\n",
      "iteration 1400 / 1500: loss 4.343972\n",
      "iteration 0 / 1500: loss 1113.170078\n",
      "iteration 100 / 1500: loss 4.524961\n",
      "iteration 200 / 1500: loss 5.804496\n",
      "iteration 300 / 1500: loss 3.977520\n",
      "iteration 400 / 1500: loss 5.294246\n",
      "iteration 500 / 1500: loss 3.929917\n",
      "iteration 600 / 1500: loss 4.268519\n",
      "iteration 700 / 1500: loss 5.104277\n",
      "iteration 800 / 1500: loss 4.429787\n",
      "iteration 900 / 1500: loss 3.800006\n",
      "iteration 1000 / 1500: loss 4.285911\n",
      "iteration 1100 / 1500: loss 3.733766\n",
      "iteration 1200 / 1500: loss 4.536902\n",
      "iteration 1300 / 1500: loss 4.284554\n",
      "iteration 1400 / 1500: loss 3.980095\n",
      "iteration 0 / 1500: loss 1142.935065\n",
      "iteration 100 / 1500: loss 3.781373\n",
      "iteration 200 / 1500: loss 3.094211\n",
      "iteration 300 / 1500: loss 5.127136\n",
      "iteration 400 / 1500: loss 4.182091\n",
      "iteration 500 / 1500: loss 5.237465\n",
      "iteration 600 / 1500: loss 3.851001\n",
      "iteration 700 / 1500: loss 4.492609\n",
      "iteration 800 / 1500: loss 4.861051\n",
      "iteration 900 / 1500: loss 5.215084\n",
      "iteration 1000 / 1500: loss 3.588097\n",
      "iteration 1100 / 1500: loss 3.929307\n",
      "iteration 1200 / 1500: loss 5.058454\n",
      "iteration 1300 / 1500: loss 5.015521\n",
      "iteration 1400 / 1500: loss 4.151571\n",
      "iteration 0 / 1500: loss 1167.395326\n",
      "iteration 100 / 1500: loss 4.360116\n",
      "iteration 200 / 1500: loss 4.582873\n",
      "iteration 300 / 1500: loss 4.329908\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 1500: loss 4.566109\n",
      "iteration 500 / 1500: loss 4.020478\n",
      "iteration 600 / 1500: loss 3.734862\n",
      "iteration 700 / 1500: loss 4.154455\n",
      "iteration 800 / 1500: loss 3.598986\n",
      "iteration 900 / 1500: loss 4.605938\n",
      "iteration 1000 / 1500: loss 4.845482\n",
      "iteration 1100 / 1500: loss 4.769449\n",
      "iteration 1200 / 1500: loss 4.647003\n",
      "iteration 1300 / 1500: loss 4.662266\n",
      "iteration 1400 / 1500: loss 5.619502\n",
      "iteration 0 / 1500: loss 1205.019486\n",
      "iteration 100 / 1500: loss 3.510931\n",
      "iteration 200 / 1500: loss 5.556262\n",
      "iteration 300 / 1500: loss 4.544948\n",
      "iteration 400 / 1500: loss 4.733134\n",
      "iteration 500 / 1500: loss 4.773155\n",
      "iteration 600 / 1500: loss 4.552715\n",
      "iteration 700 / 1500: loss 4.044849\n",
      "iteration 800 / 1500: loss 5.313025\n",
      "iteration 900 / 1500: loss 3.883329\n",
      "iteration 1000 / 1500: loss 5.153309\n",
      "iteration 1100 / 1500: loss 3.426639\n",
      "iteration 1200 / 1500: loss 4.064561\n",
      "iteration 1300 / 1500: loss 5.130236\n",
      "iteration 1400 / 1500: loss 4.185084\n",
      "iteration 0 / 1500: loss 1230.363607\n",
      "iteration 100 / 1500: loss 3.995677\n",
      "iteration 200 / 1500: loss 4.936269\n",
      "iteration 300 / 1500: loss 4.696923\n",
      "iteration 400 / 1500: loss 4.094896\n",
      "iteration 500 / 1500: loss 5.149458\n",
      "iteration 600 / 1500: loss 4.123698\n",
      "iteration 700 / 1500: loss 4.523296\n",
      "iteration 800 / 1500: loss 3.773195\n",
      "iteration 900 / 1500: loss 5.013091\n",
      "iteration 1000 / 1500: loss 4.301999\n",
      "iteration 1100 / 1500: loss 3.590870\n",
      "iteration 1200 / 1500: loss 4.194248\n",
      "iteration 1300 / 1500: loss 5.166915\n",
      "iteration 1400 / 1500: loss 5.117041\n",
      "iteration 0 / 1500: loss 1276.667366\n",
      "iteration 100 / 1500: loss 4.697917\n",
      "iteration 200 / 1500: loss 6.569425\n",
      "iteration 300 / 1500: loss 4.780340\n",
      "iteration 400 / 1500: loss 6.262282\n",
      "iteration 500 / 1500: loss 3.397571\n",
      "iteration 600 / 1500: loss 3.958152\n",
      "iteration 700 / 1500: loss 5.318356\n",
      "iteration 800 / 1500: loss 3.488255\n",
      "iteration 900 / 1500: loss 3.713407\n",
      "iteration 1000 / 1500: loss 4.877487\n",
      "iteration 1100 / 1500: loss 4.453720\n",
      "iteration 1200 / 1500: loss 4.543321\n",
      "iteration 1300 / 1500: loss 4.269537\n",
      "iteration 1400 / 1500: loss 5.446710\n",
      "iteration 0 / 1500: loss 1303.026373\n",
      "iteration 100 / 1500: loss 3.750538\n",
      "iteration 200 / 1500: loss 3.721583\n",
      "iteration 300 / 1500: loss 4.801531\n",
      "iteration 400 / 1500: loss 4.038081\n",
      "iteration 500 / 1500: loss 6.084872\n",
      "iteration 600 / 1500: loss 5.255703\n",
      "iteration 700 / 1500: loss 5.882308\n",
      "iteration 800 / 1500: loss 4.574309\n",
      "iteration 900 / 1500: loss 5.508751\n",
      "iteration 1000 / 1500: loss 5.138915\n",
      "iteration 1100 / 1500: loss 4.347528\n",
      "iteration 1200 / 1500: loss 5.440517\n",
      "iteration 1300 / 1500: loss 4.180135\n",
      "iteration 1400 / 1500: loss 5.337103\n",
      "iteration 0 / 1500: loss 1295.856331\n",
      "iteration 100 / 1500: loss 3.882633\n",
      "iteration 200 / 1500: loss 4.557867\n",
      "iteration 300 / 1500: loss 5.637366\n",
      "iteration 400 / 1500: loss 4.643188\n",
      "iteration 500 / 1500: loss 4.455649\n",
      "iteration 600 / 1500: loss 5.352832\n",
      "iteration 700 / 1500: loss 4.803904\n",
      "iteration 800 / 1500: loss 5.653655\n",
      "iteration 900 / 1500: loss 3.794491\n",
      "iteration 1000 / 1500: loss 4.670636\n",
      "iteration 1100 / 1500: loss 3.804350\n",
      "iteration 1200 / 1500: loss 5.276126\n",
      "iteration 1300 / 1500: loss 5.436095\n",
      "iteration 1400 / 1500: loss 4.728819\n",
      "iteration 0 / 1500: loss 1360.158882\n",
      "iteration 100 / 1500: loss 5.140333\n",
      "iteration 200 / 1500: loss 4.891829\n",
      "iteration 300 / 1500: loss 5.537548\n",
      "iteration 400 / 1500: loss 4.293022\n",
      "iteration 500 / 1500: loss 4.403754\n",
      "iteration 600 / 1500: loss 4.402311\n",
      "iteration 700 / 1500: loss 4.886194\n",
      "iteration 800 / 1500: loss 5.155216\n",
      "iteration 900 / 1500: loss 5.097684\n",
      "iteration 1000 / 1500: loss 4.136403\n",
      "iteration 1100 / 1500: loss 5.208093\n",
      "iteration 1200 / 1500: loss 5.503487\n",
      "iteration 1300 / 1500: loss 5.438779\n",
      "iteration 1400 / 1500: loss 5.351126\n",
      "iteration 0 / 1500: loss 1383.316278\n",
      "iteration 100 / 1500: loss 4.875894\n",
      "iteration 200 / 1500: loss 4.277027\n",
      "iteration 300 / 1500: loss 4.326402\n",
      "iteration 400 / 1500: loss 6.141762\n",
      "iteration 500 / 1500: loss 5.047678\n",
      "iteration 600 / 1500: loss 4.548961\n",
      "iteration 700 / 1500: loss 4.647597\n",
      "iteration 800 / 1500: loss 5.487244\n",
      "iteration 900 / 1500: loss 5.136960\n",
      "iteration 1000 / 1500: loss 5.225919\n",
      "iteration 1100 / 1500: loss 5.637600\n",
      "iteration 1200 / 1500: loss 4.799389\n",
      "iteration 1300 / 1500: loss 4.910820\n",
      "iteration 1400 / 1500: loss 5.626218\n",
      "iteration 0 / 1500: loss 1434.872191\n",
      "iteration 100 / 1500: loss 4.335955\n",
      "iteration 200 / 1500: loss 5.158900\n",
      "iteration 300 / 1500: loss 4.911800\n",
      "iteration 400 / 1500: loss 6.472593\n",
      "iteration 500 / 1500: loss 6.534145\n",
      "iteration 600 / 1500: loss 6.282642\n",
      "iteration 700 / 1500: loss 7.016226\n",
      "iteration 800 / 1500: loss 5.704342\n",
      "iteration 900 / 1500: loss 5.681062\n",
      "iteration 1000 / 1500: loss 5.778997\n",
      "iteration 1100 / 1500: loss 5.433153\n",
      "iteration 1200 / 1500: loss 5.165443\n",
      "iteration 1300 / 1500: loss 6.422009\n",
      "iteration 1400 / 1500: loss 5.593328\n",
      "iteration 0 / 1500: loss 1466.815539\n",
      "iteration 100 / 1500: loss 4.966491\n",
      "iteration 200 / 1500: loss 4.414912\n",
      "iteration 300 / 1500: loss 4.760098\n",
      "iteration 400 / 1500: loss 6.262611\n",
      "iteration 500 / 1500: loss 6.607866\n",
      "iteration 600 / 1500: loss 5.194727\n",
      "iteration 700 / 1500: loss 4.806948\n",
      "iteration 800 / 1500: loss 5.263909\n",
      "iteration 900 / 1500: loss 6.104485\n",
      "iteration 1000 / 1500: loss 5.486935\n",
      "iteration 1100 / 1500: loss 4.082493\n",
      "iteration 1200 / 1500: loss 6.894670\n",
      "iteration 1300 / 1500: loss 5.932653\n",
      "iteration 1400 / 1500: loss 4.270914\n",
      "iteration 0 / 1500: loss 1497.381447\n",
      "iteration 100 / 1500: loss 6.209651\n",
      "iteration 200 / 1500: loss 5.429638\n",
      "iteration 300 / 1500: loss 6.049519\n",
      "iteration 400 / 1500: loss 6.146501\n",
      "iteration 500 / 1500: loss 5.262508\n",
      "iteration 600 / 1500: loss 4.719757\n",
      "iteration 700 / 1500: loss 4.501635\n",
      "iteration 800 / 1500: loss 4.274485\n",
      "iteration 900 / 1500: loss 5.665049\n",
      "iteration 1000 / 1500: loss 5.193098\n",
      "iteration 1100 / 1500: loss 5.703099\n",
      "iteration 1200 / 1500: loss 6.412029\n",
      "iteration 1300 / 1500: loss 5.209626\n",
      "iteration 1400 / 1500: loss 5.833063\n",
      "iteration 0 / 1500: loss 1519.009133\n",
      "iteration 100 / 1500: loss 6.340782\n",
      "iteration 200 / 1500: loss 4.232604\n",
      "iteration 300 / 1500: loss 6.240731\n",
      "iteration 400 / 1500: loss 4.221445\n",
      "iteration 500 / 1500: loss 4.725241\n",
      "iteration 600 / 1500: loss 4.870351\n",
      "iteration 700 / 1500: loss 5.255588\n",
      "iteration 800 / 1500: loss 4.881434\n",
      "iteration 900 / 1500: loss 6.069530\n",
      "iteration 1000 / 1500: loss 5.396823\n",
      "iteration 1100 / 1500: loss 5.129533\n",
      "iteration 1200 / 1500: loss 6.898578\n",
      "iteration 1300 / 1500: loss 5.438083\n",
      "iteration 1400 / 1500: loss 6.893349\n",
      "iteration 0 / 1500: loss 1553.116229\n",
      "iteration 100 / 1500: loss 5.449311\n",
      "iteration 200 / 1500: loss 4.518736\n",
      "iteration 300 / 1500: loss 4.644618\n",
      "iteration 400 / 1500: loss 4.133195\n",
      "iteration 500 / 1500: loss 4.778355\n",
      "iteration 600 / 1500: loss 4.990560\n",
      "iteration 700 / 1500: loss 4.743278\n",
      "iteration 800 / 1500: loss 5.865917\n",
      "iteration 900 / 1500: loss 3.954988\n",
      "iteration 1000 / 1500: loss 3.542959\n",
      "iteration 1100 / 1500: loss 4.059877\n",
      "iteration 1200 / 1500: loss 5.189356\n",
      "iteration 1300 / 1500: loss 5.545440\n",
      "iteration 1400 / 1500: loss 6.159528\n",
      "iteration 0 / 1500: loss 777.183141\n",
      "iteration 100 / 1500: loss 4.396037\n",
      "iteration 200 / 1500: loss 5.373337\n",
      "iteration 300 / 1500: loss 4.287097\n",
      "iteration 400 / 1500: loss 7.283097\n",
      "iteration 500 / 1500: loss 6.009835\n",
      "iteration 600 / 1500: loss 6.513014\n",
      "iteration 700 / 1500: loss 4.878685\n",
      "iteration 800 / 1500: loss 5.474581\n",
      "iteration 900 / 1500: loss 5.596750\n",
      "iteration 1000 / 1500: loss 4.106378\n",
      "iteration 1100 / 1500: loss 5.616778\n",
      "iteration 1200 / 1500: loss 4.967455\n",
      "iteration 1300 / 1500: loss 5.091411\n",
      "iteration 1400 / 1500: loss 5.199403\n",
      "iteration 0 / 1500: loss 808.739139\n",
      "iteration 100 / 1500: loss 4.627612\n",
      "iteration 200 / 1500: loss 6.463269\n",
      "iteration 300 / 1500: loss 3.884443\n",
      "iteration 400 / 1500: loss 4.848674\n",
      "iteration 500 / 1500: loss 5.807505\n",
      "iteration 600 / 1500: loss 5.447617\n",
      "iteration 700 / 1500: loss 5.247523\n",
      "iteration 800 / 1500: loss 4.831571\n",
      "iteration 900 / 1500: loss 4.004424\n",
      "iteration 1000 / 1500: loss 3.723242\n",
      "iteration 1100 / 1500: loss 5.061801\n",
      "iteration 1200 / 1500: loss 4.110374\n",
      "iteration 1300 / 1500: loss 3.651096\n",
      "iteration 1400 / 1500: loss 5.325131\n",
      "iteration 0 / 1500: loss 836.997832\n",
      "iteration 100 / 1500: loss 5.227424\n",
      "iteration 200 / 1500: loss 3.116669\n",
      "iteration 300 / 1500: loss 3.763865\n",
      "iteration 400 / 1500: loss 6.932201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500 / 1500: loss 5.994024\n",
      "iteration 600 / 1500: loss 4.465159\n",
      "iteration 700 / 1500: loss 5.326416\n",
      "iteration 800 / 1500: loss 4.964098\n",
      "iteration 900 / 1500: loss 5.435034\n",
      "iteration 1000 / 1500: loss 6.376848\n",
      "iteration 1100 / 1500: loss 5.943165\n",
      "iteration 1200 / 1500: loss 3.738527\n",
      "iteration 1300 / 1500: loss 5.601020\n",
      "iteration 1400 / 1500: loss 4.971902\n",
      "iteration 0 / 1500: loss 865.106399\n",
      "iteration 100 / 1500: loss 5.946605\n",
      "iteration 200 / 1500: loss 7.484224\n",
      "iteration 300 / 1500: loss 4.680725\n",
      "iteration 400 / 1500: loss 4.216912\n",
      "iteration 500 / 1500: loss 6.249196\n",
      "iteration 600 / 1500: loss 4.619069\n",
      "iteration 700 / 1500: loss 7.250055\n",
      "iteration 800 / 1500: loss 7.636176\n",
      "iteration 900 / 1500: loss 4.692094\n",
      "iteration 1000 / 1500: loss 5.402622\n",
      "iteration 1100 / 1500: loss 6.125013\n",
      "iteration 1200 / 1500: loss 4.207787\n",
      "iteration 1300 / 1500: loss 6.072922\n",
      "iteration 1400 / 1500: loss 5.982819\n",
      "iteration 0 / 1500: loss 900.774830\n",
      "iteration 100 / 1500: loss 5.547862\n",
      "iteration 200 / 1500: loss 6.332970\n",
      "iteration 300 / 1500: loss 4.734229\n",
      "iteration 400 / 1500: loss 6.877285\n",
      "iteration 500 / 1500: loss 4.817604\n",
      "iteration 600 / 1500: loss 5.375752\n",
      "iteration 700 / 1500: loss 6.718955\n",
      "iteration 800 / 1500: loss 6.925950\n",
      "iteration 900 / 1500: loss 5.424559\n",
      "iteration 1000 / 1500: loss 6.612773\n",
      "iteration 1100 / 1500: loss 7.102325\n",
      "iteration 1200 / 1500: loss 5.004160\n",
      "iteration 1300 / 1500: loss 4.238281\n",
      "iteration 1400 / 1500: loss 7.064337\n",
      "iteration 0 / 1500: loss 922.695413\n",
      "iteration 100 / 1500: loss 4.652550\n",
      "iteration 200 / 1500: loss 5.112025\n",
      "iteration 300 / 1500: loss 5.365569\n",
      "iteration 400 / 1500: loss 5.704570\n",
      "iteration 500 / 1500: loss 4.125026\n",
      "iteration 600 / 1500: loss 5.040406\n",
      "iteration 700 / 1500: loss 5.464136\n",
      "iteration 800 / 1500: loss 8.281682\n",
      "iteration 900 / 1500: loss 6.063071\n",
      "iteration 1000 / 1500: loss 5.616846\n",
      "iteration 1100 / 1500: loss 6.730615\n",
      "iteration 1200 / 1500: loss 4.548208\n",
      "iteration 1300 / 1500: loss 5.269557\n",
      "iteration 1400 / 1500: loss 5.033987\n",
      "iteration 0 / 1500: loss 948.994592\n",
      "iteration 100 / 1500: loss 6.781148\n",
      "iteration 200 / 1500: loss 6.918156\n",
      "iteration 300 / 1500: loss 7.024070\n",
      "iteration 400 / 1500: loss 3.874900\n",
      "iteration 500 / 1500: loss 4.520913\n",
      "iteration 600 / 1500: loss 4.114246\n",
      "iteration 700 / 1500: loss 4.527301\n",
      "iteration 800 / 1500: loss 7.562003\n",
      "iteration 900 / 1500: loss 6.536958\n",
      "iteration 1000 / 1500: loss 6.423811\n",
      "iteration 1100 / 1500: loss 5.961890\n",
      "iteration 1200 / 1500: loss 5.482920\n",
      "iteration 1300 / 1500: loss 5.288169\n",
      "iteration 1400 / 1500: loss 6.290246\n",
      "iteration 0 / 1500: loss 991.347043\n",
      "iteration 100 / 1500: loss 7.133741\n",
      "iteration 200 / 1500: loss 6.912220\n",
      "iteration 300 / 1500: loss 6.107841\n",
      "iteration 400 / 1500: loss 6.091847\n",
      "iteration 500 / 1500: loss 3.626737\n",
      "iteration 600 / 1500: loss 7.256851\n",
      "iteration 700 / 1500: loss 5.245620\n",
      "iteration 800 / 1500: loss 6.618023\n",
      "iteration 900 / 1500: loss 5.731558\n",
      "iteration 1000 / 1500: loss 6.348868\n",
      "iteration 1100 / 1500: loss 7.122980\n",
      "iteration 1200 / 1500: loss 6.364837\n",
      "iteration 1300 / 1500: loss 6.094057\n",
      "iteration 1400 / 1500: loss 4.514746\n",
      "iteration 0 / 1500: loss 1024.606904\n",
      "iteration 100 / 1500: loss 5.828098\n",
      "iteration 200 / 1500: loss 5.625784\n",
      "iteration 300 / 1500: loss 7.349947\n",
      "iteration 400 / 1500: loss 4.060783\n",
      "iteration 500 / 1500: loss 6.194272\n",
      "iteration 600 / 1500: loss 7.640384\n",
      "iteration 700 / 1500: loss 9.354524\n",
      "iteration 800 / 1500: loss 5.207173\n",
      "iteration 900 / 1500: loss 5.810033\n",
      "iteration 1000 / 1500: loss 6.944365\n",
      "iteration 1100 / 1500: loss 8.496774\n",
      "iteration 1200 / 1500: loss 6.234431\n",
      "iteration 1300 / 1500: loss 4.100666\n",
      "iteration 1400 / 1500: loss 7.014175\n",
      "iteration 0 / 1500: loss 1044.655127\n",
      "iteration 100 / 1500: loss 4.929810\n",
      "iteration 200 / 1500: loss 6.834347\n",
      "iteration 300 / 1500: loss 6.379816\n",
      "iteration 400 / 1500: loss 5.731647\n",
      "iteration 500 / 1500: loss 6.312311\n",
      "iteration 600 / 1500: loss 5.463033\n",
      "iteration 700 / 1500: loss 7.788899\n",
      "iteration 800 / 1500: loss 5.557893\n",
      "iteration 900 / 1500: loss 5.846179\n",
      "iteration 1000 / 1500: loss 7.260072\n",
      "iteration 1100 / 1500: loss 6.032337\n",
      "iteration 1200 / 1500: loss 6.304714\n",
      "iteration 1300 / 1500: loss 6.472484\n",
      "iteration 1400 / 1500: loss 6.595013\n",
      "iteration 0 / 1500: loss 1078.849367\n",
      "iteration 100 / 1500: loss 6.281308\n",
      "iteration 200 / 1500: loss 4.498845\n",
      "iteration 300 / 1500: loss 5.514383\n",
      "iteration 400 / 1500: loss 4.796474\n",
      "iteration 500 / 1500: loss 6.995205\n",
      "iteration 600 / 1500: loss 7.845281\n",
      "iteration 700 / 1500: loss 8.368361\n",
      "iteration 800 / 1500: loss 8.546936\n",
      "iteration 900 / 1500: loss 5.352548\n",
      "iteration 1000 / 1500: loss 8.772063\n",
      "iteration 1100 / 1500: loss 5.815310\n",
      "iteration 1200 / 1500: loss 6.437409\n",
      "iteration 1300 / 1500: loss 6.092230\n",
      "iteration 1400 / 1500: loss 8.827580\n",
      "iteration 0 / 1500: loss 1123.424027\n",
      "iteration 100 / 1500: loss 7.917446\n",
      "iteration 200 / 1500: loss 9.489337\n",
      "iteration 300 / 1500: loss 6.532074\n",
      "iteration 400 / 1500: loss 6.621429\n",
      "iteration 500 / 1500: loss 7.601330\n",
      "iteration 600 / 1500: loss 6.567765\n",
      "iteration 700 / 1500: loss 8.336856\n",
      "iteration 800 / 1500: loss 7.009064\n",
      "iteration 900 / 1500: loss 6.870806\n",
      "iteration 1000 / 1500: loss 6.491507\n",
      "iteration 1100 / 1500: loss 5.221446\n",
      "iteration 1200 / 1500: loss 5.429200\n",
      "iteration 1300 / 1500: loss 5.426169\n",
      "iteration 1400 / 1500: loss 6.166778\n",
      "iteration 0 / 1500: loss 1147.594519\n",
      "iteration 100 / 1500: loss 8.091420\n",
      "iteration 200 / 1500: loss 8.154367\n",
      "iteration 300 / 1500: loss 5.115416\n",
      "iteration 400 / 1500: loss 6.501654\n",
      "iteration 500 / 1500: loss 6.386980\n",
      "iteration 600 / 1500: loss 4.289128\n",
      "iteration 700 / 1500: loss 5.581383\n",
      "iteration 800 / 1500: loss 6.908961\n",
      "iteration 900 / 1500: loss 6.117988\n",
      "iteration 1000 / 1500: loss 5.305889\n",
      "iteration 1100 / 1500: loss 8.426766\n",
      "iteration 1200 / 1500: loss 7.878555\n",
      "iteration 1300 / 1500: loss 5.161805\n",
      "iteration 1400 / 1500: loss 6.570274\n",
      "iteration 0 / 1500: loss 1172.834786\n",
      "iteration 100 / 1500: loss 7.126027\n",
      "iteration 200 / 1500: loss 6.430546\n",
      "iteration 300 / 1500: loss 8.719379\n",
      "iteration 400 / 1500: loss 10.333638\n",
      "iteration 500 / 1500: loss 8.503433\n",
      "iteration 600 / 1500: loss 8.634300\n",
      "iteration 700 / 1500: loss 6.488521\n",
      "iteration 800 / 1500: loss 8.088243\n",
      "iteration 900 / 1500: loss 6.643460\n",
      "iteration 1000 / 1500: loss 9.122463\n",
      "iteration 1100 / 1500: loss 9.041244\n",
      "iteration 1200 / 1500: loss 5.718594\n",
      "iteration 1300 / 1500: loss 7.305491\n",
      "iteration 1400 / 1500: loss 8.713170\n",
      "iteration 0 / 1500: loss 1201.260375\n",
      "iteration 100 / 1500: loss 7.196361\n",
      "iteration 200 / 1500: loss 7.189245\n",
      "iteration 300 / 1500: loss 6.896129\n",
      "iteration 400 / 1500: loss 7.281554\n",
      "iteration 500 / 1500: loss 6.702566\n",
      "iteration 600 / 1500: loss 6.142452\n",
      "iteration 700 / 1500: loss 5.147762\n",
      "iteration 800 / 1500: loss 9.121564\n",
      "iteration 900 / 1500: loss 7.030579\n",
      "iteration 1000 / 1500: loss 8.839678\n",
      "iteration 1100 / 1500: loss 9.331031\n",
      "iteration 1200 / 1500: loss 4.820351\n",
      "iteration 1300 / 1500: loss 7.078092\n",
      "iteration 1400 / 1500: loss 7.960835\n",
      "iteration 0 / 1500: loss 1252.335714\n",
      "iteration 100 / 1500: loss 6.531992\n",
      "iteration 200 / 1500: loss 8.440425\n",
      "iteration 300 / 1500: loss 6.688863\n",
      "iteration 400 / 1500: loss 6.197108\n",
      "iteration 500 / 1500: loss 6.376344\n",
      "iteration 600 / 1500: loss 5.807840\n",
      "iteration 700 / 1500: loss 7.030427\n",
      "iteration 800 / 1500: loss 6.491075\n",
      "iteration 900 / 1500: loss 7.445548\n",
      "iteration 1000 / 1500: loss 6.875748\n",
      "iteration 1100 / 1500: loss 7.157530\n",
      "iteration 1200 / 1500: loss 6.151452\n",
      "iteration 1300 / 1500: loss 5.773310\n",
      "iteration 1400 / 1500: loss 5.784546\n",
      "iteration 0 / 1500: loss 1257.602683\n",
      "iteration 100 / 1500: loss 7.267436\n",
      "iteration 200 / 1500: loss 7.632699\n",
      "iteration 300 / 1500: loss 7.123430\n",
      "iteration 400 / 1500: loss 6.942845\n",
      "iteration 500 / 1500: loss 6.496554\n",
      "iteration 600 / 1500: loss 7.708639\n",
      "iteration 700 / 1500: loss 6.261817\n",
      "iteration 800 / 1500: loss 8.289033\n",
      "iteration 900 / 1500: loss 5.380304\n",
      "iteration 1000 / 1500: loss 5.970639\n",
      "iteration 1100 / 1500: loss 5.823104\n",
      "iteration 1200 / 1500: loss 7.247387\n",
      "iteration 1300 / 1500: loss 7.065622\n",
      "iteration 1400 / 1500: loss 7.331084\n",
      "iteration 0 / 1500: loss 1313.555176\n",
      "iteration 100 / 1500: loss 8.079870\n",
      "iteration 200 / 1500: loss 7.956686\n",
      "iteration 300 / 1500: loss 9.576254\n",
      "iteration 400 / 1500: loss 6.903921\n",
      "iteration 500 / 1500: loss 10.615996\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 1500: loss 6.221070\n",
      "iteration 700 / 1500: loss 10.177396\n",
      "iteration 800 / 1500: loss 8.670301\n",
      "iteration 900 / 1500: loss 6.769774\n",
      "iteration 1000 / 1500: loss 6.964026\n",
      "iteration 1100 / 1500: loss 8.882904\n",
      "iteration 1200 / 1500: loss 8.463757\n",
      "iteration 1300 / 1500: loss 5.418339\n",
      "iteration 1400 / 1500: loss 7.413853\n",
      "iteration 0 / 1500: loss 1319.999315\n",
      "iteration 100 / 1500: loss 8.351642\n",
      "iteration 200 / 1500: loss 8.966128\n",
      "iteration 300 / 1500: loss 7.627096\n",
      "iteration 400 / 1500: loss 7.783123\n",
      "iteration 500 / 1500: loss 7.013592\n",
      "iteration 600 / 1500: loss 7.532081\n",
      "iteration 700 / 1500: loss 7.854639\n",
      "iteration 800 / 1500: loss 5.573306\n",
      "iteration 900 / 1500: loss 7.599683\n",
      "iteration 1000 / 1500: loss 7.373760\n",
      "iteration 1100 / 1500: loss 7.019953\n",
      "iteration 1200 / 1500: loss 8.951466\n",
      "iteration 1300 / 1500: loss 7.397727\n",
      "iteration 1400 / 1500: loss 6.521404\n",
      "iteration 0 / 1500: loss 1361.604445\n",
      "iteration 100 / 1500: loss 6.221240\n",
      "iteration 200 / 1500: loss 9.649993\n",
      "iteration 300 / 1500: loss 7.565599\n",
      "iteration 400 / 1500: loss 7.384901\n",
      "iteration 500 / 1500: loss 7.364136\n",
      "iteration 600 / 1500: loss 6.995656\n",
      "iteration 700 / 1500: loss 7.924887\n",
      "iteration 800 / 1500: loss 7.877768\n",
      "iteration 900 / 1500: loss 8.217238\n",
      "iteration 1000 / 1500: loss 8.501997\n",
      "iteration 1100 / 1500: loss 9.406988\n",
      "iteration 1200 / 1500: loss 9.378396\n",
      "iteration 1300 / 1500: loss 9.652591\n",
      "iteration 1400 / 1500: loss 10.503785\n",
      "iteration 0 / 1500: loss 1403.036964\n",
      "iteration 100 / 1500: loss 8.645637\n",
      "iteration 200 / 1500: loss 7.771505\n",
      "iteration 300 / 1500: loss 6.542153\n",
      "iteration 400 / 1500: loss 10.611967\n",
      "iteration 500 / 1500: loss 8.465476\n",
      "iteration 600 / 1500: loss 8.425456\n",
      "iteration 700 / 1500: loss 8.969365\n",
      "iteration 800 / 1500: loss 8.804378\n",
      "iteration 900 / 1500: loss 7.379291\n",
      "iteration 1000 / 1500: loss 7.735627\n",
      "iteration 1100 / 1500: loss 9.076700\n",
      "iteration 1200 / 1500: loss 8.660999\n",
      "iteration 1300 / 1500: loss 7.758919\n",
      "iteration 1400 / 1500: loss 11.053526\n",
      "iteration 0 / 1500: loss 1432.685558\n",
      "iteration 100 / 1500: loss 9.787087\n",
      "iteration 200 / 1500: loss 6.759061\n",
      "iteration 300 / 1500: loss 8.394451\n",
      "iteration 400 / 1500: loss 9.476736\n",
      "iteration 500 / 1500: loss 7.606540\n",
      "iteration 600 / 1500: loss 8.263564\n",
      "iteration 700 / 1500: loss 8.966969\n",
      "iteration 800 / 1500: loss 8.271284\n",
      "iteration 900 / 1500: loss 7.640159\n",
      "iteration 1000 / 1500: loss 9.257079\n",
      "iteration 1100 / 1500: loss 9.287129\n",
      "iteration 1200 / 1500: loss 7.404445\n",
      "iteration 1300 / 1500: loss 7.456085\n",
      "iteration 1400 / 1500: loss 8.263200\n",
      "iteration 0 / 1500: loss 1445.637983\n",
      "iteration 100 / 1500: loss 6.942241\n",
      "iteration 200 / 1500: loss 8.846883\n",
      "iteration 300 / 1500: loss 9.332318\n",
      "iteration 400 / 1500: loss 6.958012\n",
      "iteration 500 / 1500: loss 7.215242\n",
      "iteration 600 / 1500: loss 8.695957\n",
      "iteration 700 / 1500: loss 7.364878\n",
      "iteration 800 / 1500: loss 8.928324\n",
      "iteration 900 / 1500: loss 7.131307\n",
      "iteration 1000 / 1500: loss 8.161895\n",
      "iteration 1100 / 1500: loss 8.383651\n",
      "iteration 1200 / 1500: loss 7.465651\n",
      "iteration 1300 / 1500: loss 5.945066\n",
      "iteration 1400 / 1500: loss 9.133695\n",
      "iteration 0 / 1500: loss 1485.199208\n",
      "iteration 100 / 1500: loss 7.913424\n",
      "iteration 200 / 1500: loss 8.731122\n",
      "iteration 300 / 1500: loss 9.349164\n",
      "iteration 400 / 1500: loss 8.901260\n",
      "iteration 500 / 1500: loss 8.140606\n",
      "iteration 600 / 1500: loss 8.171364\n",
      "iteration 700 / 1500: loss 8.470479\n",
      "iteration 800 / 1500: loss 7.470428\n",
      "iteration 900 / 1500: loss 8.974422\n",
      "iteration 1000 / 1500: loss 9.525549\n",
      "iteration 1100 / 1500: loss 9.433376\n",
      "iteration 1200 / 1500: loss 7.566698\n",
      "iteration 1300 / 1500: loss 10.305568\n",
      "iteration 1400 / 1500: loss 7.745712\n",
      "iteration 0 / 1500: loss 1520.906188\n",
      "iteration 100 / 1500: loss 9.529088\n",
      "iteration 200 / 1500: loss 7.991502\n",
      "iteration 300 / 1500: loss 9.357913\n",
      "iteration 400 / 1500: loss 7.745534\n",
      "iteration 500 / 1500: loss 9.447232\n",
      "iteration 600 / 1500: loss 11.499789\n",
      "iteration 700 / 1500: loss 7.306564\n",
      "iteration 800 / 1500: loss 7.509813\n",
      "iteration 900 / 1500: loss 10.069526\n",
      "iteration 1000 / 1500: loss 10.897514\n",
      "iteration 1100 / 1500: loss 6.634565\n",
      "iteration 1200 / 1500: loss 8.186025\n",
      "iteration 1300 / 1500: loss 8.138295\n",
      "iteration 1400 / 1500: loss 8.400840\n",
      "iteration 0 / 1500: loss 1516.788584\n",
      "iteration 100 / 1500: loss 11.983199\n",
      "iteration 200 / 1500: loss 9.980424\n",
      "iteration 300 / 1500: loss 10.014057\n",
      "iteration 400 / 1500: loss 10.037309\n",
      "iteration 500 / 1500: loss 10.139684\n",
      "iteration 600 / 1500: loss 9.650548\n",
      "iteration 700 / 1500: loss 9.542183\n",
      "iteration 800 / 1500: loss 9.850171\n",
      "iteration 900 / 1500: loss 9.504598\n",
      "iteration 1000 / 1500: loss 8.246487\n",
      "iteration 1100 / 1500: loss 8.507161\n",
      "iteration 1200 / 1500: loss 10.059863\n",
      "iteration 1300 / 1500: loss 9.357511\n",
      "iteration 1400 / 1500: loss 8.856372\n",
      "iteration 0 / 1500: loss 776.397657\n",
      "iteration 100 / 1500: loss 7.832154\n",
      "iteration 200 / 1500: loss 7.034774\n",
      "iteration 300 / 1500: loss 8.364437\n",
      "iteration 400 / 1500: loss 5.896444\n",
      "iteration 500 / 1500: loss 6.741604\n",
      "iteration 600 / 1500: loss 6.810391\n",
      "iteration 700 / 1500: loss 8.463120\n",
      "iteration 800 / 1500: loss 5.913948\n",
      "iteration 900 / 1500: loss 7.671583\n",
      "iteration 1000 / 1500: loss 6.450112\n",
      "iteration 1100 / 1500: loss 9.743878\n",
      "iteration 1200 / 1500: loss 7.742671\n",
      "iteration 1300 / 1500: loss 9.042541\n",
      "iteration 1400 / 1500: loss 9.069502\n",
      "iteration 0 / 1500: loss 796.951183\n",
      "iteration 100 / 1500: loss 9.247129\n",
      "iteration 200 / 1500: loss 7.371015\n",
      "iteration 300 / 1500: loss 6.002195\n",
      "iteration 400 / 1500: loss 5.510373\n",
      "iteration 500 / 1500: loss 9.708113\n",
      "iteration 600 / 1500: loss 9.566295\n",
      "iteration 700 / 1500: loss 7.374881\n",
      "iteration 800 / 1500: loss 8.820536\n",
      "iteration 900 / 1500: loss 5.615504\n",
      "iteration 1000 / 1500: loss 6.050256\n",
      "iteration 1100 / 1500: loss 7.706602\n",
      "iteration 1200 / 1500: loss 9.085585\n",
      "iteration 1300 / 1500: loss 8.728830\n",
      "iteration 1400 / 1500: loss 7.930481\n",
      "iteration 0 / 1500: loss 840.800070\n",
      "iteration 100 / 1500: loss 8.235873\n",
      "iteration 200 / 1500: loss 9.309363\n",
      "iteration 300 / 1500: loss 6.716451\n",
      "iteration 400 / 1500: loss 9.888094\n",
      "iteration 500 / 1500: loss 8.455967\n",
      "iteration 600 / 1500: loss 6.921859\n",
      "iteration 700 / 1500: loss 10.296524\n",
      "iteration 800 / 1500: loss 7.761116\n",
      "iteration 900 / 1500: loss 7.573231\n",
      "iteration 1000 / 1500: loss 9.284420\n",
      "iteration 1100 / 1500: loss 7.040614\n",
      "iteration 1200 / 1500: loss 9.076780\n",
      "iteration 1300 / 1500: loss 7.348669\n",
      "iteration 1400 / 1500: loss 8.829431\n",
      "iteration 0 / 1500: loss 868.804362\n",
      "iteration 100 / 1500: loss 9.023026\n",
      "iteration 200 / 1500: loss 9.397211\n",
      "iteration 300 / 1500: loss 6.934370\n",
      "iteration 400 / 1500: loss 8.436972\n",
      "iteration 500 / 1500: loss 6.220756\n",
      "iteration 600 / 1500: loss 10.941752\n",
      "iteration 700 / 1500: loss 9.528635\n",
      "iteration 800 / 1500: loss 10.639203\n",
      "iteration 900 / 1500: loss 9.488990\n",
      "iteration 1000 / 1500: loss 11.025150\n",
      "iteration 1100 / 1500: loss 8.949113\n",
      "iteration 1200 / 1500: loss 9.714422\n",
      "iteration 1300 / 1500: loss 7.618944\n",
      "iteration 1400 / 1500: loss 9.826291\n",
      "iteration 0 / 1500: loss 899.424437\n",
      "iteration 100 / 1500: loss 9.021105\n",
      "iteration 200 / 1500: loss 9.285302\n",
      "iteration 300 / 1500: loss 7.005251\n",
      "iteration 400 / 1500: loss 7.024537\n",
      "iteration 500 / 1500: loss 6.716499\n",
      "iteration 600 / 1500: loss 5.939764\n",
      "iteration 700 / 1500: loss 8.211559\n",
      "iteration 800 / 1500: loss 7.966542\n",
      "iteration 900 / 1500: loss 10.017303\n",
      "iteration 1000 / 1500: loss 9.480055\n",
      "iteration 1100 / 1500: loss 8.200006\n",
      "iteration 1200 / 1500: loss 8.969811\n",
      "iteration 1300 / 1500: loss 10.308291\n",
      "iteration 1400 / 1500: loss 10.221634\n",
      "iteration 0 / 1500: loss 939.524324\n",
      "iteration 100 / 1500: loss 8.044463\n",
      "iteration 200 / 1500: loss 8.933626\n",
      "iteration 300 / 1500: loss 10.808765\n",
      "iteration 400 / 1500: loss 13.351901\n",
      "iteration 500 / 1500: loss 9.112987\n",
      "iteration 600 / 1500: loss 9.065129\n",
      "iteration 700 / 1500: loss 11.293519\n",
      "iteration 800 / 1500: loss 11.417560\n",
      "iteration 900 / 1500: loss 8.797679\n",
      "iteration 1000 / 1500: loss 9.265256\n",
      "iteration 1100 / 1500: loss 7.495809\n",
      "iteration 1200 / 1500: loss 6.586344\n",
      "iteration 1300 / 1500: loss 11.489359\n",
      "iteration 1400 / 1500: loss 7.814426\n",
      "iteration 0 / 1500: loss 961.446809\n",
      "iteration 100 / 1500: loss 8.028943\n",
      "iteration 200 / 1500: loss 9.351128\n",
      "iteration 300 / 1500: loss 8.659160\n",
      "iteration 400 / 1500: loss 9.125850\n",
      "iteration 500 / 1500: loss 9.136932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 1500: loss 8.304792\n",
      "iteration 700 / 1500: loss 7.531351\n",
      "iteration 800 / 1500: loss 11.317433\n",
      "iteration 900 / 1500: loss 7.670591\n",
      "iteration 1000 / 1500: loss 6.555700\n",
      "iteration 1100 / 1500: loss 8.661330\n",
      "iteration 1200 / 1500: loss 7.844564\n",
      "iteration 1300 / 1500: loss 8.937265\n",
      "iteration 1400 / 1500: loss 9.338997\n",
      "iteration 0 / 1500: loss 987.931346\n",
      "iteration 100 / 1500: loss 9.404877\n",
      "iteration 200 / 1500: loss 10.005758\n",
      "iteration 300 / 1500: loss 9.640578\n",
      "iteration 400 / 1500: loss 7.951825\n",
      "iteration 500 / 1500: loss 10.547263\n",
      "iteration 600 / 1500: loss 8.855363\n",
      "iteration 700 / 1500: loss 11.219637\n",
      "iteration 800 / 1500: loss 8.281104\n",
      "iteration 900 / 1500: loss 9.681128\n",
      "iteration 1000 / 1500: loss 6.675738\n",
      "iteration 1100 / 1500: loss 10.421041\n",
      "iteration 1200 / 1500: loss 8.937256\n",
      "iteration 1300 / 1500: loss 9.842361\n",
      "iteration 1400 / 1500: loss 11.699415\n",
      "iteration 0 / 1500: loss 1013.673243\n",
      "iteration 100 / 1500: loss 9.999855\n",
      "iteration 200 / 1500: loss 8.178770\n",
      "iteration 300 / 1500: loss 8.511805\n",
      "iteration 400 / 1500: loss 10.434095\n",
      "iteration 500 / 1500: loss 9.356175\n",
      "iteration 600 / 1500: loss 12.029072\n",
      "iteration 700 / 1500: loss 12.781725\n",
      "iteration 800 / 1500: loss 9.791439\n",
      "iteration 900 / 1500: loss 12.467016\n",
      "iteration 1000 / 1500: loss 10.827600\n",
      "iteration 1100 / 1500: loss 9.114499\n",
      "iteration 1200 / 1500: loss 8.789593\n",
      "iteration 1300 / 1500: loss 10.468648\n",
      "iteration 1400 / 1500: loss 11.000814\n",
      "iteration 0 / 1500: loss 1048.594329\n",
      "iteration 100 / 1500: loss 10.819010\n",
      "iteration 200 / 1500: loss 11.660652\n",
      "iteration 300 / 1500: loss 11.447418\n",
      "iteration 400 / 1500: loss 10.647671\n",
      "iteration 500 / 1500: loss 12.994457\n",
      "iteration 600 / 1500: loss 12.541694\n",
      "iteration 700 / 1500: loss 9.733012\n",
      "iteration 800 / 1500: loss 10.842525\n",
      "iteration 900 / 1500: loss 11.536768\n",
      "iteration 1000 / 1500: loss 9.714125\n",
      "iteration 1100 / 1500: loss 11.263640\n",
      "iteration 1200 / 1500: loss 10.376761\n",
      "iteration 1300 / 1500: loss 10.498111\n",
      "iteration 1400 / 1500: loss 11.894819\n",
      "iteration 0 / 1500: loss 1089.167210\n",
      "iteration 100 / 1500: loss 8.645366\n",
      "iteration 200 / 1500: loss 12.873485\n",
      "iteration 300 / 1500: loss 9.779813\n",
      "iteration 400 / 1500: loss 8.156928\n",
      "iteration 500 / 1500: loss 11.207626\n",
      "iteration 600 / 1500: loss 12.205392\n",
      "iteration 700 / 1500: loss 10.321735\n",
      "iteration 800 / 1500: loss 8.907410\n",
      "iteration 900 / 1500: loss 10.352578\n",
      "iteration 1000 / 1500: loss 10.871183\n",
      "iteration 1100 / 1500: loss 10.690150\n",
      "iteration 1200 / 1500: loss 9.924088\n",
      "iteration 1300 / 1500: loss 11.255656\n",
      "iteration 1400 / 1500: loss 9.806127\n",
      "iteration 0 / 1500: loss 1116.734374\n",
      "iteration 100 / 1500: loss 9.812835\n",
      "iteration 200 / 1500: loss 6.277658\n",
      "iteration 300 / 1500: loss 8.647730\n",
      "iteration 400 / 1500: loss 10.851396\n",
      "iteration 500 / 1500: loss 10.901899\n",
      "iteration 600 / 1500: loss 10.135557\n",
      "iteration 700 / 1500: loss 9.640776\n",
      "iteration 800 / 1500: loss 8.902829\n",
      "iteration 900 / 1500: loss 10.837902\n",
      "iteration 1000 / 1500: loss 10.601630\n",
      "iteration 1100 / 1500: loss 9.559782\n",
      "iteration 1200 / 1500: loss 11.683690\n",
      "iteration 1300 / 1500: loss 9.310592\n",
      "iteration 1400 / 1500: loss 9.331227\n",
      "iteration 0 / 1500: loss 1136.801135\n",
      "iteration 100 / 1500: loss 12.298824\n",
      "iteration 200 / 1500: loss 10.245309\n",
      "iteration 300 / 1500: loss 11.301129\n",
      "iteration 400 / 1500: loss 12.707484\n",
      "iteration 500 / 1500: loss 11.430637\n",
      "iteration 600 / 1500: loss 12.087779\n",
      "iteration 700 / 1500: loss 11.598429\n",
      "iteration 800 / 1500: loss 9.684130\n",
      "iteration 900 / 1500: loss 8.505399\n",
      "iteration 1000 / 1500: loss 11.100548\n",
      "iteration 1100 / 1500: loss 9.479793\n",
      "iteration 1200 / 1500: loss 11.119237\n",
      "iteration 1300 / 1500: loss 15.606147\n",
      "iteration 1400 / 1500: loss 10.526901\n",
      "iteration 0 / 1500: loss 1162.094651\n",
      "iteration 100 / 1500: loss 11.471020\n",
      "iteration 200 / 1500: loss 10.410766\n",
      "iteration 300 / 1500: loss 9.021209\n",
      "iteration 400 / 1500: loss 12.330419\n",
      "iteration 500 / 1500: loss 11.067456\n",
      "iteration 600 / 1500: loss 11.509616\n",
      "iteration 700 / 1500: loss 8.903839\n",
      "iteration 800 / 1500: loss 11.319558\n",
      "iteration 900 / 1500: loss 10.053441\n",
      "iteration 1000 / 1500: loss 10.537395\n",
      "iteration 1100 / 1500: loss 10.668890\n",
      "iteration 1200 / 1500: loss 12.894948\n",
      "iteration 1300 / 1500: loss 10.931642\n",
      "iteration 1400 / 1500: loss 12.340124\n",
      "iteration 0 / 1500: loss 1212.164941\n",
      "iteration 100 / 1500: loss 10.733944\n",
      "iteration 200 / 1500: loss 10.858853\n",
      "iteration 300 / 1500: loss 10.507652\n",
      "iteration 400 / 1500: loss 11.016767\n",
      "iteration 500 / 1500: loss 9.954604\n",
      "iteration 600 / 1500: loss 10.729289\n",
      "iteration 700 / 1500: loss 14.326011\n",
      "iteration 800 / 1500: loss 12.523473\n",
      "iteration 900 / 1500: loss 11.038974\n",
      "iteration 1000 / 1500: loss 13.628995\n",
      "iteration 1100 / 1500: loss 12.497716\n",
      "iteration 1200 / 1500: loss 10.513054\n",
      "iteration 1300 / 1500: loss 13.384627\n",
      "iteration 1400 / 1500: loss 12.337412\n",
      "iteration 0 / 1500: loss 1249.408405\n",
      "iteration 100 / 1500: loss 9.937519\n",
      "iteration 200 / 1500: loss 13.803655\n",
      "iteration 300 / 1500: loss 13.368728\n",
      "iteration 400 / 1500: loss 13.713137\n",
      "iteration 500 / 1500: loss 12.901050\n",
      "iteration 600 / 1500: loss 12.420726\n",
      "iteration 700 / 1500: loss 11.233129\n",
      "iteration 800 / 1500: loss 11.488812\n",
      "iteration 900 / 1500: loss 12.193878\n",
      "iteration 1000 / 1500: loss 12.662247\n",
      "iteration 1100 / 1500: loss 11.244565\n",
      "iteration 1200 / 1500: loss 13.260175\n",
      "iteration 1300 / 1500: loss 12.657307\n",
      "iteration 1400 / 1500: loss 9.572490\n",
      "iteration 0 / 1500: loss 1264.937731\n",
      "iteration 100 / 1500: loss 10.058920\n",
      "iteration 200 / 1500: loss 9.718388\n",
      "iteration 300 / 1500: loss 10.791684\n",
      "iteration 400 / 1500: loss 12.497838\n",
      "iteration 500 / 1500: loss 10.981982\n",
      "iteration 600 / 1500: loss 12.204268\n",
      "iteration 700 / 1500: loss 12.077887\n",
      "iteration 800 / 1500: loss 12.215213\n",
      "iteration 900 / 1500: loss 10.238500\n",
      "iteration 1000 / 1500: loss 9.106384\n",
      "iteration 1100 / 1500: loss 11.942199\n",
      "iteration 1200 / 1500: loss 10.838939\n",
      "iteration 1300 / 1500: loss 10.420730\n",
      "iteration 1400 / 1500: loss 10.168298\n",
      "iteration 0 / 1500: loss 1263.878868\n",
      "iteration 100 / 1500: loss 13.783980\n",
      "iteration 200 / 1500: loss 14.673353\n",
      "iteration 300 / 1500: loss 14.105004\n",
      "iteration 400 / 1500: loss 12.686291\n",
      "iteration 500 / 1500: loss 12.445027\n",
      "iteration 600 / 1500: loss 14.942194\n",
      "iteration 700 / 1500: loss 15.123761\n",
      "iteration 800 / 1500: loss 12.897096\n",
      "iteration 900 / 1500: loss 13.229272\n",
      "iteration 1000 / 1500: loss 15.733971\n",
      "iteration 1100 / 1500: loss 13.270891\n",
      "iteration 1200 / 1500: loss 12.231465\n",
      "iteration 1300 / 1500: loss 14.491885\n",
      "iteration 1400 / 1500: loss 11.340338\n",
      "iteration 0 / 1500: loss 1313.700611\n",
      "iteration 100 / 1500: loss 13.291544\n",
      "iteration 200 / 1500: loss 13.636383\n",
      "iteration 300 / 1500: loss 11.886601\n",
      "iteration 400 / 1500: loss 11.136555\n",
      "iteration 500 / 1500: loss 13.509615\n",
      "iteration 600 / 1500: loss 11.668778\n",
      "iteration 700 / 1500: loss 12.538870\n",
      "iteration 800 / 1500: loss 13.270971\n",
      "iteration 900 / 1500: loss 12.502867\n",
      "iteration 1000 / 1500: loss 10.971594\n",
      "iteration 1100 / 1500: loss 14.796122\n",
      "iteration 1200 / 1500: loss 9.307707\n",
      "iteration 1300 / 1500: loss 10.569117\n",
      "iteration 1400 / 1500: loss 13.347833\n",
      "iteration 0 / 1500: loss 1365.922219\n",
      "iteration 100 / 1500: loss 15.012846\n",
      "iteration 200 / 1500: loss 15.172313\n",
      "iteration 300 / 1500: loss 16.373523\n",
      "iteration 400 / 1500: loss 12.138874\n",
      "iteration 500 / 1500: loss 14.960807\n",
      "iteration 600 / 1500: loss 12.344704\n",
      "iteration 700 / 1500: loss 13.265348\n",
      "iteration 800 / 1500: loss 14.450971\n",
      "iteration 900 / 1500: loss 12.826405\n",
      "iteration 1000 / 1500: loss 12.871001\n",
      "iteration 1100 / 1500: loss 13.034372\n",
      "iteration 1200 / 1500: loss 14.217357\n",
      "iteration 1300 / 1500: loss 12.998655\n",
      "iteration 1400 / 1500: loss 15.637652\n",
      "iteration 0 / 1500: loss 1397.069745\n",
      "iteration 100 / 1500: loss 12.471727\n",
      "iteration 200 / 1500: loss 12.642935\n",
      "iteration 300 / 1500: loss 11.941088\n",
      "iteration 400 / 1500: loss 12.620714\n",
      "iteration 500 / 1500: loss 13.050683\n",
      "iteration 600 / 1500: loss 14.029772\n",
      "iteration 700 / 1500: loss 15.704875\n",
      "iteration 800 / 1500: loss 14.447453\n",
      "iteration 900 / 1500: loss 12.606180\n",
      "iteration 1000 / 1500: loss 15.027130\n",
      "iteration 1100 / 1500: loss 13.571896\n",
      "iteration 1200 / 1500: loss 12.865603\n",
      "iteration 1300 / 1500: loss 10.724377\n",
      "iteration 1400 / 1500: loss 11.288344\n",
      "iteration 0 / 1500: loss 1417.102976\n",
      "iteration 100 / 1500: loss 13.404302\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 1500: loss 15.248594\n",
      "iteration 300 / 1500: loss 15.379506\n",
      "iteration 400 / 1500: loss 14.642336\n",
      "iteration 500 / 1500: loss 13.668563\n",
      "iteration 600 / 1500: loss 13.586755\n",
      "iteration 700 / 1500: loss 13.430776\n",
      "iteration 800 / 1500: loss 15.465975\n",
      "iteration 900 / 1500: loss 15.965912\n",
      "iteration 1000 / 1500: loss 14.044224\n",
      "iteration 1100 / 1500: loss 14.826423\n",
      "iteration 1200 / 1500: loss 16.615228\n",
      "iteration 1300 / 1500: loss 12.595079\n",
      "iteration 1400 / 1500: loss 15.630924\n",
      "iteration 0 / 1500: loss 1458.843967\n",
      "iteration 100 / 1500: loss 15.646468\n",
      "iteration 200 / 1500: loss 15.576071\n",
      "iteration 300 / 1500: loss 14.495135\n",
      "iteration 400 / 1500: loss 12.975069\n",
      "iteration 500 / 1500: loss 16.747411\n",
      "iteration 600 / 1500: loss 15.126865\n",
      "iteration 700 / 1500: loss 12.956282\n",
      "iteration 800 / 1500: loss 12.697646\n",
      "iteration 900 / 1500: loss 12.492166\n",
      "iteration 1000 / 1500: loss 14.371404\n",
      "iteration 1100 / 1500: loss 12.789323\n",
      "iteration 1200 / 1500: loss 16.625179\n",
      "iteration 1300 / 1500: loss 13.614717\n",
      "iteration 1400 / 1500: loss 12.630088\n",
      "iteration 0 / 1500: loss 1479.478419\n",
      "iteration 100 / 1500: loss 14.930109\n",
      "iteration 200 / 1500: loss 15.183172\n",
      "iteration 300 / 1500: loss 15.493821\n",
      "iteration 400 / 1500: loss 13.968879\n",
      "iteration 500 / 1500: loss 16.835828\n",
      "iteration 600 / 1500: loss 16.396816\n",
      "iteration 700 / 1500: loss 15.469100\n",
      "iteration 800 / 1500: loss 17.262481\n",
      "iteration 900 / 1500: loss 17.059417\n",
      "iteration 1000 / 1500: loss 14.119669\n",
      "iteration 1100 / 1500: loss 15.266869\n",
      "iteration 1200 / 1500: loss 16.177531\n",
      "iteration 1300 / 1500: loss 14.394226\n",
      "iteration 1400 / 1500: loss 19.496534\n",
      "iteration 0 / 1500: loss 1482.398113\n",
      "iteration 100 / 1500: loss 15.481892\n",
      "iteration 200 / 1500: loss 15.388200\n",
      "iteration 300 / 1500: loss 18.493566\n",
      "iteration 400 / 1500: loss 15.997567\n",
      "iteration 500 / 1500: loss 16.285964\n",
      "iteration 600 / 1500: loss 16.418603\n",
      "iteration 700 / 1500: loss 16.561101\n",
      "iteration 800 / 1500: loss 15.929851\n",
      "iteration 900 / 1500: loss 16.416960\n",
      "iteration 1000 / 1500: loss 17.582990\n",
      "iteration 1100 / 1500: loss 15.005680\n",
      "iteration 1200 / 1500: loss 17.429334\n",
      "iteration 1300 / 1500: loss 16.002626\n",
      "iteration 1400 / 1500: loss 16.382748\n",
      "iteration 0 / 1500: loss 1521.475234\n",
      "iteration 100 / 1500: loss 16.631017\n",
      "iteration 200 / 1500: loss 14.163585\n",
      "iteration 300 / 1500: loss 17.895895\n",
      "iteration 400 / 1500: loss 17.958374\n",
      "iteration 500 / 1500: loss 16.925671\n",
      "iteration 600 / 1500: loss 17.948599\n",
      "iteration 700 / 1500: loss 16.069857\n",
      "iteration 800 / 1500: loss 17.082758\n",
      "iteration 900 / 1500: loss 13.957811\n",
      "iteration 1000 / 1500: loss 16.674281\n",
      "iteration 1100 / 1500: loss 16.469290\n",
      "iteration 1200 / 1500: loss 18.750923\n",
      "iteration 1300 / 1500: loss 17.455042\n",
      "iteration 1400 / 1500: loss 15.535912\n",
      "lr 1.000000e-07 reg 2.500000e+04 train accuracy: 0.320551 val accuracy: 0.331000\n",
      "lr 1.000000e-07 reg 2.600000e+04 train accuracy: 0.313102 val accuracy: 0.325000\n",
      "lr 1.000000e-07 reg 2.700000e+04 train accuracy: 0.318020 val accuracy: 0.329000\n",
      "lr 1.000000e-07 reg 2.800000e+04 train accuracy: 0.319041 val accuracy: 0.325000\n",
      "lr 1.000000e-07 reg 2.900000e+04 train accuracy: 0.314939 val accuracy: 0.330000\n",
      "lr 1.000000e-07 reg 3.000000e+04 train accuracy: 0.317306 val accuracy: 0.327000\n",
      "lr 1.000000e-07 reg 3.100000e+04 train accuracy: 0.309939 val accuracy: 0.318000\n",
      "lr 1.000000e-07 reg 3.200000e+04 train accuracy: 0.313898 val accuracy: 0.321000\n",
      "lr 1.000000e-07 reg 3.300000e+04 train accuracy: 0.314265 val accuracy: 0.320000\n",
      "lr 1.000000e-07 reg 3.400000e+04 train accuracy: 0.311367 val accuracy: 0.324000\n",
      "lr 1.000000e-07 reg 3.500000e+04 train accuracy: 0.309082 val accuracy: 0.321000\n",
      "lr 1.000000e-07 reg 3.600000e+04 train accuracy: 0.311245 val accuracy: 0.313000\n",
      "lr 1.000000e-07 reg 3.700000e+04 train accuracy: 0.310000 val accuracy: 0.312000\n",
      "lr 1.000000e-07 reg 3.800000e+04 train accuracy: 0.307694 val accuracy: 0.321000\n",
      "lr 1.000000e-07 reg 3.900000e+04 train accuracy: 0.308531 val accuracy: 0.311000\n",
      "lr 1.000000e-07 reg 4.000000e+04 train accuracy: 0.308347 val accuracy: 0.315000\n",
      "lr 1.000000e-07 reg 4.100000e+04 train accuracy: 0.305857 val accuracy: 0.316000\n",
      "lr 1.000000e-07 reg 4.200000e+04 train accuracy: 0.303020 val accuracy: 0.314000\n",
      "lr 1.000000e-07 reg 4.300000e+04 train accuracy: 0.299122 val accuracy: 0.309000\n",
      "lr 1.000000e-07 reg 4.400000e+04 train accuracy: 0.305959 val accuracy: 0.309000\n",
      "lr 1.000000e-07 reg 4.500000e+04 train accuracy: 0.305837 val accuracy: 0.315000\n",
      "lr 1.000000e-07 reg 4.600000e+04 train accuracy: 0.301286 val accuracy: 0.312000\n",
      "lr 1.000000e-07 reg 4.700000e+04 train accuracy: 0.300449 val accuracy: 0.305000\n",
      "lr 1.000000e-07 reg 4.800000e+04 train accuracy: 0.300000 val accuracy: 0.305000\n",
      "lr 1.000000e-07 reg 4.900000e+04 train accuracy: 0.301286 val accuracy: 0.307000\n",
      "lr 1.000000e-07 reg 5.000000e+04 train accuracy: 0.296551 val accuracy: 0.310000\n",
      "lr 1.258925e-07 reg 2.500000e+04 train accuracy: 0.320204 val accuracy: 0.320000\n",
      "lr 1.258925e-07 reg 2.600000e+04 train accuracy: 0.316286 val accuracy: 0.328000\n",
      "lr 1.258925e-07 reg 2.700000e+04 train accuracy: 0.317449 val accuracy: 0.332000\n",
      "lr 1.258925e-07 reg 2.800000e+04 train accuracy: 0.316653 val accuracy: 0.327000\n",
      "lr 1.258925e-07 reg 2.900000e+04 train accuracy: 0.321551 val accuracy: 0.332000\n",
      "lr 1.258925e-07 reg 3.000000e+04 train accuracy: 0.314633 val accuracy: 0.317000\n",
      "lr 1.258925e-07 reg 3.100000e+04 train accuracy: 0.309857 val accuracy: 0.319000\n",
      "lr 1.258925e-07 reg 3.200000e+04 train accuracy: 0.314531 val accuracy: 0.322000\n",
      "lr 1.258925e-07 reg 3.300000e+04 train accuracy: 0.311184 val accuracy: 0.321000\n",
      "lr 1.258925e-07 reg 3.400000e+04 train accuracy: 0.311449 val accuracy: 0.318000\n",
      "lr 1.258925e-07 reg 3.500000e+04 train accuracy: 0.311367 val accuracy: 0.320000\n",
      "lr 1.258925e-07 reg 3.600000e+04 train accuracy: 0.309224 val accuracy: 0.319000\n",
      "lr 1.258925e-07 reg 3.700000e+04 train accuracy: 0.310020 val accuracy: 0.313000\n",
      "lr 1.258925e-07 reg 3.800000e+04 train accuracy: 0.309347 val accuracy: 0.316000\n",
      "lr 1.258925e-07 reg 3.900000e+04 train accuracy: 0.308939 val accuracy: 0.309000\n",
      "lr 1.258925e-07 reg 4.000000e+04 train accuracy: 0.310694 val accuracy: 0.314000\n",
      "lr 1.258925e-07 reg 4.100000e+04 train accuracy: 0.303796 val accuracy: 0.311000\n",
      "lr 1.258925e-07 reg 4.200000e+04 train accuracy: 0.305898 val accuracy: 0.317000\n",
      "lr 1.258925e-07 reg 4.300000e+04 train accuracy: 0.301653 val accuracy: 0.316000\n",
      "lr 1.258925e-07 reg 4.400000e+04 train accuracy: 0.305694 val accuracy: 0.309000\n",
      "lr 1.258925e-07 reg 4.500000e+04 train accuracy: 0.306367 val accuracy: 0.312000\n",
      "lr 1.258925e-07 reg 4.600000e+04 train accuracy: 0.302347 val accuracy: 0.319000\n",
      "lr 1.258925e-07 reg 4.700000e+04 train accuracy: 0.299408 val accuracy: 0.307000\n",
      "lr 1.258925e-07 reg 4.800000e+04 train accuracy: 0.301449 val accuracy: 0.311000\n",
      "lr 1.258925e-07 reg 4.900000e+04 train accuracy: 0.301408 val accuracy: 0.308000\n",
      "lr 1.258925e-07 reg 5.000000e+04 train accuracy: 0.298755 val accuracy: 0.307000\n",
      "lr 1.584893e-07 reg 2.500000e+04 train accuracy: 0.320041 val accuracy: 0.327000\n",
      "lr 1.584893e-07 reg 2.600000e+04 train accuracy: 0.318347 val accuracy: 0.324000\n",
      "lr 1.584893e-07 reg 2.700000e+04 train accuracy: 0.317184 val accuracy: 0.334000\n",
      "lr 1.584893e-07 reg 2.800000e+04 train accuracy: 0.315633 val accuracy: 0.322000\n",
      "lr 1.584893e-07 reg 2.900000e+04 train accuracy: 0.316469 val accuracy: 0.329000\n",
      "lr 1.584893e-07 reg 3.000000e+04 train accuracy: 0.307694 val accuracy: 0.323000\n",
      "lr 1.584893e-07 reg 3.100000e+04 train accuracy: 0.316612 val accuracy: 0.323000\n",
      "lr 1.584893e-07 reg 3.200000e+04 train accuracy: 0.311020 val accuracy: 0.323000\n",
      "lr 1.584893e-07 reg 3.300000e+04 train accuracy: 0.312857 val accuracy: 0.328000\n",
      "lr 1.584893e-07 reg 3.400000e+04 train accuracy: 0.307000 val accuracy: 0.318000\n",
      "lr 1.584893e-07 reg 3.500000e+04 train accuracy: 0.308816 val accuracy: 0.320000\n",
      "lr 1.584893e-07 reg 3.600000e+04 train accuracy: 0.311347 val accuracy: 0.315000\n",
      "lr 1.584893e-07 reg 3.700000e+04 train accuracy: 0.310816 val accuracy: 0.321000\n",
      "lr 1.584893e-07 reg 3.800000e+04 train accuracy: 0.312531 val accuracy: 0.311000\n",
      "lr 1.584893e-07 reg 3.900000e+04 train accuracy: 0.306816 val accuracy: 0.323000\n",
      "lr 1.584893e-07 reg 4.000000e+04 train accuracy: 0.301898 val accuracy: 0.310000\n",
      "lr 1.584893e-07 reg 4.100000e+04 train accuracy: 0.309735 val accuracy: 0.324000\n",
      "lr 1.584893e-07 reg 4.200000e+04 train accuracy: 0.302061 val accuracy: 0.302000\n",
      "lr 1.584893e-07 reg 4.300000e+04 train accuracy: 0.304224 val accuracy: 0.315000\n",
      "lr 1.584893e-07 reg 4.400000e+04 train accuracy: 0.291673 val accuracy: 0.308000\n",
      "lr 1.584893e-07 reg 4.500000e+04 train accuracy: 0.297184 val accuracy: 0.314000\n",
      "lr 1.584893e-07 reg 4.600000e+04 train accuracy: 0.304673 val accuracy: 0.317000\n",
      "lr 1.584893e-07 reg 4.700000e+04 train accuracy: 0.300694 val accuracy: 0.309000\n",
      "lr 1.584893e-07 reg 4.800000e+04 train accuracy: 0.307143 val accuracy: 0.312000\n",
      "lr 1.584893e-07 reg 4.900000e+04 train accuracy: 0.289449 val accuracy: 0.304000\n",
      "lr 1.584893e-07 reg 5.000000e+04 train accuracy: 0.303020 val accuracy: 0.312000\n",
      "lr 1.995262e-07 reg 2.500000e+04 train accuracy: 0.316061 val accuracy: 0.329000\n",
      "lr 1.995262e-07 reg 2.600000e+04 train accuracy: 0.322020 val accuracy: 0.334000\n",
      "lr 1.995262e-07 reg 2.700000e+04 train accuracy: 0.315102 val accuracy: 0.329000\n",
      "lr 1.995262e-07 reg 2.800000e+04 train accuracy: 0.314143 val accuracy: 0.330000\n",
      "lr 1.995262e-07 reg 2.900000e+04 train accuracy: 0.315612 val accuracy: 0.332000\n",
      "lr 1.995262e-07 reg 3.000000e+04 train accuracy: 0.317592 val accuracy: 0.320000\n",
      "lr 1.995262e-07 reg 3.100000e+04 train accuracy: 0.314429 val accuracy: 0.323000\n",
      "lr 1.995262e-07 reg 3.200000e+04 train accuracy: 0.315163 val accuracy: 0.318000\n",
      "lr 1.995262e-07 reg 3.300000e+04 train accuracy: 0.313612 val accuracy: 0.318000\n",
      "lr 1.995262e-07 reg 3.400000e+04 train accuracy: 0.306918 val accuracy: 0.323000\n",
      "lr 1.995262e-07 reg 3.500000e+04 train accuracy: 0.303469 val accuracy: 0.312000\n",
      "lr 1.995262e-07 reg 3.600000e+04 train accuracy: 0.308224 val accuracy: 0.318000\n",
      "lr 1.995262e-07 reg 3.700000e+04 train accuracy: 0.303918 val accuracy: 0.322000\n",
      "lr 1.995262e-07 reg 3.800000e+04 train accuracy: 0.314367 val accuracy: 0.312000\n",
      "lr 1.995262e-07 reg 3.900000e+04 train accuracy: 0.298388 val accuracy: 0.309000\n",
      "lr 1.995262e-07 reg 4.000000e+04 train accuracy: 0.308980 val accuracy: 0.324000\n",
      "lr 1.995262e-07 reg 4.100000e+04 train accuracy: 0.307673 val accuracy: 0.319000\n",
      "lr 1.995262e-07 reg 4.200000e+04 train accuracy: 0.301633 val accuracy: 0.308000\n",
      "lr 1.995262e-07 reg 4.300000e+04 train accuracy: 0.298102 val accuracy: 0.304000\n",
      "lr 1.995262e-07 reg 4.400000e+04 train accuracy: 0.308510 val accuracy: 0.318000\n",
      "lr 1.995262e-07 reg 4.500000e+04 train accuracy: 0.301327 val accuracy: 0.310000\n",
      "lr 1.995262e-07 reg 4.600000e+04 train accuracy: 0.303122 val accuracy: 0.325000\n",
      "lr 1.995262e-07 reg 4.700000e+04 train accuracy: 0.302592 val accuracy: 0.312000\n",
      "lr 1.995262e-07 reg 4.800000e+04 train accuracy: 0.299327 val accuracy: 0.305000\n",
      "lr 1.995262e-07 reg 4.900000e+04 train accuracy: 0.302224 val accuracy: 0.312000\n",
      "lr 1.995262e-07 reg 5.000000e+04 train accuracy: 0.305816 val accuracy: 0.313000\n",
      "lr 2.511886e-07 reg 2.500000e+04 train accuracy: 0.318939 val accuracy: 0.333000\n",
      "lr 2.511886e-07 reg 2.600000e+04 train accuracy: 0.317408 val accuracy: 0.333000\n",
      "lr 2.511886e-07 reg 2.700000e+04 train accuracy: 0.315939 val accuracy: 0.327000\n",
      "lr 2.511886e-07 reg 2.800000e+04 train accuracy: 0.320367 val accuracy: 0.335000\n",
      "lr 2.511886e-07 reg 2.900000e+04 train accuracy: 0.315796 val accuracy: 0.322000\n",
      "lr 2.511886e-07 reg 3.000000e+04 train accuracy: 0.312735 val accuracy: 0.326000\n",
      "lr 2.511886e-07 reg 3.100000e+04 train accuracy: 0.319265 val accuracy: 0.328000\n",
      "lr 2.511886e-07 reg 3.200000e+04 train accuracy: 0.311939 val accuracy: 0.328000\n",
      "lr 2.511886e-07 reg 3.300000e+04 train accuracy: 0.314592 val accuracy: 0.327000\n",
      "lr 2.511886e-07 reg 3.400000e+04 train accuracy: 0.307878 val accuracy: 0.325000\n",
      "lr 2.511886e-07 reg 3.500000e+04 train accuracy: 0.309592 val accuracy: 0.318000\n",
      "lr 2.511886e-07 reg 3.600000e+04 train accuracy: 0.311122 val accuracy: 0.318000\n",
      "lr 2.511886e-07 reg 3.700000e+04 train accuracy: 0.301082 val accuracy: 0.314000\n",
      "lr 2.511886e-07 reg 3.800000e+04 train accuracy: 0.308204 val accuracy: 0.317000\n",
      "lr 2.511886e-07 reg 3.900000e+04 train accuracy: 0.300980 val accuracy: 0.312000\n",
      "lr 2.511886e-07 reg 4.000000e+04 train accuracy: 0.302673 val accuracy: 0.317000\n",
      "lr 2.511886e-07 reg 4.100000e+04 train accuracy: 0.302959 val accuracy: 0.308000\n",
      "lr 2.511886e-07 reg 4.200000e+04 train accuracy: 0.299816 val accuracy: 0.313000\n",
      "lr 2.511886e-07 reg 4.300000e+04 train accuracy: 0.305184 val accuracy: 0.308000\n",
      "lr 2.511886e-07 reg 4.400000e+04 train accuracy: 0.303735 val accuracy: 0.311000\n",
      "lr 2.511886e-07 reg 4.500000e+04 train accuracy: 0.309714 val accuracy: 0.310000\n",
      "lr 2.511886e-07 reg 4.600000e+04 train accuracy: 0.291347 val accuracy: 0.307000\n",
      "lr 2.511886e-07 reg 4.700000e+04 train accuracy: 0.305041 val accuracy: 0.310000\n",
      "lr 2.511886e-07 reg 4.800000e+04 train accuracy: 0.307571 val accuracy: 0.323000\n",
      "lr 2.511886e-07 reg 4.900000e+04 train accuracy: 0.296327 val accuracy: 0.312000\n",
      "lr 2.511886e-07 reg 5.000000e+04 train accuracy: 0.291816 val accuracy: 0.300000\n",
      "lr 3.162278e-07 reg 2.500000e+04 train accuracy: 0.322837 val accuracy: 0.332000\n",
      "lr 3.162278e-07 reg 2.600000e+04 train accuracy: 0.319000 val accuracy: 0.322000\n",
      "lr 3.162278e-07 reg 2.700000e+04 train accuracy: 0.319224 val accuracy: 0.322000\n",
      "lr 3.162278e-07 reg 2.800000e+04 train accuracy: 0.314837 val accuracy: 0.326000\n",
      "lr 3.162278e-07 reg 2.900000e+04 train accuracy: 0.312878 val accuracy: 0.322000\n",
      "lr 3.162278e-07 reg 3.000000e+04 train accuracy: 0.305429 val accuracy: 0.322000\n",
      "lr 3.162278e-07 reg 3.100000e+04 train accuracy: 0.313265 val accuracy: 0.319000\n",
      "lr 3.162278e-07 reg 3.200000e+04 train accuracy: 0.312327 val accuracy: 0.322000\n",
      "lr 3.162278e-07 reg 3.300000e+04 train accuracy: 0.310490 val accuracy: 0.321000\n",
      "lr 3.162278e-07 reg 3.400000e+04 train accuracy: 0.296020 val accuracy: 0.306000\n",
      "lr 3.162278e-07 reg 3.500000e+04 train accuracy: 0.308837 val accuracy: 0.322000\n",
      "lr 3.162278e-07 reg 3.600000e+04 train accuracy: 0.301184 val accuracy: 0.306000\n",
      "lr 3.162278e-07 reg 3.700000e+04 train accuracy: 0.301959 val accuracy: 0.304000\n",
      "lr 3.162278e-07 reg 3.800000e+04 train accuracy: 0.304837 val accuracy: 0.318000\n",
      "lr 3.162278e-07 reg 3.900000e+04 train accuracy: 0.305408 val accuracy: 0.321000\n",
      "lr 3.162278e-07 reg 4.000000e+04 train accuracy: 0.310286 val accuracy: 0.314000\n",
      "lr 3.162278e-07 reg 4.100000e+04 train accuracy: 0.299490 val accuracy: 0.314000\n",
      "lr 3.162278e-07 reg 4.200000e+04 train accuracy: 0.296592 val accuracy: 0.307000\n",
      "lr 3.162278e-07 reg 4.300000e+04 train accuracy: 0.298939 val accuracy: 0.305000\n",
      "lr 3.162278e-07 reg 4.400000e+04 train accuracy: 0.301245 val accuracy: 0.311000\n",
      "lr 3.162278e-07 reg 4.500000e+04 train accuracy: 0.300918 val accuracy: 0.315000\n",
      "lr 3.162278e-07 reg 4.600000e+04 train accuracy: 0.292020 val accuracy: 0.308000\n",
      "lr 3.162278e-07 reg 4.700000e+04 train accuracy: 0.300714 val accuracy: 0.306000\n",
      "lr 3.162278e-07 reg 4.800000e+04 train accuracy: 0.299776 val accuracy: 0.314000\n",
      "lr 3.162278e-07 reg 4.900000e+04 train accuracy: 0.294224 val accuracy: 0.305000\n",
      "lr 3.162278e-07 reg 5.000000e+04 train accuracy: 0.300959 val accuracy: 0.306000\n",
      "lr 3.981072e-07 reg 2.500000e+04 train accuracy: 0.315592 val accuracy: 0.322000\n",
      "lr 3.981072e-07 reg 2.600000e+04 train accuracy: 0.319816 val accuracy: 0.328000\n",
      "lr 3.981072e-07 reg 2.700000e+04 train accuracy: 0.321918 val accuracy: 0.335000\n",
      "lr 3.981072e-07 reg 2.800000e+04 train accuracy: 0.316245 val accuracy: 0.330000\n",
      "lr 3.981072e-07 reg 2.900000e+04 train accuracy: 0.312347 val accuracy: 0.328000\n",
      "lr 3.981072e-07 reg 3.000000e+04 train accuracy: 0.317469 val accuracy: 0.325000\n",
      "lr 3.981072e-07 reg 3.100000e+04 train accuracy: 0.310796 val accuracy: 0.322000\n",
      "lr 3.981072e-07 reg 3.200000e+04 train accuracy: 0.308020 val accuracy: 0.311000\n",
      "lr 3.981072e-07 reg 3.300000e+04 train accuracy: 0.311857 val accuracy: 0.313000\n",
      "lr 3.981072e-07 reg 3.400000e+04 train accuracy: 0.299347 val accuracy: 0.314000\n",
      "lr 3.981072e-07 reg 3.500000e+04 train accuracy: 0.308000 val accuracy: 0.318000\n",
      "lr 3.981072e-07 reg 3.600000e+04 train accuracy: 0.294408 val accuracy: 0.303000\n",
      "lr 3.981072e-07 reg 3.700000e+04 train accuracy: 0.289551 val accuracy: 0.308000\n",
      "lr 3.981072e-07 reg 3.800000e+04 train accuracy: 0.309878 val accuracy: 0.314000\n",
      "lr 3.981072e-07 reg 3.900000e+04 train accuracy: 0.299878 val accuracy: 0.309000\n",
      "lr 3.981072e-07 reg 4.000000e+04 train accuracy: 0.300531 val accuracy: 0.314000\n",
      "lr 3.981072e-07 reg 4.100000e+04 train accuracy: 0.295347 val accuracy: 0.304000\n",
      "lr 3.981072e-07 reg 4.200000e+04 train accuracy: 0.308122 val accuracy: 0.314000\n",
      "lr 3.981072e-07 reg 4.300000e+04 train accuracy: 0.300020 val accuracy: 0.306000\n",
      "lr 3.981072e-07 reg 4.400000e+04 train accuracy: 0.308531 val accuracy: 0.310000\n",
      "lr 3.981072e-07 reg 4.500000e+04 train accuracy: 0.301612 val accuracy: 0.307000\n",
      "lr 3.981072e-07 reg 4.600000e+04 train accuracy: 0.304061 val accuracy: 0.317000\n",
      "lr 3.981072e-07 reg 4.700000e+04 train accuracy: 0.297286 val accuracy: 0.308000\n",
      "lr 3.981072e-07 reg 4.800000e+04 train accuracy: 0.300041 val accuracy: 0.307000\n",
      "lr 3.981072e-07 reg 4.900000e+04 train accuracy: 0.296429 val accuracy: 0.315000\n",
      "lr 3.981072e-07 reg 5.000000e+04 train accuracy: 0.293449 val accuracy: 0.307000\n",
      "lr 5.011872e-07 reg 2.500000e+04 train accuracy: 0.322898 val accuracy: 0.328000\n",
      "lr 5.011872e-07 reg 2.600000e+04 train accuracy: 0.313184 val accuracy: 0.317000\n",
      "lr 5.011872e-07 reg 2.700000e+04 train accuracy: 0.318143 val accuracy: 0.340000\n",
      "lr 5.011872e-07 reg 2.800000e+04 train accuracy: 0.306694 val accuracy: 0.316000\n",
      "lr 5.011872e-07 reg 2.900000e+04 train accuracy: 0.302245 val accuracy: 0.320000\n",
      "lr 5.011872e-07 reg 3.000000e+04 train accuracy: 0.310143 val accuracy: 0.328000\n",
      "lr 5.011872e-07 reg 3.100000e+04 train accuracy: 0.311367 val accuracy: 0.318000\n",
      "lr 5.011872e-07 reg 3.200000e+04 train accuracy: 0.309612 val accuracy: 0.328000\n",
      "lr 5.011872e-07 reg 3.300000e+04 train accuracy: 0.314000 val accuracy: 0.328000\n",
      "lr 5.011872e-07 reg 3.400000e+04 train accuracy: 0.304102 val accuracy: 0.319000\n",
      "lr 5.011872e-07 reg 3.500000e+04 train accuracy: 0.308367 val accuracy: 0.323000\n",
      "lr 5.011872e-07 reg 3.600000e+04 train accuracy: 0.300143 val accuracy: 0.304000\n",
      "lr 5.011872e-07 reg 3.700000e+04 train accuracy: 0.296347 val accuracy: 0.312000\n",
      "lr 5.011872e-07 reg 3.800000e+04 train accuracy: 0.291429 val accuracy: 0.314000\n",
      "lr 5.011872e-07 reg 3.900000e+04 train accuracy: 0.300286 val accuracy: 0.308000\n",
      "lr 5.011872e-07 reg 4.000000e+04 train accuracy: 0.305408 val accuracy: 0.322000\n",
      "lr 5.011872e-07 reg 4.100000e+04 train accuracy: 0.290143 val accuracy: 0.304000\n",
      "lr 5.011872e-07 reg 4.200000e+04 train accuracy: 0.303408 val accuracy: 0.311000\n",
      "lr 5.011872e-07 reg 4.300000e+04 train accuracy: 0.308959 val accuracy: 0.318000\n",
      "lr 5.011872e-07 reg 4.400000e+04 train accuracy: 0.298469 val accuracy: 0.302000\n",
      "lr 5.011872e-07 reg 4.500000e+04 train accuracy: 0.293816 val accuracy: 0.312000\n",
      "lr 5.011872e-07 reg 4.600000e+04 train accuracy: 0.292551 val accuracy: 0.314000\n",
      "lr 5.011872e-07 reg 4.700000e+04 train accuracy: 0.299816 val accuracy: 0.303000\n",
      "lr 5.011872e-07 reg 4.800000e+04 train accuracy: 0.297429 val accuracy: 0.305000\n",
      "lr 5.011872e-07 reg 4.900000e+04 train accuracy: 0.292286 val accuracy: 0.304000\n",
      "lr 5.011872e-07 reg 5.000000e+04 train accuracy: 0.292531 val accuracy: 0.306000\n",
      "lr 6.309573e-07 reg 2.500000e+04 train accuracy: 0.318000 val accuracy: 0.324000\n",
      "lr 6.309573e-07 reg 2.600000e+04 train accuracy: 0.318776 val accuracy: 0.339000\n",
      "lr 6.309573e-07 reg 2.700000e+04 train accuracy: 0.313347 val accuracy: 0.327000\n",
      "lr 6.309573e-07 reg 2.800000e+04 train accuracy: 0.305837 val accuracy: 0.324000\n",
      "lr 6.309573e-07 reg 2.900000e+04 train accuracy: 0.314020 val accuracy: 0.316000\n",
      "lr 6.309573e-07 reg 3.000000e+04 train accuracy: 0.314286 val accuracy: 0.328000\n",
      "lr 6.309573e-07 reg 3.100000e+04 train accuracy: 0.314204 val accuracy: 0.325000\n",
      "lr 6.309573e-07 reg 3.200000e+04 train accuracy: 0.313388 val accuracy: 0.318000\n",
      "lr 6.309573e-07 reg 3.300000e+04 train accuracy: 0.305510 val accuracy: 0.310000\n",
      "lr 6.309573e-07 reg 3.400000e+04 train accuracy: 0.317286 val accuracy: 0.316000\n",
      "lr 6.309573e-07 reg 3.500000e+04 train accuracy: 0.301327 val accuracy: 0.316000\n",
      "lr 6.309573e-07 reg 3.600000e+04 train accuracy: 0.314388 val accuracy: 0.326000\n",
      "lr 6.309573e-07 reg 3.700000e+04 train accuracy: 0.292367 val accuracy: 0.297000\n",
      "lr 6.309573e-07 reg 3.800000e+04 train accuracy: 0.314816 val accuracy: 0.318000\n",
      "lr 6.309573e-07 reg 3.900000e+04 train accuracy: 0.312878 val accuracy: 0.330000\n",
      "lr 6.309573e-07 reg 4.000000e+04 train accuracy: 0.299551 val accuracy: 0.314000\n",
      "lr 6.309573e-07 reg 4.100000e+04 train accuracy: 0.297571 val accuracy: 0.309000\n",
      "lr 6.309573e-07 reg 4.200000e+04 train accuracy: 0.308286 val accuracy: 0.315000\n",
      "lr 6.309573e-07 reg 4.300000e+04 train accuracy: 0.291020 val accuracy: 0.293000\n",
      "lr 6.309573e-07 reg 4.400000e+04 train accuracy: 0.297837 val accuracy: 0.317000\n",
      "lr 6.309573e-07 reg 4.500000e+04 train accuracy: 0.286918 val accuracy: 0.299000\n",
      "lr 6.309573e-07 reg 4.600000e+04 train accuracy: 0.302388 val accuracy: 0.306000\n",
      "lr 6.309573e-07 reg 4.700000e+04 train accuracy: 0.297184 val accuracy: 0.304000\n",
      "lr 6.309573e-07 reg 4.800000e+04 train accuracy: 0.301429 val accuracy: 0.307000\n",
      "lr 6.309573e-07 reg 4.900000e+04 train accuracy: 0.302857 val accuracy: 0.316000\n",
      "lr 6.309573e-07 reg 5.000000e+04 train accuracy: 0.297388 val accuracy: 0.307000\n",
      "lr 7.943282e-07 reg 2.500000e+04 train accuracy: 0.309327 val accuracy: 0.319000\n",
      "lr 7.943282e-07 reg 2.600000e+04 train accuracy: 0.319449 val accuracy: 0.337000\n",
      "lr 7.943282e-07 reg 2.700000e+04 train accuracy: 0.318980 val accuracy: 0.336000\n",
      "lr 7.943282e-07 reg 2.800000e+04 train accuracy: 0.321592 val accuracy: 0.334000\n",
      "lr 7.943282e-07 reg 2.900000e+04 train accuracy: 0.313755 val accuracy: 0.329000\n",
      "lr 7.943282e-07 reg 3.000000e+04 train accuracy: 0.312327 val accuracy: 0.323000\n",
      "lr 7.943282e-07 reg 3.100000e+04 train accuracy: 0.300061 val accuracy: 0.303000\n",
      "lr 7.943282e-07 reg 3.200000e+04 train accuracy: 0.311918 val accuracy: 0.334000\n",
      "lr 7.943282e-07 reg 3.300000e+04 train accuracy: 0.307510 val accuracy: 0.320000\n",
      "lr 7.943282e-07 reg 3.400000e+04 train accuracy: 0.302327 val accuracy: 0.320000\n",
      "lr 7.943282e-07 reg 3.500000e+04 train accuracy: 0.306918 val accuracy: 0.315000\n",
      "lr 7.943282e-07 reg 3.600000e+04 train accuracy: 0.295041 val accuracy: 0.307000\n",
      "lr 7.943282e-07 reg 3.700000e+04 train accuracy: 0.307449 val accuracy: 0.327000\n",
      "lr 7.943282e-07 reg 3.800000e+04 train accuracy: 0.294102 val accuracy: 0.314000\n",
      "lr 7.943282e-07 reg 3.900000e+04 train accuracy: 0.298673 val accuracy: 0.320000\n",
      "lr 7.943282e-07 reg 4.000000e+04 train accuracy: 0.285000 val accuracy: 0.301000\n",
      "lr 7.943282e-07 reg 4.100000e+04 train accuracy: 0.290122 val accuracy: 0.302000\n",
      "lr 7.943282e-07 reg 4.200000e+04 train accuracy: 0.294898 val accuracy: 0.313000\n",
      "lr 7.943282e-07 reg 4.300000e+04 train accuracy: 0.300265 val accuracy: 0.318000\n",
      "lr 7.943282e-07 reg 4.400000e+04 train accuracy: 0.301265 val accuracy: 0.297000\n",
      "lr 7.943282e-07 reg 4.500000e+04 train accuracy: 0.309429 val accuracy: 0.320000\n",
      "lr 7.943282e-07 reg 4.600000e+04 train accuracy: 0.294184 val accuracy: 0.305000\n",
      "lr 7.943282e-07 reg 4.700000e+04 train accuracy: 0.293408 val accuracy: 0.290000\n",
      "lr 7.943282e-07 reg 4.800000e+04 train accuracy: 0.277224 val accuracy: 0.290000\n",
      "lr 7.943282e-07 reg 4.900000e+04 train accuracy: 0.292551 val accuracy: 0.306000\n",
      "lr 7.943282e-07 reg 5.000000e+04 train accuracy: 0.290204 val accuracy: 0.312000\n",
      "lr 1.000000e-06 reg 2.500000e+04 train accuracy: 0.317122 val accuracy: 0.322000\n",
      "lr 1.000000e-06 reg 2.600000e+04 train accuracy: 0.309265 val accuracy: 0.321000\n",
      "lr 1.000000e-06 reg 2.700000e+04 train accuracy: 0.305633 val accuracy: 0.320000\n",
      "lr 1.000000e-06 reg 2.800000e+04 train accuracy: 0.315327 val accuracy: 0.333000\n",
      "lr 1.000000e-06 reg 2.900000e+04 train accuracy: 0.305245 val accuracy: 0.319000\n",
      "lr 1.000000e-06 reg 3.000000e+04 train accuracy: 0.300163 val accuracy: 0.315000\n",
      "lr 1.000000e-06 reg 3.100000e+04 train accuracy: 0.304755 val accuracy: 0.322000\n",
      "lr 1.000000e-06 reg 3.200000e+04 train accuracy: 0.293061 val accuracy: 0.301000\n",
      "lr 1.000000e-06 reg 3.300000e+04 train accuracy: 0.308551 val accuracy: 0.316000\n",
      "lr 1.000000e-06 reg 3.400000e+04 train accuracy: 0.301429 val accuracy: 0.305000\n",
      "lr 1.000000e-06 reg 3.500000e+04 train accuracy: 0.293184 val accuracy: 0.309000\n",
      "lr 1.000000e-06 reg 3.600000e+04 train accuracy: 0.294204 val accuracy: 0.298000\n",
      "lr 1.000000e-06 reg 3.700000e+04 train accuracy: 0.297082 val accuracy: 0.317000\n",
      "lr 1.000000e-06 reg 3.800000e+04 train accuracy: 0.306837 val accuracy: 0.316000\n",
      "lr 1.000000e-06 reg 3.900000e+04 train accuracy: 0.304673 val accuracy: 0.305000\n",
      "lr 1.000000e-06 reg 4.000000e+04 train accuracy: 0.296000 val accuracy: 0.300000\n",
      "lr 1.000000e-06 reg 4.100000e+04 train accuracy: 0.291388 val accuracy: 0.298000\n",
      "lr 1.000000e-06 reg 4.200000e+04 train accuracy: 0.301592 val accuracy: 0.314000\n",
      "lr 1.000000e-06 reg 4.300000e+04 train accuracy: 0.295061 val accuracy: 0.314000\n",
      "lr 1.000000e-06 reg 4.400000e+04 train accuracy: 0.298163 val accuracy: 0.307000\n",
      "lr 1.000000e-06 reg 4.500000e+04 train accuracy: 0.278327 val accuracy: 0.286000\n",
      "lr 1.000000e-06 reg 4.600000e+04 train accuracy: 0.292531 val accuracy: 0.301000\n",
      "lr 1.000000e-06 reg 4.700000e+04 train accuracy: 0.284061 val accuracy: 0.294000\n",
      "lr 1.000000e-06 reg 4.800000e+04 train accuracy: 0.299959 val accuracy: 0.307000\n",
      "lr 1.000000e-06 reg 4.900000e+04 train accuracy: 0.280612 val accuracy: 0.307000\n",
      "lr 1.000000e-06 reg 5.000000e+04 train accuracy: 0.289122 val accuracy: 0.296000\n",
      "lr 1.258925e-06 reg 2.500000e+04 train accuracy: 0.313898 val accuracy: 0.324000\n",
      "lr 1.258925e-06 reg 2.600000e+04 train accuracy: 0.293265 val accuracy: 0.302000\n",
      "lr 1.258925e-06 reg 2.700000e+04 train accuracy: 0.322245 val accuracy: 0.347000\n",
      "lr 1.258925e-06 reg 2.800000e+04 train accuracy: 0.313939 val accuracy: 0.339000\n",
      "lr 1.258925e-06 reg 2.900000e+04 train accuracy: 0.311102 val accuracy: 0.323000\n",
      "lr 1.258925e-06 reg 3.000000e+04 train accuracy: 0.284857 val accuracy: 0.292000\n",
      "lr 1.258925e-06 reg 3.100000e+04 train accuracy: 0.294592 val accuracy: 0.309000\n",
      "lr 1.258925e-06 reg 3.200000e+04 train accuracy: 0.308143 val accuracy: 0.327000\n",
      "lr 1.258925e-06 reg 3.300000e+04 train accuracy: 0.310857 val accuracy: 0.321000\n",
      "lr 1.258925e-06 reg 3.400000e+04 train accuracy: 0.306551 val accuracy: 0.322000\n",
      "lr 1.258925e-06 reg 3.500000e+04 train accuracy: 0.293184 val accuracy: 0.311000\n",
      "lr 1.258925e-06 reg 3.600000e+04 train accuracy: 0.293898 val accuracy: 0.315000\n",
      "lr 1.258925e-06 reg 3.700000e+04 train accuracy: 0.295612 val accuracy: 0.301000\n",
      "lr 1.258925e-06 reg 3.800000e+04 train accuracy: 0.298020 val accuracy: 0.312000\n",
      "lr 1.258925e-06 reg 3.900000e+04 train accuracy: 0.289714 val accuracy: 0.300000\n",
      "lr 1.258925e-06 reg 4.000000e+04 train accuracy: 0.295449 val accuracy: 0.311000\n",
      "lr 1.258925e-06 reg 4.100000e+04 train accuracy: 0.293224 val accuracy: 0.302000\n",
      "lr 1.258925e-06 reg 4.200000e+04 train accuracy: 0.276306 val accuracy: 0.286000\n",
      "lr 1.258925e-06 reg 4.300000e+04 train accuracy: 0.274939 val accuracy: 0.284000\n",
      "lr 1.258925e-06 reg 4.400000e+04 train accuracy: 0.285918 val accuracy: 0.296000\n",
      "lr 1.258925e-06 reg 4.500000e+04 train accuracy: 0.274265 val accuracy: 0.289000\n",
      "lr 1.258925e-06 reg 4.600000e+04 train accuracy: 0.283776 val accuracy: 0.296000\n",
      "lr 1.258925e-06 reg 4.700000e+04 train accuracy: 0.279184 val accuracy: 0.290000\n",
      "lr 1.258925e-06 reg 4.800000e+04 train accuracy: 0.292490 val accuracy: 0.306000\n",
      "lr 1.258925e-06 reg 4.900000e+04 train accuracy: 0.281490 val accuracy: 0.301000\n",
      "lr 1.258925e-06 reg 5.000000e+04 train accuracy: 0.277592 val accuracy: 0.286000\n",
      "lr 1.584893e-06 reg 2.500000e+04 train accuracy: 0.311510 val accuracy: 0.325000\n",
      "lr 1.584893e-06 reg 2.600000e+04 train accuracy: 0.299857 val accuracy: 0.313000\n",
      "lr 1.584893e-06 reg 2.700000e+04 train accuracy: 0.301735 val accuracy: 0.309000\n",
      "lr 1.584893e-06 reg 2.800000e+04 train accuracy: 0.306837 val accuracy: 0.311000\n",
      "lr 1.584893e-06 reg 2.900000e+04 train accuracy: 0.316388 val accuracy: 0.336000\n",
      "lr 1.584893e-06 reg 3.000000e+04 train accuracy: 0.300673 val accuracy: 0.309000\n",
      "lr 1.584893e-06 reg 3.100000e+04 train accuracy: 0.290367 val accuracy: 0.307000\n",
      "lr 1.584893e-06 reg 3.200000e+04 train accuracy: 0.292878 val accuracy: 0.304000\n",
      "lr 1.584893e-06 reg 3.300000e+04 train accuracy: 0.292755 val accuracy: 0.309000\n",
      "lr 1.584893e-06 reg 3.400000e+04 train accuracy: 0.279633 val accuracy: 0.287000\n",
      "lr 1.584893e-06 reg 3.500000e+04 train accuracy: 0.294327 val accuracy: 0.299000\n",
      "lr 1.584893e-06 reg 3.600000e+04 train accuracy: 0.267837 val accuracy: 0.279000\n",
      "lr 1.584893e-06 reg 3.700000e+04 train accuracy: 0.292694 val accuracy: 0.316000\n",
      "lr 1.584893e-06 reg 3.800000e+04 train accuracy: 0.284163 val accuracy: 0.294000\n",
      "lr 1.584893e-06 reg 3.900000e+04 train accuracy: 0.276592 val accuracy: 0.287000\n",
      "lr 1.584893e-06 reg 4.000000e+04 train accuracy: 0.314347 val accuracy: 0.316000\n",
      "lr 1.584893e-06 reg 4.100000e+04 train accuracy: 0.281367 val accuracy: 0.300000\n",
      "lr 1.584893e-06 reg 4.200000e+04 train accuracy: 0.294367 val accuracy: 0.305000\n",
      "lr 1.584893e-06 reg 4.300000e+04 train accuracy: 0.291367 val accuracy: 0.299000\n",
      "lr 1.584893e-06 reg 4.400000e+04 train accuracy: 0.295122 val accuracy: 0.304000\n",
      "lr 1.584893e-06 reg 4.500000e+04 train accuracy: 0.304429 val accuracy: 0.317000\n",
      "lr 1.584893e-06 reg 4.600000e+04 train accuracy: 0.293143 val accuracy: 0.317000\n",
      "lr 1.584893e-06 reg 4.700000e+04 train accuracy: 0.289714 val accuracy: 0.302000\n",
      "lr 1.584893e-06 reg 4.800000e+04 train accuracy: 0.289449 val accuracy: 0.296000\n",
      "lr 1.584893e-06 reg 4.900000e+04 train accuracy: 0.298571 val accuracy: 0.306000\n",
      "lr 1.584893e-06 reg 5.000000e+04 train accuracy: 0.266143 val accuracy: 0.277000\n",
      "lr 1.995262e-06 reg 2.500000e+04 train accuracy: 0.287735 val accuracy: 0.281000\n",
      "lr 1.995262e-06 reg 2.600000e+04 train accuracy: 0.284510 val accuracy: 0.299000\n",
      "lr 1.995262e-06 reg 2.700000e+04 train accuracy: 0.299082 val accuracy: 0.302000\n",
      "lr 1.995262e-06 reg 2.800000e+04 train accuracy: 0.288020 val accuracy: 0.283000\n",
      "lr 1.995262e-06 reg 2.900000e+04 train accuracy: 0.298857 val accuracy: 0.299000\n",
      "lr 1.995262e-06 reg 3.000000e+04 train accuracy: 0.278082 val accuracy: 0.293000\n",
      "lr 1.995262e-06 reg 3.100000e+04 train accuracy: 0.287857 val accuracy: 0.290000\n",
      "lr 1.995262e-06 reg 3.200000e+04 train accuracy: 0.277816 val accuracy: 0.285000\n",
      "lr 1.995262e-06 reg 3.300000e+04 train accuracy: 0.279755 val accuracy: 0.311000\n",
      "lr 1.995262e-06 reg 3.400000e+04 train accuracy: 0.264286 val accuracy: 0.269000\n",
      "lr 1.995262e-06 reg 3.500000e+04 train accuracy: 0.318918 val accuracy: 0.333000\n",
      "lr 1.995262e-06 reg 3.600000e+04 train accuracy: 0.266714 val accuracy: 0.260000\n",
      "lr 1.995262e-06 reg 3.700000e+04 train accuracy: 0.286898 val accuracy: 0.298000\n",
      "lr 1.995262e-06 reg 3.800000e+04 train accuracy: 0.299694 val accuracy: 0.310000\n",
      "lr 1.995262e-06 reg 3.900000e+04 train accuracy: 0.285755 val accuracy: 0.299000\n",
      "lr 1.995262e-06 reg 4.000000e+04 train accuracy: 0.277347 val accuracy: 0.304000\n",
      "lr 1.995262e-06 reg 4.100000e+04 train accuracy: 0.281469 val accuracy: 0.295000\n",
      "lr 1.995262e-06 reg 4.200000e+04 train accuracy: 0.281939 val accuracy: 0.290000\n",
      "lr 1.995262e-06 reg 4.300000e+04 train accuracy: 0.272490 val accuracy: 0.287000\n",
      "lr 1.995262e-06 reg 4.400000e+04 train accuracy: 0.273265 val accuracy: 0.287000\n",
      "lr 1.995262e-06 reg 4.500000e+04 train accuracy: 0.277306 val accuracy: 0.293000\n",
      "lr 1.995262e-06 reg 4.600000e+04 train accuracy: 0.295388 val accuracy: 0.313000\n",
      "lr 1.995262e-06 reg 4.700000e+04 train accuracy: 0.274367 val accuracy: 0.280000\n",
      "lr 1.995262e-06 reg 4.800000e+04 train accuracy: 0.286429 val accuracy: 0.283000\n",
      "lr 1.995262e-06 reg 4.900000e+04 train accuracy: 0.285653 val accuracy: 0.297000\n",
      "lr 1.995262e-06 reg 5.000000e+04 train accuracy: 0.263449 val accuracy: 0.254000\n",
      "lr 2.511886e-06 reg 2.500000e+04 train accuracy: 0.296776 val accuracy: 0.313000\n",
      "lr 2.511886e-06 reg 2.600000e+04 train accuracy: 0.292510 val accuracy: 0.312000\n",
      "lr 2.511886e-06 reg 2.700000e+04 train accuracy: 0.282020 val accuracy: 0.300000\n",
      "lr 2.511886e-06 reg 2.800000e+04 train accuracy: 0.283673 val accuracy: 0.286000\n",
      "lr 2.511886e-06 reg 2.900000e+04 train accuracy: 0.307041 val accuracy: 0.324000\n",
      "lr 2.511886e-06 reg 3.000000e+04 train accuracy: 0.281224 val accuracy: 0.302000\n",
      "lr 2.511886e-06 reg 3.100000e+04 train accuracy: 0.279490 val accuracy: 0.288000\n",
      "lr 2.511886e-06 reg 3.200000e+04 train accuracy: 0.270551 val accuracy: 0.282000\n",
      "lr 2.511886e-06 reg 3.300000e+04 train accuracy: 0.289122 val accuracy: 0.297000\n",
      "lr 2.511886e-06 reg 3.400000e+04 train accuracy: 0.265490 val accuracy: 0.301000\n",
      "lr 2.511886e-06 reg 3.500000e+04 train accuracy: 0.294449 val accuracy: 0.316000\n",
      "lr 2.511886e-06 reg 3.600000e+04 train accuracy: 0.282857 val accuracy: 0.286000\n",
      "lr 2.511886e-06 reg 3.700000e+04 train accuracy: 0.265714 val accuracy: 0.283000\n",
      "lr 2.511886e-06 reg 3.800000e+04 train accuracy: 0.275694 val accuracy: 0.288000\n",
      "lr 2.511886e-06 reg 3.900000e+04 train accuracy: 0.255714 val accuracy: 0.267000\n",
      "lr 2.511886e-06 reg 4.000000e+04 train accuracy: 0.259388 val accuracy: 0.269000\n",
      "lr 2.511886e-06 reg 4.100000e+04 train accuracy: 0.278653 val accuracy: 0.276000\n",
      "lr 2.511886e-06 reg 4.200000e+04 train accuracy: 0.297551 val accuracy: 0.314000\n",
      "lr 2.511886e-06 reg 4.300000e+04 train accuracy: 0.271184 val accuracy: 0.288000\n",
      "lr 2.511886e-06 reg 4.400000e+04 train accuracy: 0.271449 val accuracy: 0.280000\n",
      "lr 2.511886e-06 reg 4.500000e+04 train accuracy: 0.265878 val accuracy: 0.287000\n",
      "lr 2.511886e-06 reg 4.600000e+04 train accuracy: 0.249347 val accuracy: 0.260000\n",
      "lr 2.511886e-06 reg 4.700000e+04 train accuracy: 0.267020 val accuracy: 0.287000\n",
      "lr 2.511886e-06 reg 4.800000e+04 train accuracy: 0.265224 val accuracy: 0.267000\n",
      "lr 2.511886e-06 reg 4.900000e+04 train accuracy: 0.245306 val accuracy: 0.263000\n",
      "lr 2.511886e-06 reg 5.000000e+04 train accuracy: 0.287633 val accuracy: 0.285000\n",
      "lr 3.162278e-06 reg 2.500000e+04 train accuracy: 0.287163 val accuracy: 0.314000\n",
      "lr 3.162278e-06 reg 2.600000e+04 train accuracy: 0.280327 val accuracy: 0.271000\n",
      "lr 3.162278e-06 reg 2.700000e+04 train accuracy: 0.278327 val accuracy: 0.277000\n",
      "lr 3.162278e-06 reg 2.800000e+04 train accuracy: 0.260388 val accuracy: 0.263000\n",
      "lr 3.162278e-06 reg 2.900000e+04 train accuracy: 0.265245 val accuracy: 0.281000\n",
      "lr 3.162278e-06 reg 3.000000e+04 train accuracy: 0.265592 val accuracy: 0.277000\n",
      "lr 3.162278e-06 reg 3.100000e+04 train accuracy: 0.280571 val accuracy: 0.289000\n",
      "lr 3.162278e-06 reg 3.200000e+04 train accuracy: 0.279776 val accuracy: 0.274000\n",
      "lr 3.162278e-06 reg 3.300000e+04 train accuracy: 0.295020 val accuracy: 0.314000\n",
      "lr 3.162278e-06 reg 3.400000e+04 train accuracy: 0.272265 val accuracy: 0.271000\n",
      "lr 3.162278e-06 reg 3.500000e+04 train accuracy: 0.254755 val accuracy: 0.249000\n",
      "lr 3.162278e-06 reg 3.600000e+04 train accuracy: 0.249245 val accuracy: 0.259000\n",
      "lr 3.162278e-06 reg 3.700000e+04 train accuracy: 0.261041 val accuracy: 0.255000\n",
      "lr 3.162278e-06 reg 3.800000e+04 train accuracy: 0.266898 val accuracy: 0.273000\n",
      "lr 3.162278e-06 reg 3.900000e+04 train accuracy: 0.269245 val accuracy: 0.280000\n",
      "lr 3.162278e-06 reg 4.000000e+04 train accuracy: 0.232776 val accuracy: 0.244000\n",
      "lr 3.162278e-06 reg 4.100000e+04 train accuracy: 0.247122 val accuracy: 0.272000\n",
      "lr 3.162278e-06 reg 4.200000e+04 train accuracy: 0.247429 val accuracy: 0.254000\n",
      "lr 3.162278e-06 reg 4.300000e+04 train accuracy: 0.238388 val accuracy: 0.256000\n",
      "lr 3.162278e-06 reg 4.400000e+04 train accuracy: 0.261286 val accuracy: 0.270000\n",
      "lr 3.162278e-06 reg 4.500000e+04 train accuracy: 0.240776 val accuracy: 0.246000\n",
      "lr 3.162278e-06 reg 4.600000e+04 train accuracy: 0.219327 val accuracy: 0.224000\n",
      "lr 3.162278e-06 reg 4.700000e+04 train accuracy: 0.239592 val accuracy: 0.257000\n",
      "lr 3.162278e-06 reg 4.800000e+04 train accuracy: 0.257327 val accuracy: 0.253000\n",
      "lr 3.162278e-06 reg 4.900000e+04 train accuracy: 0.259776 val accuracy: 0.276000\n",
      "lr 3.162278e-06 reg 5.000000e+04 train accuracy: 0.256041 val accuracy: 0.268000\n",
      "lr 3.981072e-06 reg 2.500000e+04 train accuracy: 0.268510 val accuracy: 0.288000\n",
      "lr 3.981072e-06 reg 2.600000e+04 train accuracy: 0.245000 val accuracy: 0.236000\n",
      "lr 3.981072e-06 reg 2.700000e+04 train accuracy: 0.239939 val accuracy: 0.240000\n",
      "lr 3.981072e-06 reg 2.800000e+04 train accuracy: 0.273531 val accuracy: 0.303000\n",
      "lr 3.981072e-06 reg 2.900000e+04 train accuracy: 0.243776 val accuracy: 0.260000\n",
      "lr 3.981072e-06 reg 3.000000e+04 train accuracy: 0.236143 val accuracy: 0.235000\n",
      "lr 3.981072e-06 reg 3.100000e+04 train accuracy: 0.226245 val accuracy: 0.264000\n",
      "lr 3.981072e-06 reg 3.200000e+04 train accuracy: 0.259367 val accuracy: 0.279000\n",
      "lr 3.981072e-06 reg 3.300000e+04 train accuracy: 0.215959 val accuracy: 0.229000\n",
      "lr 3.981072e-06 reg 3.400000e+04 train accuracy: 0.258204 val accuracy: 0.272000\n",
      "lr 3.981072e-06 reg 3.500000e+04 train accuracy: 0.237061 val accuracy: 0.258000\n",
      "lr 3.981072e-06 reg 3.600000e+04 train accuracy: 0.293367 val accuracy: 0.306000\n",
      "lr 3.981072e-06 reg 3.700000e+04 train accuracy: 0.196653 val accuracy: 0.221000\n",
      "lr 3.981072e-06 reg 3.800000e+04 train accuracy: 0.239653 val accuracy: 0.255000\n",
      "lr 3.981072e-06 reg 3.900000e+04 train accuracy: 0.274959 val accuracy: 0.270000\n",
      "lr 3.981072e-06 reg 4.000000e+04 train accuracy: 0.268082 val accuracy: 0.263000\n",
      "lr 3.981072e-06 reg 4.100000e+04 train accuracy: 0.204510 val accuracy: 0.208000\n",
      "lr 3.981072e-06 reg 4.200000e+04 train accuracy: 0.242796 val accuracy: 0.274000\n",
      "lr 3.981072e-06 reg 4.300000e+04 train accuracy: 0.236184 val accuracy: 0.254000\n",
      "lr 3.981072e-06 reg 4.400000e+04 train accuracy: 0.239245 val accuracy: 0.241000\n",
      "lr 3.981072e-06 reg 4.500000e+04 train accuracy: 0.260735 val accuracy: 0.280000\n",
      "lr 3.981072e-06 reg 4.600000e+04 train accuracy: 0.224510 val accuracy: 0.229000\n",
      "lr 3.981072e-06 reg 4.700000e+04 train accuracy: 0.245653 val accuracy: 0.269000\n",
      "lr 3.981072e-06 reg 4.800000e+04 train accuracy: 0.250816 val accuracy: 0.266000\n",
      "lr 3.981072e-06 reg 4.900000e+04 train accuracy: 0.239224 val accuracy: 0.256000\n",
      "lr 3.981072e-06 reg 5.000000e+04 train accuracy: 0.235694 val accuracy: 0.265000\n",
      "lr 5.011872e-06 reg 2.500000e+04 train accuracy: 0.255531 val accuracy: 0.273000\n",
      "lr 5.011872e-06 reg 2.600000e+04 train accuracy: 0.198898 val accuracy: 0.194000\n",
      "lr 5.011872e-06 reg 2.700000e+04 train accuracy: 0.199878 val accuracy: 0.203000\n",
      "lr 5.011872e-06 reg 2.800000e+04 train accuracy: 0.221980 val accuracy: 0.253000\n",
      "lr 5.011872e-06 reg 2.900000e+04 train accuracy: 0.188102 val accuracy: 0.188000\n",
      "lr 5.011872e-06 reg 3.000000e+04 train accuracy: 0.183816 val accuracy: 0.183000\n",
      "lr 5.011872e-06 reg 3.100000e+04 train accuracy: 0.171694 val accuracy: 0.178000\n",
      "lr 5.011872e-06 reg 3.200000e+04 train accuracy: 0.213633 val accuracy: 0.230000\n",
      "lr 5.011872e-06 reg 3.300000e+04 train accuracy: 0.167592 val accuracy: 0.163000\n",
      "lr 5.011872e-06 reg 3.400000e+04 train accuracy: 0.192469 val accuracy: 0.197000\n",
      "lr 5.011872e-06 reg 3.500000e+04 train accuracy: 0.236878 val accuracy: 0.225000\n",
      "lr 5.011872e-06 reg 3.600000e+04 train accuracy: 0.192735 val accuracy: 0.224000\n",
      "lr 5.011872e-06 reg 3.700000e+04 train accuracy: 0.178122 val accuracy: 0.177000\n",
      "lr 5.011872e-06 reg 3.800000e+04 train accuracy: 0.163490 val accuracy: 0.157000\n",
      "lr 5.011872e-06 reg 3.900000e+04 train accuracy: 0.150918 val accuracy: 0.167000\n",
      "lr 5.011872e-06 reg 4.000000e+04 train accuracy: 0.139796 val accuracy: 0.142000\n",
      "lr 5.011872e-06 reg 4.100000e+04 train accuracy: 0.176449 val accuracy: 0.181000\n",
      "lr 5.011872e-06 reg 4.200000e+04 train accuracy: 0.147633 val accuracy: 0.138000\n",
      "lr 5.011872e-06 reg 4.300000e+04 train accuracy: 0.225898 val accuracy: 0.211000\n",
      "lr 5.011872e-06 reg 4.400000e+04 train accuracy: 0.129857 val accuracy: 0.105000\n",
      "lr 5.011872e-06 reg 4.500000e+04 train accuracy: 0.145714 val accuracy: 0.157000\n",
      "lr 5.011872e-06 reg 4.600000e+04 train accuracy: 0.165469 val accuracy: 0.175000\n",
      "lr 5.011872e-06 reg 4.700000e+04 train accuracy: 0.175082 val accuracy: 0.192000\n",
      "lr 5.011872e-06 reg 4.800000e+04 train accuracy: 0.144388 val accuracy: 0.164000\n",
      "lr 5.011872e-06 reg 4.900000e+04 train accuracy: 0.152204 val accuracy: 0.154000\n",
      "lr 5.011872e-06 reg 5.000000e+04 train accuracy: 0.191694 val accuracy: 0.196000\n",
      "lr 6.309573e-06 reg 2.500000e+04 train accuracy: 0.170633 val accuracy: 0.179000\n",
      "lr 6.309573e-06 reg 2.600000e+04 train accuracy: 0.186020 val accuracy: 0.202000\n",
      "lr 6.309573e-06 reg 2.700000e+04 train accuracy: 0.157531 val accuracy: 0.150000\n",
      "lr 6.309573e-06 reg 2.800000e+04 train accuracy: 0.226184 val accuracy: 0.226000\n",
      "lr 6.309573e-06 reg 2.900000e+04 train accuracy: 0.144837 val accuracy: 0.139000\n",
      "lr 6.309573e-06 reg 3.000000e+04 train accuracy: 0.171490 val accuracy: 0.157000\n",
      "lr 6.309573e-06 reg 3.100000e+04 train accuracy: 0.158755 val accuracy: 0.157000\n",
      "lr 6.309573e-06 reg 3.200000e+04 train accuracy: 0.201714 val accuracy: 0.218000\n",
      "lr 6.309573e-06 reg 3.300000e+04 train accuracy: 0.190633 val accuracy: 0.195000\n",
      "lr 6.309573e-06 reg 3.400000e+04 train accuracy: 0.163673 val accuracy: 0.180000\n",
      "lr 6.309573e-06 reg 3.500000e+04 train accuracy: 0.145224 val accuracy: 0.159000\n",
      "lr 6.309573e-06 reg 3.600000e+04 train accuracy: 0.140245 val accuracy: 0.156000\n",
      "lr 6.309573e-06 reg 3.700000e+04 train accuracy: 0.145918 val accuracy: 0.150000\n",
      "lr 6.309573e-06 reg 3.800000e+04 train accuracy: 0.213224 val accuracy: 0.223000\n",
      "lr 6.309573e-06 reg 3.900000e+04 train accuracy: 0.164776 val accuracy: 0.176000\n",
      "lr 6.309573e-06 reg 4.000000e+04 train accuracy: 0.118102 val accuracy: 0.098000\n",
      "lr 6.309573e-06 reg 4.100000e+04 train accuracy: 0.118837 val accuracy: 0.124000\n",
      "lr 6.309573e-06 reg 4.200000e+04 train accuracy: 0.115224 val accuracy: 0.126000\n",
      "lr 6.309573e-06 reg 4.300000e+04 train accuracy: 0.134653 val accuracy: 0.135000\n",
      "lr 6.309573e-06 reg 4.400000e+04 train accuracy: 0.136204 val accuracy: 0.146000\n",
      "lr 6.309573e-06 reg 4.500000e+04 train accuracy: 0.131592 val accuracy: 0.145000\n",
      "lr 6.309573e-06 reg 4.600000e+04 train accuracy: 0.143735 val accuracy: 0.145000\n",
      "lr 6.309573e-06 reg 4.700000e+04 train accuracy: 0.124959 val accuracy: 0.138000\n",
      "lr 6.309573e-06 reg 4.800000e+04 train accuracy: 0.137245 val accuracy: 0.120000\n",
      "lr 6.309573e-06 reg 4.900000e+04 train accuracy: 0.128673 val accuracy: 0.127000\n",
      "lr 6.309573e-06 reg 5.000000e+04 train accuracy: 0.096837 val accuracy: 0.106000\n",
      "lr 7.943282e-06 reg 2.500000e+04 train accuracy: 0.153020 val accuracy: 0.160000\n",
      "lr 7.943282e-06 reg 2.600000e+04 train accuracy: 0.166653 val accuracy: 0.159000\n",
      "lr 7.943282e-06 reg 2.700000e+04 train accuracy: 0.192000 val accuracy: 0.196000\n",
      "lr 7.943282e-06 reg 2.800000e+04 train accuracy: 0.102714 val accuracy: 0.127000\n",
      "lr 7.943282e-06 reg 2.900000e+04 train accuracy: 0.134469 val accuracy: 0.139000\n",
      "lr 7.943282e-06 reg 3.000000e+04 train accuracy: 0.168469 val accuracy: 0.170000\n",
      "lr 7.943282e-06 reg 3.100000e+04 train accuracy: 0.093510 val accuracy: 0.090000\n",
      "lr 7.943282e-06 reg 3.200000e+04 train accuracy: 0.129245 val accuracy: 0.123000\n",
      "lr 7.943282e-06 reg 3.300000e+04 train accuracy: 0.117286 val accuracy: 0.134000\n",
      "lr 7.943282e-06 reg 3.400000e+04 train accuracy: 0.163755 val accuracy: 0.178000\n",
      "lr 7.943282e-06 reg 3.500000e+04 train accuracy: 0.128551 val accuracy: 0.134000\n",
      "lr 7.943282e-06 reg 3.600000e+04 train accuracy: 0.159000 val accuracy: 0.180000\n",
      "lr 7.943282e-06 reg 3.700000e+04 train accuracy: 0.130837 val accuracy: 0.126000\n",
      "lr 7.943282e-06 reg 3.800000e+04 train accuracy: 0.156857 val accuracy: 0.137000\n",
      "lr 7.943282e-06 reg 3.900000e+04 train accuracy: 0.118571 val accuracy: 0.111000\n",
      "lr 7.943282e-06 reg 4.000000e+04 train accuracy: 0.094204 val accuracy: 0.097000\n",
      "lr 7.943282e-06 reg 4.100000e+04 train accuracy: 0.095122 val accuracy: 0.100000\n",
      "lr 7.943282e-06 reg 4.200000e+04 train accuracy: 0.124306 val accuracy: 0.116000\n",
      "lr 7.943282e-06 reg 4.300000e+04 train accuracy: 0.097000 val accuracy: 0.095000\n",
      "lr 7.943282e-06 reg 4.400000e+04 train accuracy: 0.128429 val accuracy: 0.122000\n",
      "lr 7.943282e-06 reg 4.500000e+04 train accuracy: 0.130306 val accuracy: 0.117000\n",
      "lr 7.943282e-06 reg 4.600000e+04 train accuracy: 0.119306 val accuracy: 0.105000\n",
      "lr 7.943282e-06 reg 4.700000e+04 train accuracy: 0.091469 val accuracy: 0.077000\n",
      "lr 7.943282e-06 reg 4.800000e+04 train accuracy: 0.120837 val accuracy: 0.111000\n",
      "lr 7.943282e-06 reg 4.900000e+04 train accuracy: 0.101245 val accuracy: 0.111000\n",
      "lr 7.943282e-06 reg 5.000000e+04 train accuracy: 0.123000 val accuracy: 0.115000\n",
      "lr 1.000000e-05 reg 2.500000e+04 train accuracy: 0.170633 val accuracy: 0.172000\n",
      "lr 1.000000e-05 reg 2.600000e+04 train accuracy: 0.155245 val accuracy: 0.144000\n",
      "lr 1.000000e-05 reg 2.700000e+04 train accuracy: 0.136347 val accuracy: 0.133000\n",
      "lr 1.000000e-05 reg 2.800000e+04 train accuracy: 0.152347 val accuracy: 0.141000\n",
      "lr 1.000000e-05 reg 2.900000e+04 train accuracy: 0.160673 val accuracy: 0.174000\n",
      "lr 1.000000e-05 reg 3.000000e+04 train accuracy: 0.132327 val accuracy: 0.120000\n",
      "lr 1.000000e-05 reg 3.100000e+04 train accuracy: 0.132469 val accuracy: 0.143000\n",
      "lr 1.000000e-05 reg 3.200000e+04 train accuracy: 0.135490 val accuracy: 0.161000\n",
      "lr 1.000000e-05 reg 3.300000e+04 train accuracy: 0.115388 val accuracy: 0.120000\n",
      "lr 1.000000e-05 reg 3.400000e+04 train accuracy: 0.147918 val accuracy: 0.155000\n",
      "lr 1.000000e-05 reg 3.500000e+04 train accuracy: 0.090694 val accuracy: 0.090000\n",
      "lr 1.000000e-05 reg 3.600000e+04 train accuracy: 0.091694 val accuracy: 0.076000\n",
      "lr 1.000000e-05 reg 3.700000e+04 train accuracy: 0.119408 val accuracy: 0.109000\n",
      "lr 1.000000e-05 reg 3.800000e+04 train accuracy: 0.106449 val accuracy: 0.100000\n",
      "lr 1.000000e-05 reg 3.900000e+04 train accuracy: 0.117020 val accuracy: 0.111000\n",
      "lr 1.000000e-05 reg 4.000000e+04 train accuracy: 0.098878 val accuracy: 0.081000\n",
      "lr 1.000000e-05 reg 4.100000e+04 train accuracy: 0.075122 val accuracy: 0.072000\n",
      "lr 1.000000e-05 reg 4.200000e+04 train accuracy: 0.139714 val accuracy: 0.142000\n",
      "lr 1.000000e-05 reg 4.300000e+04 train accuracy: 0.078061 val accuracy: 0.099000\n",
      "lr 1.000000e-05 reg 4.400000e+04 train accuracy: 0.113082 val accuracy: 0.113000\n",
      "lr 1.000000e-05 reg 4.500000e+04 train accuracy: 0.082184 val accuracy: 0.089000\n",
      "lr 1.000000e-05 reg 4.600000e+04 train accuracy: 0.105122 val accuracy: 0.114000\n",
      "lr 1.000000e-05 reg 4.700000e+04 train accuracy: 0.072796 val accuracy: 0.080000\n",
      "lr 1.000000e-05 reg 4.800000e+04 train accuracy: 0.122041 val accuracy: 0.115000\n",
      "lr 1.000000e-05 reg 4.900000e+04 train accuracy: 0.082490 val accuracy: 0.107000\n",
      "lr 1.000000e-05 reg 5.000000e+04 train accuracy: 0.111816 val accuracy: 0.105000\n",
      "best validation accuracy achieved during cross-validation: 0.347000\n"
     ]
    }
   ],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths; if you are careful you should be able to\n",
    "# get a classification accuracy of over 0.35 on the validation set.\n",
    "\n",
    "from cs231n.classifiers import Softmax\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Use the validation set to set the learning rate and regularization strength. #\n",
    "# This should be identical to the validation that you did for the SVM; save    #\n",
    "# the best trained softmax classifer in best_softmax.                          #\n",
    "################################################################################\n",
    "\n",
    "# Provided as a reference. You may or may not want to change these hyperparameters\n",
    "learning_rates = [1e-7, 5e-7]\n",
    "regularization_strengths = [2.5e4, 5e4]\n",
    "\n",
    "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "\n",
    "for log_rate in np.linspace(-7,-5,21):\n",
    "    rate = 10 ** log_rate\n",
    "    for r in np.linspace(2.5,5,26):\n",
    "        reg = r * 10 ** 4\n",
    "        softmax = Softmax()\n",
    "        _ = softmax.train(X_train, y_train, learning_rate=rate, reg=reg,\n",
    "                      num_iters=1500, verbose=True)\n",
    "        y_train_pred = softmax.predict(X_train)\n",
    "        y_val_pred = softmax.predict(X_val)\n",
    "        accu_train = np.mean(y_train == y_train_pred)\n",
    "        accu_val = np.mean(y_val == y_val_pred)\n",
    "        results[(rate,reg)] = (accu_train,accu_val)\n",
    "        if best_val < accu_val:\n",
    "            best_val = accu_val\n",
    "            best_softmax = softmax\n",
    "\n",
    "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print('lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy))\n",
    "    \n",
    "print('best validation accuracy achieved during cross-validation: %f' % best_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "deb37cc6",
   "metadata": {
    "test": "test"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.319000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print('softmax on raw pixels final test set accuracy: %f' % (test_accuracy, ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df501314",
   "metadata": {
    "tags": [
     "pdf-inline"
    ]
   },
   "source": [
    "**Inline Question 2** - *True or False*\n",
    "\n",
    "Suppose the overall training loss is defined as the sum of the per-datapoint loss over all training examples. It is possible to add a new datapoint to a training set that would leave the SVM loss unchanged, but this is not the case with the Softmax classifier loss.\n",
    "\n",
    "$\\color{blue}{\\textit Your Answer:}$ Possible.\n",
    "\n",
    "\n",
    "$\\color{blue}{\\textit Your Explanation:}$\\\n",
    "加入一张新图片，若该图片正确分类的得分满足$\\forall j\\ne y_i, score_{y_i}-score_j > \\triangle$,则该图片的SVM loss为0，不会影响原来总体SVM loss的数值；但Softmax loss不可能真正达到0，因此会有影响。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ade33adc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjwAAAFrCAYAAADVbFNIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAACR80lEQVR4nO39ebRu+3rXBT6/2b7tavbe55zbJIESitAbtAApUSIwCF0kFQqBwkDQWEUVCVAODUGDFTUaRLGhELUCitIlek0hEYaDwQgq2ICCaJFUpUiK2+R2ZzerebvZ/+qPte56PnNl7bXPPfddZ9/z3u9njDvuPGu/a75z/ro51/P9fZ8nxBhNCCGEEOKQSV73BQghhBBCPDR64RFCCCHEwaMXHiGEEEIcPHrhEUIIIcTBoxceIYQQQhw8euERQgghxMHzvn3hCSF8dQjhx173dQghnBDCR0MIv/SOn/8DIYQf3se5hBDvnhDCHwshfOfrvo7Xwfv2hUcI8f4hxviXY4xf+bqvQ7x36IVVfLGhFx5xMIQQstd9DeLzR/0mxPub98sc/qJ/4bn+K+H3hBB+KIRwFkL4D0IIkzs+920hhB8NIayuP/u/w799Ywjhr4QQ/rXrc/ydEMKvwL8fhxD+aAjh0yGET4YQvjOEkL5X9yiuCCF8eQjh+0IIT0MIz0MIfyiE8JNCCD9w/d/PQgh/MoRwgt/5aAjhd4cQ/hcz27xfJt6B83Nvz9fbEvRd/RZC+IYQwseu+/qffY3XL27x+c7NEMIfN7OvMLPvDyGsQwjf+lpv4EuYEMLPCSH8jetn4/ea2QT/9qtDCH8zhHAeQvhvQwg/G//2oRDCf3rd538nhPA78G/fEUL4SAjhT4QQLs3sG9/Tm3qXfNG/8Fzzm8zsa8zsJ5nZTzGzb7/jMz9qZv+AmR2b2T9vZn8ihPBB/PvPN7MfNrMnZvb7zeyPhhDC9b/9MTPrzOwnm9nPMbNfZmbftPe7EC/l+gXzPzezj5nZTzSzD5vZ95hZMLPvMrMPmdlPM7MvN7PvuPXrv9HMfpWZncQYu/fmisU9vJP5aoZ+u/7cv2Nm32BXff3YzL7soS9UvJp3MzdjjN9gZh83s6+NMS5ijL//Pb9wYSGEwsz+jJn9cTN7ZGb/iZn92ut/+zlm9u+b2f/Jrubbv2dmfzaEUIYQEjP7fjP7n+2qv3+Jmf2uEMLX4PS/xsw+Ylfz90++B7fzhRNj/KL+n5l91Mx+G/77V9rVy81Xm9mP3fN7f9PMfs318Tea2Y/g32ZmFs3sA2b2lpnVZjbFv/9GM/tLr/vev5T+Z2a/wMyemln2is99nZn9T7fGxz/2uq9f/xv1xyvn6+1+M7N/zsy+B/89N7PGzH7p676nL/X/fYFzU/33evvuHzSzT5lZwM/+WzP7Trv6A+NfvPX5HzazX2RXAYKP3/q332Nm/8H18XeY2X/9uu/v8/3f+yX8/wkcf8yu/qIYEUL4zWb2T9rVXyBmZgu7iuZ8js987iDGuL0O7izs6q03N7NPe8DHklvfKR6eLzezj8VbEZoQwltm9m/ZVfRuaVd9c3brd9VXX1y8cr7e8bkP8b9jjJsQwvMHuDbx+fOFzE3xevmQmX0yXr+lXPOx6///CWb2W0II34J/K65/pzezD4UQzvFvqZn9Zfz3+27dfb9IWl+O46+wqzfWG0IIP8HMvtvMvtnMHscYT8zsb9lVyPVVfMKuIjxPYown1/87ijH+jL1cuXinfMLMvuKOPTj/sl1F435WjPHIzP5R+/H9Gk18MXHvfAXst0/z90IIM7sKs4vXz7udm5qXr59Pm9mHsX3D7GpOml3167+E595JjHEWY/zT1//2d2792zLG+Ctxnvdd/75fXnh+ewjhy0IIj8zsnzWz773173O7avynZmYhhN9qZj/znZw4xvhpM/sLZvYHQghHIYTkejPeL9rf5Yt3wF+zq8n5+0II8+uNrn+/Xf3luDazixDCh83sn36dFyneEa+ar3fxETP71SGEX3i97+BfsPfP+nTovNu5+Vkz+7ve20sVt/jv7Gp/6u8IIeQhhK83s593/W/fbWa/LYTw88MV8xDCrwohLO2qz1fXxoJpCCENIfzMEMLPfU33sRfeLwvKn7Krl5L/n13tBxglTYox/pCZ/QG76tzPmtnPMrP/5vM4/2+2q1DeD9lVSPYjZvbBe39D7JUYY29mX2tXG8c/bmY/Zma/3q42oP89ZnZhZn/OzL7vdV2jeMfcO1/vIsb4g2b2269/99N2NQ+VWPSLgC9gbn6XmX37tQPon3rvrlh8jhhjY2Zfb1f7WF/YVb993/W//Y9m9k+Y2R+yq/n2I9ef+1yf/2oz+yoz+ztm9szM/ohdmYLet4SxtPfFRwjho2b2TTHGv/i6r0UIIYQQ70/eLxEeIYQQQoh3jV54hBBCCHHwfNFLWkIIIYQQXyiK8AghhBDi4Lk38eC3/L4fuAn/DLG/+fkweFSoH4ab44j3px6Ro4DUDGnqn4mDn7Nt2pvjuvWf8/wJbP8J0gp0vX+eWSCSZFwOK03TO/8ty4s7zxvx3X3vObeSxD+T5Rl+ntx5jOayuqr8dzM/z2w68+/Fff7f/6lf9E5yCb0j/qPv/OdvTpwk+c3Pd7utfzdegctJiWv1+2wab4tdXd8cF4W3Y5b65ye4t5D67WzxvUP0n8+n/r1t6+NiV/l3Xf2OtxO/I+J3UvOxUeK8HcZYhw7KC2+Xrvefd9HHQovfzTCeS/wu798GH2u/4Xd+817689t+79fcXFxaerufHp/cHHetX/9mvbk5DsGvP8/82uZzzAPcV9NhjiP3XIr2j+af4VgZDSjMubpuRveTYewYfr/DutChX3scx57X4d8xYH3Z7fz+E4yJaeHfxTlYltOb47bjeuf3822/+3v3Njf/3d/662++JKAtC8zBiHWnrXY3xynaOC9YZhBrJMcv1svdeoXf9T6YzeZ+fs7rDHNo49fQ9uOKLgOOk+DX1w5+TbxWrh1XLurP3cHd89Q49/GrKcZO1fgYY0eF3MdIj/P/ln/1j+ylP3/dN/5CPPz8lAHr+uPHnmJqyTnbcb3z9u3QorOZj9Npgf7APO1aP05zXAPWXz6v+YzO0/FzM8MUXm98vPSR3+HXUVW+rm9Xa78fjhfMqen06OY4wfgKGO8JxmxEGw34eYr2/Q+/+y/c2ZeK8AghhBDi4NELjxBCCCEOnnslrRQh6CLzuCFlpoGSE0KXDGlSAiMRmcopN+WFf57nDwlC1zhlhbA3N2Ent97nJhMPoWa5yw8DQsJMwE3JLSICz7NmgdLY3c3JcH2J72UrFaOfPwwN2inP/S5GodOuwbGH9XPIQeXcQ6pJ8HNWrUtODIN2jZ8fSp/VWw999ghN5ubfS7lyvXJZwswsRoyZ1Nuvh1zStX6cQIrjBaaZ/25KqRShdY7PFiHVtnGJ0iLksMTHWtvuTfm4YUA4+Wju8sMU4W6qDAVkr6FDu3feZ+ybHCHqBG1FmWha+M+73tuEcl7oIXNjHVhMF6P7oVzRQj7Pp96mTeo31CboV8zIFcZI2/rnB0hxEW0XAmVuu5M4QGKJD/M3YkSb9WjLgPFf5Fi/IGVw7Jelt+sAqW/gPEA7To9x/xjjWenfVULqswRyI/psgDxtZpbj+rh9YOjxvMC5EqydgeNn4FYK/90SUmSZ+XWk6MQAyaypMU8D2wXbIfbE8enpzXE12sKALQ+430ipF/Ox6SDX4BnSQA7iY4PPmRbnGcnHaCvKtoZnd4PxZ2YWIBNmnI94bgx4lk0WPga55aHHloQB92YJJTe0C57xSUYpFOsOtznEV/elIjxCCCGEOHj0wiOEEEKIg+deSYsx3oEOKe7i5sb54W7JaSSB8fQM8UFWsMQ/lQc6vOBS6HhOvLdxV/wtJW0kXeA4M4bs4UjBuZIS14FzRoTR+s5DxRmcABQ0KC1w136KsB7lin0yUHLkcWS7+v0EOvPg7EnQDzbF/SBcOgrZ8vwIIdOn06OVVlsPlRqup8IufzOz7Q4hUsoADSQRtGWceJ8Xc7gKEI4PO8hkCOvTwVLQgTXSQTD2EK4fHqCocDlyxFFaggTCzyAMXFfeVslokkB6wDUXpbcV5ZYkRXg887ZKE8qLfj01wtvTOcLpZpai7UpKHQj3rzfu+IgDpCuEtSmNsi1SSPKca3TsJHBr0hHFxSwN+5cnzcxmS3eqdFhH6FLsjdfq7bdYnvjPsaRXO58vuP2xlITxS2ctHZDDyIGHfoLMXZTjRwnbe4v+YfPR+RuxvpQYb2kCaQxrQRJ5TX5OPkdKtBefQZShbzs/98HwkuWbDuDd1q9hiH6PDdYuOg5DUtz587b3cy4WmFOcgxhPg/n90jHNcWC3cvONnJnYbjItsPWg4/YUSmD+maJjw+A6Cr+flFseKn9CtK2PZXzcUrRFUbx6W4giPEIIIYQ4ePTCI4QQQoiD515Ji7vHmQyPYWaGuyJtIQybMuEfY1YIo9E5MUrgxggyfjVFiDYgeRqTBebp+H0uoRQRGVtFqG1g+A8hVFwrFZ0GEg2/LUkpn40TOd11rQ8lY5GLy8ub4+Xy+Oa4gHQzmSHRGa1WaMsM4fTZkTs4eB7KLKsL/97d2kO51drDmhuEL1OEugMSvbX9uD8ruK4izlVCmlgixN1zPKB/6q1fX7326ygLdz/VA8Ly6M4pwvocquUM7VXsXwaZwwlRwn24gxulhUunHSD5YahFhMST0Tzw80wGOh/xEczxAmNiCgfWRXWB30Cyy2o83gskussLjCn0X90hBN/SBoiQONaFIvN7LuBKzNAAaQJnB8YEHVGGJG45EqM9FHHkBKPkdPffpzVkQyau22w9SVwJd02ec0sC5Dqsjzs6HSElT9htkEPpGjMbbzkIYTRqbo4aOMfoYAqpj6UUP5/QLYQHQxaYkJKSvH/r6ArQRttbzs990MNF1eE4w3Nwt/N77/EMTbC49MZnmY/HkesN90h5K8NEmE18bu12ZzfHdeNrWtuhj4vx8+plCWh7PBPhgRs5sKZYm1L0QoP1ImKN5/gdoh83o2SDkPHwvEpzJq+8G0V4hBBCCHHw6IVHCCGEEAfPvZLWgCA9jxM6s/B5yj6jRGcIwVF+orw1qto+SvR1d2g0RcalGO52aRS3pKQM4XHWGolMUIbPs77XqE4W62dh530xRW0YhAEppQ0vSdrIhG7hgZwgrHdTtx6EpIQYEEKu4KoZUAMnm/t5WL8ljqQbOLMQpn0Bd8KLnZ/nYo3aQAibMjFcHm7twkeotob8lCERX4e+qunyoxugQNtDTsnhFlmdeVtscQ8YOrZcohYT+p/1oPZFCgmBU4fuQzpTmDgxhbxjCAOPnDkDa/pA5mOSO7je0pQuKPQT5myHvqxvuWNq1P2aLljQDesOkjmGABkAY3bGemBwe9YVpIuBLjWuRx5mn8/dNTXJ/OecB/uEMgXrR1GWoVxDB2mb3p14tYdkxIR2Fign4MeYHz3WR8PYoUQT7lbGrj5H2RR/Vxdw9tDJmKAgFpMw8jyspVZCNqUUS7dm10M2Qp+PtjO8g2R1ny8Rz50Z1qUpEv3FAMkQlxCR7DQr6fpErUGsM7z+Gm4syn8lHJrVBvMGv1tiyi2mY3kyye52TbKuXMnnfcbnAyRTOvzgIK2wvtCtO7IW4px8tlDGmmLOvgxFeIQQQghx8OiFRwghhBAHz/2JB5l47yWvRgz7F6x3wXooTEKH8GgDPQAlVqxu75alctRGSUaJA3HJCHve3rVNaWlkVEjhEBplsfL7iS+pezVEhNBHEhrCkTiOL5EDR4kQX+Lq+kKZlh4KZUKvGsmuBkiCO+yGDwhBJnTtMEkanAeGn/fo3AFOnWK29GuDXJEh9Dsgpt3txo6KCqHgSeEh7nTx6OZ4jXovKeSIduvX2uwgpyC0Psu9vbLcvywb/B4quAdyOFuY0KxhLZs90Y0GEsLAmF9MEsh5FBKOL8itmI8MoRcYN8sppMOEIWrvv57jCfM6GWkgY2moh+Nuu/IEgx1lHIzZfkCiM0zZyRRySOMJJXcYgwGLWTbxsTaZeUg8Uj6lQzN7dXKzd8MRXGpbtE2DenEbjOUeWhfUwJGUesxkhqhzF3D+DHWyWpwow76FUbJAdCgdO+mtv51TzOEWbqAc43CC5J+s7cd6gxvI3pQ7KM9TZmH/5Ph8j27bMIHpA2wfqLA9IYesmsJlN1tA0mGyQfTNFPLsZMK6lnA7QbbcrN1tmsE9W7OWH+c+ro2tkGXjNmlrb682QLouff3uR4lWuf0DNdmQapaJPRMkLQ2Y70wIm8MemKHPMjrIZq92UCrCI4QQQoiDRy88QgghhDh47pW0Jjl3ZKOeBsLmJcJraULXEUKOrJMDmSmBqFNzOz7C43lOiQmOD9gLcjhWRq6LfBx+TrDrO4kMCTNM67+/KOEEY5iSch3Cax0TLzIBIpwq/QB5A+ekCy48UNh8tMMe77oVrpuOuoq1XJD0rVv5PUyDh80b7No/u3D5iYmv0umJX495yPIUIViGw1989vnN8a4dO3t2HcK2lX933njb943382zm46rDfSawKLSQLi8YTkeouZz499IJQxmkQXuF3O9/X1B+yJn8EM7HPOXP6UZiLR7MO8h5A+oNcWqy7loIdCtS5sW1mbfVyIEyGY9xuo7oLqshGe7ophw4B/3zSU4XkR9PppTD/HvbgWucX0MNeXaA5BDu3wTwrikovyPnYZbAgYX1soFM1KON+t7vZwlJq2khmRe4Cax3VedztoNTLGItzyFRLCAnTCENmpm1TCaHZJjcGkH5mIkhS6x/Aa5AJnmd4HcXmNcpOrdtvA9fvPCEe9ae+/cmr05W9/mS4Lm5gzuuw1p0OmfCPMMxJCPWeUvxbEWSQCYzbAc+T+n083W24zMHrrwUEub5islCzTpI8rP5iR+jflqFOb+pWCcM/UFLK6RaelhrrPEV1qAspzOL7j5seZm8eiuIIjxCCCGEOHj0wiOEEEKIg+feAG1BCQk/TxD+4o7uEon3Koa1EBLtUOZ9QnmLdVywmzvBbu7JDDv/WcML4TvWsLqVC8sCrxuvelSZmNwro3SF4wIh4VFixBoheyZrQrivQ7KnDGFcJjDM0v07B8zGSQVZByVFOJqmhZw5uSDLJXAbdLBKXaBvn16ytpV/fonkUEnGRFzeXmdnHlp/8dwdO8MwlkGGzOUSjp8Id9ViCTkJDrST2RP/XTjKutq/L2NWtgbhZdRfmmEM50i+lcO10N0eiPsAbomi9HtEpNh61NJiArCmwjH0qmLh/ZFjLqeQ7SqM8RmkvQIyZI92W0y8HVoMqD651SiQ4mo4LTu46Tr0E8ParAGVJHCaQZKdz/36KsiBlN94fjqZAqX3WwkT98UAKTVAymi3Lkc0CPf3tzP9XTNqFyaYxFiokVB0i0R0OyaexK/GkbvIx9oEc2t2K+nb6hJOOzjKOsgaUGZsQifRlDK0uy+nM7831rArkDg14rnTQEqjA3Hk9oX8vS/mvDaWasMzgW1HZ+xAZzTGdYP+KyFvzdE+bUTfow5VAlnt4uL85jhmlHwxh4Zxm3R05uFZe3GJc+V+z6xBeb7yz+wuvcOnWLPGc7/Csc/9HPdpuO6Ituh/3BP/x6MIjxBCCCEOHr3wCCGEEOLguVfSouODTqs8pVPDP59P7w6Jb9d37yoflZ3HbvmkgFzFWi+sN8PwMxOJ4XrCLWkoUn7C/RR0p0C6iQwDox5SO/A8Tp4wORTqkCE0lyLsxpAla4P18WHeQ1lrZEC9o4ikgnS8rBEe3rV3J5PrIHFEhFQjkhZe7pD8j44lNF4FV9c5Qp/P8PP2ljaUITS9MA955rVfRwZXSbs996+G+2WBsUrHS0F3XYrYNOryTAq4+hYYz3AEbnYPUEsLtasiE1tiQka4lOi0YP9lGHcJZDLWximRXDRWLrGwbh0TLRYBchjmAWVh1hIyG4fyV1tI1BynqI1kBZxgqBVE99IMTrBmy3p7fhquHXSRhFGSRD/s96+AXEF35M7n3bb2scOErKzzR0knTbmO+Ol7OHiYjLPihzAP6IJiMssUa7xh7OTzsUsr6yBH7HBvlV/3ao0+gcOG9dBazLWWyRM5H82/e3Xuyfea7cqPMa8DpKXuFbl33w10NBdTOM5YGG3kIETb4RkV0WehpzxJx5a3wwdOPOFqD2l+dYkkq1ivWV4sR/K/fDJuEyb6S+HG3K59LZgdYx3Ec32HsdllqN/IrTCQ0mmamy78mqZMzLk+vzneYK7QQfgyFOERQgghxMGjFx4hhBBCHDz3xvNGdWPgiirgBGDot6+RbC+j64hfw3cs/90pavQsT/y4Z60mSEPna/8uuhcC4tWxG8efU4SvWdNlQM2hfrh7x3yLMGWLUCzPycRt25Yhe+wqR+iaRosWzpThgbKb9UgweHHp4cji+OTmmJH8Te1tUSNBW9si7BqRWKtgEqjjm+MJQpMJpJizc09wxXoyu87vfzWg5he1CDPLa0o5fk2Pn/j3zZdv3hwPcFRtLp7dHIeBDjTWnfHDFD9PeroWIFHCwJOhc7OBHsf9kLI2FqRbSlSMoDc5ZSLMU0hXsyOXBRMmDNsxAZiffw2pboskjUcTb7gBUhcT2CXZuE02FevqQRpmQj6MNc7TBqH/HaQeJlhcbSjp+fUVSC7asUYclr48h1SfP8zfiAOyDY7KjCXe9pG1iCCJBPRJDYlqVJ8Mc7ZnMr/e23QH182bszdujjO4jlJIFHTa7dqxRGloV0OCupE8iGtl8tcC35dgfa2wRtZwYAXI51tIl0zOeclEkpQ001cnq/t8afAFKdb7DOt6A2mvLCBpYQxO0U8Dkih2GOMZykcVeN6xBtlu45+vIBdz3JQnvt1hRgeVmRVzJFGFHr7bcQ1lslHvjyW2HUxP6aDG87uHXQ8uyzmepxFW6h5yKV2W1e7WGLwDRXiEEEIIcfDohUcIIYQQB88rJC2EyvFR1rcqSg9ZNZWHlOh4mEzggqEEhnAUXUoldq1niMsP2FZedy7JpEgElyV0KYwlLSYAHOU9gwtlO0CiwUfGSgd+GddEV9coxIt7oIsiQGJqcTyZjkOKewPXwabpWSsIiR7TeHdyt5r9DPdPH5Fwa3ZyczwpPVz6/LnXxnr2trsomCTuYud9cHnpoeuiGDtBAmQd6+Dyee7nfRv9djRB/7OvLt0JdrpwWeeI4fupN1gKmW2Wwe1VwJmHdhz6/UtaBZx1GeLaGerKUdLI4GqiuyoiQWKC3612Pr8G/l0EFxQT8i2wDmzxvTskoFudeQg8L8dSAmsm9ZAB2I4ZaoPlSP45csKkSMSGZs+mS/9dfFcL5xeTf7Je2hTtYvFhbFpdgiScSBCazRDixxrZIplcgs8HSIAt7qdBm24blxAixkUHF9gayQlTZLMspqgjByl12/k8NRvL9R1ko4AEd0w8m1Jag/OIpRRTfB+Tze6QwbCDVJLhuZNCWtnA+Vk/QOLBDLJ9gnUtxzO0wJjKML9YD4vP1sC6jnjGsRJYtfb7opOLSVBPTjzhagsJLOJx1WzGyTVz1P3qcd4pn9OYp9sLXge2AuD+a/RZPSAhIbY/cOtM1/h5jhY+l3vcQ5Zgnr4ERXiEEEIIcfDohUcIIYQQB8+9khYlpFGyQUa4ER6tkNAqR7h7DvdOhmRgZysPfa2HpzfHdFexzkiPndrNxkPlCZObIdxXZuPbi9jdnkJaGxAq3m08cdUOSb9YMysNSABW+T1Q9ktQM4tB0wHSiyHBYAupoMj27xwwM4t4vy3gwIuQpSYIWU9yWgBQK2WKQCoSUa0Nx6jXVEOimiJM+/iRJ8qq6KLp/P5PPuifWc49lHn13eh3OO1KuIFKOAdT9OExwrFMQvnlTz5wc3yEBFxbjKscHZqbh/LTEgkm0UYDJJR9MZ97O2ajuD9qbCGkP1nAOQW9gWP8xYVLgUzylkMyKXHO6sJddgMcMR36fnX24ub4+ac+fXO8xVphZnbyyEPtiyVcUXBqTBfoA9xyiTVlduxy8Itzl+Ui3B+8nxSOMDpNEs4PjI9RQsI90iKx3EBnz4RtASljgIRYe1teIDEkE8dWTLwH19QCCQNnR6c3xz1diVhHI87Z4jmQ5eN2qTCuKC1RXlmhrpNhzbtEfT6D5JjhmZKz7hfWNTrHJqwNhzWuW/mcrbr9y83zKccsknZSfsEzq0YNwth7+1yu/BmX41mR4wHMRI50L7EGZYk1t4Bbr4ZsvcGzuGvG61V96esCnXXHT05ujqdItJrD0U1ZebejTOo/p4ubMtls6vL0CrUZmaS4gJyfJq9eZxXhEUIIIcTBoxceIYQQQhw890tacJcEuCpYr6ZGYjEW6SlooNl5SKzD7ux248dN7SE11oapUftjvfHPNChBP8Eu8oFJCG9FK6dM1oU734zqi/g1hZfUNRmYnLBFbSE6OHA8KgGFcGSJ2h8DhK+ERU72yHTmocAa9a0KSFcn2AE/DjV6uDuduOuqQ/j9sytvi3kORxhcHjUSBF6e+PXsEMZO4T2YT/x6kmxcK6VAzRaDwySPqDOEJHux9RDxMZJpnc5dBnk083vrIQ8MCKP2jY/DHmNqB5dEiZByUry6xsvnyxQyQVZ6GJhuHMoSRUGnJMYm/ubJMHwHyAcd5vWAdmY4/WLtc7ze4URIWthCnu5Yw8nG0loxg2OJUwGJDneQaBbQt6a5S7KG8TJg3cmY0Iz19Zgwjsn/OsrZDyNpMVFcT3kW6y4dlEzc1kJyu4RswK0H2cjlA6kW6hETNc6X3o4lHIH9S5KobrpxUtAa/82agR0kJCjXo7Wmx3XwtAFJH3ccbxg7c0h0DcYYk7kO6MOmZ0fvB9ahSzE2d43PhYHOWEjz1XaNz7hEtYS0GeloxJYCOl2rCnOcbmDWlEOivi0SOY4TBZs1kJ8ZIelwfdWUzwpfT1PIoRNsBSnRLnR4cVvIFM+l8pRz2e9thnUweQd10RThEUIIIcTBoxceIYQQQhw898aA+hYhOLgWOiYW23kYP8UO/gVCTQNC6E3tMkFDtwiksQSSGRNYbRpIDHQywS5wBPfKrBiHKwNksAY72luE5uhGG9XxQliQb4mLGZIy7c5vjiuECAvWJkG4OsBNNENSvTS+uibIu4EhT9aPKhH7foxw7BKujcdvfNnNcc90VwiPf/DU2/fswvtqhb69hEtggfpBayT2W0x8d34B2Y9ymJnZ8olfX4AdYMqkf5BK251f6wdOvNbXWxgzLWTToccYg7OlggQaIC1wTG626P/ZLXfZHmiRAIwiSwk5mLW0WtaegtTVMvEaHEt03w1IwdkgIR2ULmtwnrNnZ34aSE8ruqaGsQRytISMgVpqa8gVJ+bHDGW3kCV2G7/WDUL2BSZtBQclnZKzglIqQuVIesZaR/tkzTpRkJtPUqxncPy0kBMp12SQgBvI81xfzSAFox8mkKEfL7yW1hJzZYNtCJGJAG281nao+9VjzNR0i2HdwbJgLZy/KaTkhNL4AKkI568al4Ra1uHbor0oYyWvlkE+X06P4XZLIV1x/W39eIbaYf0EWyrwzMkwZilLRUzCsnAZkk7MAe7kYeNzsMUWhAIy4u0tFbs1a+lBDv2Muy4Nz8Fw/NjPW3pbL5aoq4UEr1MmxMUcPJr7/azhyoYZ2Noe12bjxLR3oQiPEEIIIQ4evfAIIYQQ4uC5v5YWQlspd/MjvJaihsqAUH965HGnFKH+BR0SCKGuzzzh39kLD5UlCDkGSBoByY2Y2K7OmHxp/D4XR7KUX1+C856eeBitqpDgCaHcgF3idPUYkoGdn3tStg7xWjphJqwVEimTjMP9+6KpvK/mcO1wNzzdGYtHHnYMmfdViwR1dFSluLcCiegWJUKtkPS6lff5FA6ex1MPg54c+7U9rcZSX94zkSSkDzgaVluMSYTdc4SC2y3cf1tPlNdDljS68fBz5rpKkHwtg7w7nezf2cOkctS0AhKaFbg4SgNTJOtaB++DfksJCM5HSCx976HlS4S6tyufgxvU4mnwuw2koa4aWyjnC++DBMkvK1gcKzjE5riHfoDLpaGTCzXsZljqoG9xLhvWgRwSZopEhUOz/0R1ZmarNdyrkLdK1AlLMEfWkBB3kGguIOMZxkJEYrgU8nmJJHlvPHY5bHnqMlaOWoPGNR7yQ3orWWoD6S+kmKcYV5u1zyl2Q8dks1v/zAT30MGFFI0JKSHpZr6VoMOaOj/2ZKZt5+N/X5RYfzrIsIFaDNaKEsc0AfY7n2uscxbx/L0883mzMv/8lH3G+neQFFO0SQuXbJmN5ckU2z86rH3LI/+OKZxmGZJFFpDJ3zzyeT175ONrhbWY2xZabEfYrP15ytp2A+o9DvHVjjtFeIQQQghx8OiFRwghhBAHz72SVg0ZJ4PkYgiLtWt3ZESEwi5XqFUF6WGKejA7hC4byAqxgkMEYao0Zw0nhO6RqKxFpqrLz3gYzMys3fr9HC18Jz0T700mJ/7dCLXtdgytorZM4t89Qa2nGRKanT997udEIrwWob8Orqke9Zn2SQ7Nad3CpYb6S2+jjeqAWkm1y1inyzdvjuczl70WCCe/derhy2309j3H+c8n5/4ZuPeWSDA2hatgjgSBZmY7uOgiXAYFhnWOGHGLUPDZ07dvjhvIYZRxKS1kcPMUSKx1uXEJbAPnTIZEXGU+vu69gORjCWrBjeoewa2Wwr2VIiS8q/x4WXhfZvj8Jz7xCf88JJ0NZKmzS+8nJpHcwVFiSASXYayYme3gKsmQ6CzNfX5tIVclF3ALoV5aRHLNEk4Q1tsKKdwvGSVzv57AxI5QmLs4dgruDaxzATI+lAzD8LIKMlaNOlw1fh5wb0nhxzWkvildMY/dmTWFQybg7+IcheT6UaLGsaSVY45Y5utZoBuogeSI76CBr4ELmK4uQ9LDlPIQHJEp3J5T1JLratTkKvYvUSL3rY08fVhDMqy5AySjHHN2hxOdv/B1huffoi5YRJ3GLZ7FGeTFHgMqGbkV8ZlmLA3NII+1kKhZ02uCrjmFm/AR6m3NUZtwGDAmsNUgQb8yUSmTgraQORvcc5m9ep1VhEcIIYQQB49eeIQQQghx8NwraRUIfdMUwtek+cJDn00ysnLdHE6RPG4D6WJF5xMcJUdLD0VWcBZ95ulnb44jrCnl3D9vz7HjG3VJzMx2kEcePUKiOsgyl7imAcndEjg1UtS36ZCUbYtwf4/kgdOF339Z+jHrwQxwmnTvYLf5u4FOiueXHiI1JOXibvgSybGeHH/g5njxITg4YDtLWQMIdVNaOEeWA0LoR37OXY/Ekwi19pW3UQyQNM1ssfQQZot+eA6HXAsZpGM+rR2SXUGuXF888/uh8wSx6a5yZ8flpcuVFbrt5LGPqeTD7grZFw1C0xlk3Bpun3wKJ96cshfcFTMkScM9rhg2hkzSDH6e5xferys4tuh6TOGISjHGs2ScwI9JGzsk0ouUJCHF5NGvo0jh2oDdJ6KNApLt8XjKRKVwTeYzSlpYB+xh5mYGRylrTzGhH+tKrSENMwllHNWhwhoHCXSGcVFMfP2OWINhxrIca1+J+nUVas0xeavZuD5Ug3VxjrqHs5nP3x4OTNbny+B4yqeQqwpIM5BsJkjySmdimtH9Q4ltLK3ugwhJl8kZc8iKCcZ1gnVtCL4OptO7t0tkqN/XVX58vvLtJSs4s47xzGHiTI65kSyYjNtkyT5HMl/25cmRX+sMcj6MdZZA9yox7wZKWhg3CcZ1jzE7QMLeXvozPmSv7ktFeIQQQghx8OiFRwghhBAHj154hBBCCHHw3J9pGf86YF9FQl0SlvCycB1wAitbDp10wEmT3PXjHLbRcu7nfPacFlfXXhtovhMUfEyhYQ+3MhZ32MTx4tz3sOTQw996w/dbnJz6/gbadANshBHFIw1tNMByTusr9fkEGYjXZ77vJCse5j00G2Df38Dim6O4HPYPdY235apxffhjlz98c7w79naM+N3F6cnNcdO7kFvM/Ocl9mohMbOtRwU8keEY+0rMzFLotw0K3T575vtqehRoZUG9Et93+bZ//uLc9/Ccnfk9d8gonsM2vzjCPiJsdEPNS6u7/fcni9lyW0kY2YOxb4WZlqfe7gNs1k9R9PPZC9+ntF7Tqo8Muujvaut7rbj1aYmigtZxX4fPOTMzw/wql569fFToFPtz4Cy2ggtVxFhm4VgMHVrO69zn7DEyPDOba469BO0D9KWZWT0qBsoix/4ZZrzdIrt8jmstSp/jdYeUDjO/t+NTX+MmyOQckRpjwPFi6Xv2GmzuucT868dbsmyCPVBL7rEc7VVCu2Ju0n6+wO9mKKQa8bsZni9Z6vfPPUlV72NhC2t1dWvv0T7g/pwCVux8yrmJPXUJ7dookI2imuUS86NAwWJY+zvsRQxIz3CCPaoZ5hPHUM2UD7f6kukH2sHba+BzDeNox+djg+z+SPmSouJCge+bYZy+WPmakiMlDd9FZhPu61SmZSGEEEIIvfAIIYQQ4vC5V9LKEB7O+GqEMDiDSLR9DwgVFgghD8iOS9t403m473zj8g6z1+YIX/VIN9kgnH584uFwFsYzM8t2CBUjLJZFyBWwk/fI8nl55tedQ8aaIKPlHLJcByt2j9Byh8yetNzynGmHVJp7ZOj9+hbzE//u0i2PtJd2eB/ebPznl08/5T8/9/BwCdnkpPbP1wjZJhfnN8fxuX/+Yud90MIDPsBeGW9JlIy8cnxukLU7Qz9TjZhAfto+d0lr9cLH3tlzWNQRjX70QbfT9+Yh9Aoh4tjABpwjpL8nmAWcx2hqK5CBuUTYf1KiCCPmY4vs2w36b3XuUld96e0zySFhLrwdWBSS0l4Jy2mJApZmZpMCRXiRXZkyC63FBdaCl8l7lLpyfDfHTY/styyqmSPr9AAptR+Y22CP4Ppo6WYhzcBrxYCcjOzLLglsKqwvkJI6znH8vEGRzzns4Cnkl7ZFkePEv6ssxpZgFkCOaLOI1p8iG3ODjNwZ+m2CMQxH9ciun5X4bshhyGJgA9qrxnhpcD/7IgZm48ZzA2sOM+4nmEfN1i96hXQhhjQMGbIxl5Crjp/4dxV4tiwLyou+1u/wjHr27OnNcbUdj3Fm6U56P1fAtpWecxaaWHnkcuiA9BHM9k1pjP1aYp3ajQqa+r3NjpHCZvvqvlSERwghhBAHj154hBBCCHHw3CtpLeCwKBCOi9i13fTMZOvyBsPdPT7TVC43NAgttrAj1AizMxPm8YmHr46wC327dmkMRhCbJONd2wy7Nih8OEPWxwwZJ+nqmcD9sIB+EuACYih6gnBtxvAzMoEOCFfPETbsu/2HWc3MlidPbo4/1Hvfvv3cZYpPf8ZdV0nEznhk0q0/6585XZ7fHC9QuG+LbKZbtGOOXfgdws+XkCgj3H7s27gbS30lQ7XIZs0CoE8hVzGs3cOp0UNOS4zj3L+rR7/tkG347U+73FMhW/TRI3f43c4QvQ84Hi1izA6c0pCAexSPrOi68muezSB5Qg4+hlz1dOXurQ79Ojv1grLlHGvCzo8LzMfslqGix3hMMp/nT45c6qohbw4oZrxlkWOc+BgFgk8fPfbzY3wdPTq5OT459vFLp8oORWR7qhV7hPdmqa8L5dTbZQ7H0uQImbzp/MQ6wuzjzEzfwO3GLNKRWXghidQbZCaGHWu5PLk5HsL4b+fd5tz/A8sZHXyn2H4wg6Pq/DNe2DfDmtpDfqshuaZorzzFzyGxszBsz2zctn8yVJtNUag2xTOUWZfrBs+vBtsfsI5R5iuQURi/ajm2jrC6QYFCpVnmc3m68DVtjudvORs/f9564rLU6gISPtxY1Py59YAOwjTza+qwXcawLqywnYWG07LEOIBLdhilz3+1404RHiGEEEIcPHrhEUIIIcTBc6+kdblx91OBWGSJInuM/U5Q4CvBzusdkg+10AkKJC1sEZpimCpH0HGGQqWjXetQDHqEXNNbr3MDQqI5ChlyZ/8W4WvUaDOYBayHRJGMHBX+Cz3Cw3SUlJB0DNLgMDCZ4cMUKDxZeGjy8jPet9tzlxk3W7irULi1eeGx0/qpyzhDDVkqc2khQZvWkP1mJ3BIwFIUJgihQ2766I99/Oa47Jm4y2yJkGwZXI44yjAm4Ty6fAb3UI3CtSwMC8dLxMDaVpAcIKFcoo3yU7+eBMndmmH/f1ckCNcPxvHoA7XHGKfzokZRzTXagc6fUyRUTOJbN8cR53yKMHsWELpmkcszJCREqDtLbjlBUpdoHj1+4+Y4MLkZQt87ZBKcQfbJc8g1yGh2fOz9eoqEolMkyCtYPBRzucPkX20fxkG5rSCxmo+pPvF+mEYWaITbBq6rFo69HLJnSkkE8sCUkhnW4wipa7329aGAMyeFTBZvFTyeoShps+O2B2+/HnNnijbeFXDgoWIqx3OeMbEtioRC6uzRpi2S1lKqDsn+19oEbrcCDrIp1rgMxynmQoa5nGJd2mELRks3KFyoOaSuHu22ghu6n/k5ixnHkJ+zvZUUNIXEPFtimwsKQfMZV+N5z3cFustojjuHc3cLSS/H7wa4mAPGeETC2fHT4W4U4RFCCCHEwaMXHiGEEEIcPPdKWnQOpEgalOZ+PEGyvYCwdEsnF9wcywXClQjHrSmfwEUC5cpm2IVuCG+W2JHeIzyYZuM9+EysNoWLrJz4/URIBT1inyXC8euNO1XCQKnPz8OkhxHOLya/Y3I31gmq6v3XdzEzy+FqCnjXHRBfnDOcfOE75k+WcJ2VcERBmVitPHT68R/7pJ8HiRSftJ60rzw5uTlO4DRaw8nXw3W0qccWmdh56HWGcCmNSj3Oy1btcJ8R9W4ShKD7lAmxnC1LN0HGyt50qaSbIhyb7d+lNSAJ3QT9imE0nr+QcyPahDeWo0ZeigR2TeMnffyGu7HSqbtsmLCxxTjIUS9uCpdkFsZjnDWgppCuNysfC6tLl1JTzN8Zas+VkNVnkDzpihnVCmpZkwn1nPi3IORZSiz7pGfNMKyXNST2inI7nJwVHJF9AzcLa9sduXxRQA56dOzutSnWrMD6SZANWHtqGCVCHD9KmDSuRh2zi/NL/NzPNV/g2dFQAkPixQQuS1zHqvZxvsS8HhrIYXAhMVFj3+7fp5XBlkaHFNt0wERl3ckMzx9DcsKuRV0pPEMXqfff+tLb9hIyUcBDJyn9nDMmbITzqe/GbbJtuY6gThgk8EfHvl2iRy24AvNuu/N1YWCiTWz/6FGrzXBM5THguTzgXaHevnpuKsIjhBBCiINHLzxCCCGEOHjulbRKJA3KEJaPCEdlCFlN5h5CriCTFDjeIslbh9BZgsRTEZfVIOQ4zscHB0rl4c3tDvHTOA5x5biHEuXmU4SsM9wzXUqG62P9jhThuCTA5cE6I9hhvobLo4JzIMPvZpN7u+VdM8C2lsKdQjlxg7BotfXjDO21QBK+Bg6sHZxmK9RpqSElxsr7Z7KDZIQ6Z2/jGtrc5Y0hHfdn13iYd9Ej5okwMhNjtnBGbNcuh108O7857pkkEmF2Gq3oClmceCh3PuVcwFiY7r8/azhQFicupWUYpw0SW/aQPYo53D6QmQIi2T3cXkeovzMMrIfjfXkZPFx9cYnadqjZdnzsEli4VS8uhRQRMLfpgizhFGUNu4QuQLggl0feLkkK2Q/tkiN52uYCjjV4PrZb/3nFNWGv+BiZz7yd5kvvqwJtxGSmJda1elRXDA7SUW07/93QQ+7ANVCSYgm7usdcRt3B2I8lygYZYFNIX4upz6kd1sIV5nKCewiUHCHBVJAWz87P/Tpw/pJrNmTfHnJovR47kvZBmtNdBXcYXFfTKWp+Ie5QU0rEs2UBB90R5m8Dx2jd+rqZTyDDVvwuZ4qEom98yKXqegcnsY0dZQ3qyuWYj1Nsc6j5LaV/95z1tiCrl4YtEpjXLdouS5k02H++g6OMW1tehiI8QgghhDh49MIjhBBCiIPn3lj7tuKOfIQoWQeEO/VxzNotU5Swx8Z5gynEpsHDXVnun2dtnDlqXtEpZXA1UJKis8zMbD71UB1DsHRIJbjuo8WJ/y6cI1MkWWJ4MWFCK7YFpIIKSfUKFAsZkEyprh8muVlEMjBD0qkWbXaOcOFq5w6Ziwtv7/McLhy4MzokiTO4DTYIu372xblfAnbb96hD9ezSHT81pJXlbOx2CkhQ+cmnHmqdo87Q9sJ/vkFNoC2SqTWQPgP68xiyQRb8Wo+Q3C41SCi4/Ryh6brfv7OHCsJIuYUBi4kEA1xaA+StGq6eCFkwwfjNIDHN4ETMepchlqhPlaA+1dHE5c/TI4Tim/HfWhGSKdVJ1u07wvx/dOzXsThCDaFT/+4E47qD4+Oy9nGzhY43x1ju4UzbrP0+6+ph5maJpHElpI855uzxnE5JOEIhaTEJ4QTrXQtplwkcW6xHOeSzCMkosbsdNSnGNV2AZmNJcIBcXyw9IWcSfU2JkNwK9ENLKRbPoIyPLjhFqwGOIkgxAedcYEx2i/07Yrue65q3V4lEqwmdu2jTjokE4T48grzeoE2yubfJB77M23YBuapeYf5Cnl6e+nE5889cPL/lXMPY4dpajPrYP55kbHcf13Ri9li7U4yPN1HzroJ8VtG9hfUuyxoco77gS1CERwghhBAHj154hBBCCHHw3CtpZQi7zREiywu6YKDjQALL4BAoIHUcLT0kNkUoa4Xd8i2cXJOSiQ39qwKSBdJxxHDiFInRzMwWSES2XSEBFhNdIUlTwC7/HE0VkcQq81PaBI4XOr+Y6CyFA+Fs5dJNh1A0XWf7ZI0kXh8/e35z3MBVsEbSqF2K+iUIr27oPICDpYATZoFaP0nw9v3UZ5GQ8NOfuDmeYXyNkjMibBoiGtvMWshSHSSxCVxCGZPGod+Y9NJQs6Vg0TT8bpr452cIleeQXOnw2iJMfbaBc3BP1Gj3i+fnN8eR9zJh4j24INFWTK45qsMFdwzlyYbJ7DAHJ0fuVqshsUwgn2Row3Br5Ukxz+nYmsIttEIytTr3vjxe+rgrIa0kPGadIWjMFcZQgCY9llXgdmvHyS/3xTEl5gj5Ce1dwl3FukTz0tfUSYlxjb5dov+ZGG5AcsqeUjqTruJ3J3BNMhFssLEMMqCNN2tfL6DcW59BWt3endzOUn63f18D52eB2oZ0Ewe4KelAC6OaXK+WQT5fOkhAKaTegYl2sV2AcuCAJH8zOLMSJNHtUEsrCVyj/b4eveEORTtCPS8m+Jwj4S5cY5vzp6P7gbptR5jnlFJ5D/MFtpLg4dfCEdkP3kbsyxKJSvnA3+78WZlB815gC8IWNdtehiI8QgghhDh49MIjhBBCiIPnXklr5GSqIV2U/mupMfSLEvMI6Q84TwJrVgH31rz08FUNh0SCekk9tI4JQtQZdoWzFlKkBmZjh0GJ0B6/L58wDIpwJOQg3sOLFy6fTOCuYEK3jBIN2igi+Vqz9RDnQyTDMjNr2Q+lt8VsyUR6HjY+ewqZEfa6AgmkJk9Obo57SINV5scFklJNEYLdnLnbq1m57MPkaQNCpbdym1nLJJPYul8jWeWjR0jiduLHPeSqGiHiyYTSrfchJd2TN9wNMUWNsQXCvW8+9kRey6X/fF90SOxYXXo7dpDVNuiDI7grIkLrA1wkDeY4E20mSFjZDP5dgQn/4N6azuHMSOg+9N9Nw1gCoVyzhLsogUSX9KgZ1fl82V36MR0i+cTvmUa2EsnQmktIg2smVYMTtWGtqleHzd8NOeTHCFkDOUtHiVdh+LHpkf/uBO3YIDkl65jVrfdDT9cV1ju6XY9QV83gmkuCX1Cajx2UA+ZXhAzYoh4Y18t06vfAtaaFBpZjXlOqLiF7pYlfN5NNYpjbFmt5TMbPiH0QID+l2ArQ4Flm6OIp1pyADo+QfYbGx2aPZ0WP9ZSu59EzEZ+ZQoY8XrJenPfx5WOXV83MuhYJOZHbjyX5RjXm8Fwrp9433HbQ05WMMbWFQ7fFvGM9N3zcKszH7TtwUCrCI4QQQoiDRy88QgghhDh47pW01nC7dAgdBboFStRTgXNg9CYFiYJl3lk/i8nNSoRQGaZrkCCQFewD47uwf4TbVhBIYkxcxcSAdEtl1KJwfTaSuvzeOuz+7/hxuN1qxKU7hi9xPAyseLI/UtSQeXzqMsv67MXN8empt/2Lmd//Fs6RxryN0oJ1ULxNL+EiongxQLoskIiO8mM3Cul7u6wqD8WbmV3A5cYke2/MXHJqEyb1whjG2FjiOiZIqjlDQrujRx7mzeEMKJAMbzZF8kxIrnQI7YsB43RAuLdrEWems4P12Wg0wphtN3B/5KgRh37N0Juh8+9NICu0SCK3w2dixDoQx26nDH2TRr8OdIct0dasmzPAdbS9QK2rFfobIf4VJjwdlx3klgqyT4X6bx1D93tkGDUH2hjjtKrhIpuitiHWowKuo8Wxz/EWskSP9TiHLNVtvd9yyJJJxJqKtb9nwtZ0LA2x3iLX5zRFokcmpEX9pVUPCQVTJ8U8Clh3WfNxh+0JWyToi5DACqzHYXiA/oQMz2ciZVK2V4HPF3AlBsyRBq4ug9S1RELcUfJHuPuiMZmht3/f+zmh8tnx8dgN2zeQSbGvYAfJlMl7I9xSsxayIhaeBhpjx1qbcE0OSGyZoS12WF+2dAx3Y5n8LhThEUIIIcTBoxceIYQQQhw890parG/UYof8dufh3gy1eBJIJvMlkv5RloK7xBIPX+WoDZMi6VdT0xWBBE2IXjGEPilfvuuejhSGqWvU7GgRRkNU1lIkWerhHGHCrQoJp+rKrzVHCLXC/VQ7uk785zGMw/37ooWcVlCWZJIx3M9XfPiDN8efPfNEjS3kpylq/fQ1xgikpNXG+zlDTZgk3i0nJLiGEg6vkR5qZqH0349IOHny2L/j5MilqIHnRRj85NjHasLabad+nnKGZFpw+dRwG63hpJhtXQ4OrrztjQxjkNIrE68ZaslFtG8P9wPdh1mk+5Ihd2/nADloeYS+hwx7gVD3APmkw/FkOk74liDcnUP2KCEDnB57fyzhmmOY/e1n5zfH5yu4tyghYK1h/slJiRpLuLYdxu/klhtpX1Rw3rRwuQQkg5xM/boD+qrFmsKfF1gLuSWBsn/Eerc69zE7jM4PV99AWdj7jK4gs3FCywHrDpPK8rubCtL1CvW9qGkZZRC783i7osPV+2q2RN2ojLLceE3ZBwMkvAZaZYvkihnq8fVIyDfncwZSTzvgM3OfB7OSY4JbJPy+BiSQNR5jjc4hF6bZWBpqU2/g0nyOFKWf6ykS2TJpZQ1ZcZzIFTIknMsdnXiGeYpzdkhUGIe7Ezu+DEV4hBBCCHHw6IVHCCGEEAfPvZLWgNBRjbhhCxcNa10x5MoAZ8SO8QEuB4bHmeQQ0U0avKzH+1maM9zl52ESo7Ic7zZvcR0Vdnd3cGrQIZUi4VqHpFHNDomlICewFs8OjocU4XqGlmskSmLon/LMPplDRqCkVa8/cHOcGxI5IVnd/NRlzIQyE/qkqvC7Ow8hry5dDju/QJ0rOhiih3IrRCYzJKHrb9XrefQGkgRCijo+cZcWnSCUVilpTWf+HQzZT2beRouF3/NonGP8L+EunE79d8t0//1JmTRL73bRUJI1jKkc7b7FvJsg6eaUCcMQci4gky0wv5oG7go4TVK0c4+Qcz7hGBo7+aZ0e0IyLCBFDJjnE3zfm08e3xxnCZOeMXEo5ixqHdFFExHiH7A+xPBqJ8i7gWF6SgIB/TbjvIMrZnPu88sW+F3UyerQzyXaPhpqbGEud9Xd9cYGuOm4VgYbS0NckylL9tjSwK0ITBY6QGZKUB8qhYybBzgrMS5qyDF1jesbKL/dvX1iX2RwEPYBbkr0ZYv+5ljOBoxNPstQt47JNTsmZqS0TbcyimHx2vLC25D1y7r2lsyHrQM56pPl2Feyg1M0UMbG876HiypgrBWZX1Myh2SM58AA2Y+Ou8FG+07sVSjCI4QQQoiDRy88QgghhDh47o0B0b0UGRJEXRbrEbLGbvzNBknJEoaNPWRFB1aGEP0oEyBCcwMvF/VWWJdjlORuOpYSWoQUa0paOOZ3D4ypIXLWM0yHne787hQZE6GkWbVxaYw72Afcw0MlN8ugcRRzlyOOH5/eHEc4cuqt/3y5Y7InuPcgFfD+ezjW4uBur3PIW0wwSGMAXWA9Qqh1S8ee2WzKRHQ+JilN5JCcRnXc4LZhra8iRz00SDwTuCHynInUcD2lS1pMPJjvP2puGZKM0VHFOZVibE5R/45JLvkXD6WhKZwmKyQ5bJDA7oI1jzrWsWGdNh9nKSTfUZI0M+vhoMwwpgo0XoPfoeOQNeI6hM0zdE4OGYvjKMe4iZBo+pFrEBM4PMzfiEyyRmkxQZiebRQgD/aYRw3nDscIxvWQon2RqJBd0kEyM8hNVXP33LfbkhbW2sp8nZskkCyiz7uOLtWBshTuk7XUMOZb1sbikp/SjcXtE/6RhgXK9kT2ku+lPMsxRVMua5sleBY1aPcIeadAEtR+YDJKPH+Y+Bfy9GjdYO3LcRZMG/AAoxS14zMU8mEPCbhp7m5fzl/mC0wwCOnc5VYDPk+ZW3gK+fxlKMIjhBBCiINHLzxCCCGEOHjuDbYnkHRYroqh8mRUAwoOAcgnjJBVkHFQMsVKyGQJfjdlEieEInvEtSgxBITE4Hu4+m+G0RguY+0mhNcY4h4Q8mu722e+vibumIdE1+GaAkLiOWQVhnEjQrf7hG3JUDmTckXIPgxHHiGcXkPe6uAkiIgVD0gqSHfCmwg/N5C9mJys4jiCo6K45ewpIcEU+NwEElWO+2Fiqm7kTvLPL2Z+PGVdG4zJtkadMITiaa5j3Zh3kA/r82YHWbFD8i065RLEyuuC9XqQVI4yNBL1daixtLvw+11f+megnowkkwJSY4W5leKcs1sJ/BrIuExKmG38c5dIeNpDeqWEwHm6gxxCNw6TltZwCrHGVL31ObjduAy7/zR1V8whTTCRZIfBc7ny+3/zkTsRkwh5hzXWmDyycLmVchPrkFEy26EW3nbnWxvoxqHTjo5WM7MW0sl6B1dvDhcsPv8yFZ/3zy0JHXqCP2cNL0pIAc8yrveshbgvIurHsUgVnYIZJg+fXwNqgVGOD5AhKX+2+HzETpMcTq4UzyLW6sqYpJeSVhy/FnQdZF90c4IEoTnmPz8EU5+1eJ7UlCGx5YUZP1NKgHDhBoy1GV1nt+q53YUiPEIIIYQ4ePTCI4QQQoiD515Ji44VJjFjSInyUxwogd39u5R9mMOL9US6/u4d4xXCj0nK2lYI/SWUHsbvcykTFmE7f0OJgtfHsCmTEzJJIDQw/ryn6wwNUI5qhvlnGCoMr47MvSvyAu4iOJmWcNIwjFohFE33QzyCTDSqb0V5E9IVwts7SAgMZfZ0aTHZGpP5zcb1lyhB0LU0QbLCHFIGnSAjhxjGKuVaGnI4Llr0OWVSSlop82E9QK66Z2eewDEGH790SATEkwvUucsgJw2sbxNduhiQ/ZEyVs8kaTgPx3VIUTwsejuXaBQ67G5fR1miThZ07MutS0vZhJI25hHHEd2bGB85kicWI2kN8g7GabVD0tFbTsF9MZsjsRzWhXpUNwhSBtxyGV2wuLeyQEK39G6XXofxO2AR4hqfwnE4QTsygSPnkJlZAZl8MvH+ZOLZAX3L3+YaHlBLi86xFs8Cfn46g66D/h/V9urpZNx/UtAM60PEnY3cfjQio927/u45W6IP+lENMh8HZcGxjPvFpOjoppz4OSlvDbfkxQY/YPLXFs4pSk5TbC+Ioz0iSDrMcYStFpyPXIvpqg5oiwzf27ev3jugCI8QQgghDh698AghhBDi4AmseSKEEEIIcYgowiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiINHLzxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh4DuaFJ4Twx0II3/m6r0N8foQQvjKE8DdDCKsQwu943dcj3jkhhI+GEH7p674O8d4RQviOEMKfuOfffzCE8NXv3RWJ10EIIYYQfvLrvo7Pl+x1X4D4kudbzewvxRi/6nVfiBDiCyPG+DNe9zWIK0IIHzWzb4ox/sXXfS1fLBxMhEe8b/kJZvaDd/1DCCF9j69FvMeEEPRHlxDvMV+q8+59+8ITQvg5IYS/cS2FfK+ZTfBv/0QI4UdCCC9CCH82hPAh/NsvCyH8cAjhIoTwh0MI/1UI4Ztey018iRNC+AEz+4fM7A+FENYhhD8VQvh3Qgh/PoSwMbN/KITw00II/2UI4fw6XP4P4/cfhxC+P4RwGUL4H0II3xlC+Cuv7Ya+NPmqEML/cj2fvjeEMDF75RyMIYTfHkL422b2t8MV/0YI4e3rvvx/hRB+5vVnyxDCvxZC+HgI4bMhhH83hDB9Tff6JUUI4XeHED55vcb+cAjhl1z/UxFC+I+uf/6DIYT/DX7nRua8lr8+cj0uVtfr9d/9Wm7mS4wQwh83s68ws++/Xlu/9Xre/eMhhI+b2Q+EEL46hPBjt36P/ZeGEP6ZEMKPXvffXw8hfPkd3/ULQwifeD9Ime/LF54QQmFmf8bM/riZPTKz/8TMfu31v/1iM/suM/tHzOyDZvYxM/ue6397YmYfMbPfY2aPzeyHzex/+95evfgcMcZfbGZ/2cy+Oca4MLPGzP4PZvYvmdnSzP6qmX2/mf0FM3vTzL7FzP5kCOErr0/xb5vZxsw+YGa/5fp/4r3lHzGzX25m/ysz+9lm9o33zUHwdWb2883sp5vZLzOzf9DMfoqZHV//3vPrz/2+659/lZn9ZDP7sJn9cw91M+KK6zn2zWb2c2OMSzP7GjP76PU//8N21Z8nZvZnzewP3XOqX2NX6/MjM/tTZvZnQgj5w1y1+Bwxxm8ws4+b2dder63/8fU//SIz+2l21Z+v4p80s99oZr/SzI7M7B8zsy0/EEL45Wb2p83s18YY/8u9XPwD8r584TGzv8/McjP7N2OMbYzxI2b2P1z/228ys38/xvg3Yoy1Xb3c/IIQwk+0q477wRjj98UYOzP7g2b2mff+8sU9/Gcxxv8mxjjY1UNuYWa/L8bYxBh/wMz+czP7jddy1681s/9bjHEbY/whM/sPX9tVf+nyB2OMn4oxvrCrl9Ovsvvn4Of4rhjjixjjzsxau3rB/almFmKM/+8Y46dDCMHM/o9m9n+9/uzKzP5lM/sN79ndfenSm1lpZj89hJDHGD8aY/zR63/7KzHGPx9j7O3qj877ojZ/Pcb4kRhja2b/ul1F4v++B71ycR/fEWPcXM+7V/FNZvbtMcYfjlf8zzHG5/j3X2dm/56Z/YoY4197kKvdM+/XF54PmdknY4wRP/sY/u1zxxZjXNvVX4sfvv63T+DfopmNQnritfMJHH/IzD5x/fLzOT5mV335hl1tuv/ES35XvDfwD4atXb2g3jcHPwfn4Q/YVZTg3zazt0MI/48QwpFd9fHMzP76taR5bmb/xfXPxQMSY/wRM/tdZvYddtUn3wNZ8nafT+7ZE8J+Huxqvf3QSz4rHp7PZ438cjP70Xv+/XeZ2X8cY/xbX9AVvYe8X194Pm1mH77+C/BzfMX1/3/KrjbCmplZCGFuV/LVJ69/78vwb4H/Lb4o4Evsp8zsy0MIHKdfYVd9+dTMOhv334/Tl8Vr4b45+DnYzxZj/IMxxr/XriSun2Jm/7SZPTOznZn9jBjjyfX/jq9D9OKBiTH+qRjjL7Srvoxm9q+8i9PczMnrefxldjU+xMMTX/GzjV39QWFmNyYR/jHxCTP7Sfec/9eZ2deFEH7nF3KR7yXv1xee/86uHna/I4SQhxC+3sx+3vW//Wkz+60hhK8KIZR2FQL/qzHGj5rZnzOznxVC+Lrrv0h+u13t/xBfnPxVu/oL8luv+/mrzexrzex7rsPp32dm3xFCmIUQfqqZ/ebXdqWC3DcHfxwhhJ8bQvj513s7NmZWmdlwHRH4bjP7N0IIb15/9sMhhHey/0B8AYSr/Fi/+Lr/Krt68Rxe8Wt38feGEL7+er39XWZWm9l/v78rFffwWTP7u+759/+vXUXnftX13Pt2u5IxP8cfMbN/MYTwv742FvzsEMJj/PunzOyXmNnvDCH8n/d98Q/B+/KFJ8bYmNnXm9k3mtkLM/v1dvXws+ucA7/XzP5Tu4ro/CS71vxjjM/s6q3099tViP2nm9n/aFeTUHyRcd3PX2tmv8Ku/tr/w2b2m2OM/5/rj3yzXW1y/Yxd7SX406a+fO3cNwdfwpFdvdic2ZUU9tzM/tXrf/vdZvYjZvbfhxAuzewvmtlX3nUSsVdKu9ow/syu5tebdrUX6/PlP7Or9fnMzL7BzL7+ej+PeHi+y8y+/VoK/t/f/scY44WZ/V/s6sXmk3b1xwa3ePzrdrXZ+S+Y2aWZ/VEzm946x8ft6qXn28L7wO0cxttgvrS4DrH+mJn9phjjX3rd1yO+MEII/4qZfSDGKLeWEK+ZEMJ3mNlPjjH+o6/7WoQwe59GeL4QQghfE0I4uQ7V/jNmFkwh1vclIYSfeh1mDSGEn2dm/7iZ/T9f93UJIYT44uNLMdviL7CrfBCFmf2QmX3dO7ToiS8+lnYlY33IrvTqP2BXIXQhhBBixJe0pCWEEEKILw2+5CQtIYQQQnzpoRceIYQQQhw89+7h+bZv+Q03etf2cn3z83rl5TRO5u5SC3AbDn11c9w3nr6h7/z8m62fp5h4eZXZ0fHNcTe45BaRBmI5v8mXZEXhqQOSEsfJuNh23/o1VesNrsmvm5kMM+Q1DFD+To78u49w/zvc57bxG60r3yKE27EBxcCrxq/hxYsXN8ff/ef+Ki/pC+L3fsvff/PtaYJ3XVxH1zU3x0nmw2My8XtOgt9ntfNxkSPZ6oB+LgtvozTzfu4Hv4Z6623UNCtcG/r/lvzK/MtFflM7dtTGZxcXN8cdBl9W+HcPuNgMQ+b02Mdhnvq4Gvh3An6hrrwPW/T5fOn3/3v/rf9qL/35b37kr93c5fmF98FkNr85ZoLqgP7uMcqb1j8TB/95kXtfXlVh+dxn3PVfFv6ZZudz6+zZs5vjuvKfh9Q/XxTeX1cn9sMc4+74yNtuOvMxmGKeFxO/5ywrbo7b1k/a9z2+y4/ZGWyjDmMrSf1THB+/7Zd/5d7m5h/+82ufm/wSrEF97/fTtt4nbdfh47hW9GFTez/UO193Oae4vnb4rg59k6X8GxnjK4ybIsHvcH0tSu+fNLl7PIzWCMy1XeVjj+sU7zNiLifIV5pi7PX43Tz3tv7Wr39rL/35N/7Wxc3dH594jszLla9FL16c4ZqZ3siveTbzsZ8X3iZt6+O3mPpn2M7NzttqhrV7MvH2r1uue5z74zhIh3Ukop/rGv3Reptudr72Zblfd4dxGntkJuj9d6st1wv/3Yg5Wzf+7M5zv58k9eNf/Uv+njv7UhEeIYQQQhw890Z4juf+l1PR+1veBsdTvGHG1t+WkxR/cSeICOANLpv5G1zAW26C1HE5ojTJxM9ZZP69s6m/weYT/3mS3HpTbfAXhfGvAn/bjLg3hhBy/DUzw1+2Of4a6fDXUtLxrdhPOeA/RtEe/Mftv5b2Bd+2e/yVwBd6XkePv6h6RG96tEWBv5ympfcDh1aCv+QMb+1JxGfwc9v6taWJt8WkHOW8sq7xz3WIVPCvoQnGxm7rkRD+1Z6WPsYGRAFzXHdm/pksu7stpgufL8mR/2WXpONI4z5YnXsNv83K76vvvM/413gyuma/94i/eRJE+hgFMIzroffzI9hjHcZKhb9k69r/ekvx11i4lbSXwbu68n/LUv+rsMNfgmnta0HJNHYBFxWxdiDkwChVh79yOR8Na8cUf2mXuId9UmEsJ7iQDutRizWlafHXfINoB8ZaniGKib/+d/jrv+8419DP7BC0RYZC5wFzc2AEzW79NY+fB0SOhpf9Pn63LNneCBszuoS53KPPI8c5LiJwPib7n5u7yudOcslIGcYg5lqLqFSKiDYjQnwkZGi3rj6/OR4YBanQnni+VbW35+Xa140UUfvby1XXUwHB/SCq03a+bm4qRvS4Zvt1c41mdLhBRJjR2hRr2QZr0xA92jOZ+vr7MhThEUIIIcTBoxceIYQQQhw890paWcQGKoRy8wXC9QhxjiUGblDyc4YeUgJlH4Y0EV47OTm6OZ4ce8gqINxVYtPiZLm8Oe7jOGxelv656cTDbgzNRYTNuQnMELIrp75hcnl0cnNcoL2GS994y+voBm5CRLgaEtNs/jDFoBdLb0tuauOGtbxjSNjDl3nh7b1l2BGb4BJsHgyJtxFlkw6SXt1wMywkNsiS04Kb0saSVh/9Ohi2LSBBzClvTbixGeFlzIK+wWcgG0Tz8xRTjH9jmBob/6aQDzuG4vfD0HlYN88hE7S+YTDP/F4ipAuqtgXmUZJhbKZ3yw27lqFyHDbeFwXmb49zUj3YbbEx3cwSbNTNsNm0Rx8kMxgS+Mv4DOdsjnMOGB91dem/i3GwwybMxekjvx+0Y5aM15R9cXbum1gDwvcD1w70IeWkAKmgbv0e4kvOU0Fm7Ni+kFMGg9QF2ZqLOdcHSoZmZhkk6hC4edq/OwwcG3gW4Fp7fN8O6w6X9qbnBm5sjcA44iYBDG1LR/35xPbBbuNju0N/TGBwKbBuchN1X/u8zrgu4XnKZ2sFCX7AGrrd+DpQz3zdLyD7bLYuB9W1HxfZWNPiPvWm9vNuNi6JtXhutpD/J1N/HpfYXjCb+88rGDwosXKNTvDewD7e4nctufd15uojr/yEEEIIIcT7HL3wCCGEEOLgeUUMCCElBAXLUYgTYcnSw5jcVZ1CM2gRHmeIm7kejHkDZh5OnuQe0rbcw4MTSloI0cf09vscc474dVycueMlIDdMMUVuCIOENoW8h+9maHmK0H8LF0XFnD8jqQNtne3fOWBm9sbjN/06sNO9gVyXjsLU/rsN+m2NsOtqdB7/zHRydw6My5WHewfmn8D5Z8inMNAdhvCwmdkOLoMdQpsxeBsfIQ/GBI6PNX53ixAxXUUTjJ+jOfOEQAYa9Zt/fgGHY7UdX/c+6CDRtAiJpwgnt8gLxbxIzOeSTyCTUPbYIGcV+rhHmDnA3he6u2WSAnlOyrn35WZ9u038vBnGS5aifTFnm0uXpdrofVnCFQLDzsh1lvCc6OMZPp/iftoKeT+Qj2mfDJAEBsremCN0b+aZX0eG7QYtZB8685hHbLpg7ij2A51Zfjhy10EOomNnnEvGLMAROsqxhEU/xWciFhtKZbQMDZCoeH18pnSUtCBX0RU42N1jeF/UGzgcscYz5w9zJ7Hvu9rH9TRBu2N9W6+3OPaxT3k2ME8TxsT05PTmeElZdIA0tvXrMTMLGaROzKMa312h/yPmSF35uWZ4Ds7wDGXOPg6jAu8TlGTH+fWQy60eX/ddKMIjhBBCiINHLzxCCCGEOHjulbQShLxYKsHgamLiOYb6mVgoMLlVSZlolE3p5nCyREp/nKcfpUFHCBzfewQH2XQ6Dj+HjG4GhNFaD1l3DcLGcB0xiVeGkFoLy0s9cprRgYAEewjLZwjj5iVkv93+XT1mZjMkBuyZNKrndbCEAuRHOCEiHFgZJAS214RJCJlYy/x7S0gLJeTDyBIdl15mY7MbhywZ5owIyW5XHhYucw/hppAfZ5BXBpY+YUyVfw4k6LeCUi8cRejbDm0XE3pE9kMHWTFCotoi6dlAeS73edHAXbF7+vbN8WLhfbZBuYrLNZxfTHc/d/cHJYMWMkkBOXs293GT35qbHd2baK8CSfiq5z4WeiQwZSi/gRycJpBVMNeOTt0hMkrOh++lW7Nbw+EWbpXE2BMdw/EjKR7rCBKeZtndbsrA+0nvdnJxOA7ot9F6B7lpCnnWAsY1++aWnMAyIgHr4kCn2SjZHZLp4TMRWwMy3Cedrzy/DTzGIe6ZcrAN+/+bn2UTyhxJcSHPscxR7P35k6FNJniGrLBeVyjd0GJdLuBohXpmLZKFbuGO5LaTeuPXwMSJZmY9HJh8brZw0FW4vgC5Nc38/iOe5R1KnbS4H5bt4ec5fnf4/Hrl1w3j10tRhEcIIYQQB49eeIQQQghx8Nzv0uKm/ZFzwt+TSrilsozOKQ/9VhcuMawQvksmqHiMRG3pqO4P3D44Z4nKyUtUVz869eRRt8uk9Kj6zHokxQyJ/lAbaxgltEISuuJuSauldIeYIl1XCRL7NThnGtgVD1NLq0H16IL9htAhw+YMHTJSXCKRFV10OcLdSefn6SBpzSFllBmtQ94Wa1RL3+x87Gy3LJo0rvyLyxg5AI7nqMtUQForfPyUrX9+inHYMDEm5kKLelUNkm2yEjQlh2K+//pLNeqCbSDv1BtIlUiY12U+77ZnLg21W9TrOfJ+reHU4Ngs4WrpIR3uKsrCPm4m0ds5bODYKsaTc4p5x2SIZ29/xq8J4Wu65gIsSJtzbwsqHRmcn9Z6yJ511LjuzFAXrW6QzK+GTLZHAhP9QX5LRusI1l3I86wNte287Zsd1lrKdayNhrYIGC/sniIwMSvcp3Dj2a1aWkUBaQLzv2ZdJ9RcYiZBuvwmBZ4Ro2r2fs5xgllWlEf9sIY11jCvswf4mz9w3UCSQLiltpi/I4kK0mALm+EOa3cPt/KoXh6lXaxLT5+d3xwXufd3iQr1GyTdbG8lSu3htGKywXyCuUPJEG6sMPd5R3cc14uetSyx1tARuETSXIOL11CTLCSv7ktFeIQQQghx8OiFRwghhBAHz72S1jBK4uTxpeXiGJ+CzIRQ2+nSZaIWO7UHOJBCuNuxxPo5hhD6tPRzloVLMkzUVSGDIetvmJl1cOBAfbIE19c3/vs1ko/N4V5ikqwWocZzhNzpZhggezWjZHBwVCBh3PBAklZeMmQN5wXq2AzB+2e39hBsgCNjDhmghHQzpaOOCafw89njxzfH6/Wzm+MXz1xmqRqvK9RHD8EezcdywhKunyXceSXq+CwnftwivBxQh6tCn1BoOVn6uEgQOt0gKd9mC7cU/nxgeLVI9y9pUTLr4XjoIWNsW5ecGCrenp/fHHeQ5+oLb3e6V07feOvmuESSys3Ow9vTlBKej7M5JIM5ZJh8MnZp0TUYMF8yOEQmSCS3RAx9jT4wyAYddNgeNca6yu9zij4+fuTr2gROkzggpL86t4dgBXmQ2wFC9hLHFhK+phPUPYP0ykSdacFEgFjXDdIQkx9CTqGzp2t8bR7wZfFWHaOaTh+so5SoUsiMdOHQwcMacNkokSKcadA+mMy0gtTLOml0hKUP8Dd/Awdljf5rkJCwgmyfQRrCEjWqi7ZFDauGTqaBEqMfnz/zMX5x4d87K7wvmtT7kh67i9VzI6wft4Pc9fYzn/8B2z8oaZULv+630Gc9tjBwvBdYF/iZHFsN5imlTSQjxdx/GYrwCCGEEOLg0QuPEEIIIQ6eeyUt1tyhSyWyfhZD2QUTVyGMD6dBgURMFRwitAFN5i5XsY5LhrAn6zMxfFp91sOADO+amdUItZ0eo2w9Em5t8Ut0p/QI8UfYv3rsPL+kQwb1QcqCycNQ6yvAEcRwXP4wThBKcTXuk3JNP9BVgARgkBnKzI/HDg7/ruWMSaOQoGp3fnPcMfkhvneG351PvJ+yWx06m8LBl/i5EL22GeS3HCHxAW45hsdLhEtPKZNNfExeTuC2gETL5Gtt5T8/PT2xfcOkYYl56DeBS6fdoW4b9LZg3kBZgnbH2AyQhuP23I8Dxinud7rwfvrQE5fAIkLgC4T3Z5OxzDdAut4i9J9i7uS4vg7yS4ZkaKyH1cGZU7FejzeRFbm7OrsKDs3Ow+wDXSQDg//749kzlxEyrKmUX+imZA2sfObyNKWr0ZoN2acoeG+0smEdiNwaAPcl3Th8PvRjZ0+AzSuHa3KOBKM9pGdmBgyQ2VY7l35q1iFEUskeGQYb9FUHSSRFmzLJK5PR7guW56LkX6A2VpmhTTEVGlznGgk/KSVFOExbuLGOkRS0nPnnN5966r9b45m28C+u8BxbbzFBbt1PXaN9sQGADrTAPlh7/1288O+jDF8gSe3xCb54jsSWG0jvuOcK52+b8XXfhSI8QgghhDh49MIjhBBCiIPnXkmrwO5p6kMDJJBsFKZGqBQOqaFFIiaUrWeipALSVYNQaVKx1gtlFf/8ZgdnBvQM1jYyM3v2HO4fuD8enSLhGn6/GiXeQ82SqYcOmfhoCtdBGSHLLPzzNaShgTWHUspet7S4PfHi0sN/i6lfK2vUULqao4ZOV/lnus4dJTkSjFGW23UIxaPfdjVkzMTPOZ0hqRpcTX3lfXDbdcfQN0PzDfo2BtbPcolqiEi6RUcJcpi1SHQ4gZPvaOnyzckjhlcx9lrUjWn3HzbvkbhrgARkNep5QZIcIHWxjNUUSRGXyN441Ei6iSSEPH9AWHqAvWSFkPYEzs0En+lvJTebQ8amDFDj+gKkdCbFXMFpNodck6R+zhdbJFbDWKkqd5qkSIw4rf26a8hnIYyve19MsB2AyUx3O+/bKRyxVQVXJ3SmSYp5jXbkloEIdyiTu5WQf+mcyfkZJjms/To3m7GcMF/CqbeAjEXZAfXvdqzViGcNller4H6CqmEsf9fgPH30ccgaeUmD2oHh/ty774YektAO0tt8CccoHNCXa5/LG0hGF3CDfvapr7nFiZ9nUmJbBJx1rHdoha+BCdbrAc/unu7mdFyUKsc6XQeMR3x3j+9OUCNxgWflERKblvhuyqG7DSSqjvIxz4/6XDUdtkxAeTeK8AghhBDi4NELjxBCCCEOnnvjeSWSAOUBoTCErxZwsqSpf77aeXip5g5zOKJmGZPf+eHF1kN8TOA2mfh5mp2Hu6qau/Sx+z2OQ1wRYcT1pX9HCfdHg8SDl5dwpMCCdIow/XyBWi9I5rdDqHw59Z/no2SO3hZTOBn65mHeQyMaOcvZV0gglfp95pCZmNCrhduNO/Xzid/Ps8981r8YNbMenSDUipDzG5D9kGPK+h0cKzZ2r/U9rg8S2spcuixKDy/PF0xYhTFce8h9RwcAJLrJxPtwilpix49R1ybzcO9u5+d5sX51QqzPl2rj9zgg1D+jww+1w6ZIBngyx7iDdDGBw6da+ZiYlEwGBskXTslR3TkmMIQM9fzck0sePTod3c/syaOb4xQuwBTOHEodk+A/fxPySYt1KkDeyE79+1DmzS5WLrFGQwLHCPchnYjxYVxaKevtwU2YZZB9S441/0wBF+wcfch6ZXSaBczlDONlgbFw8hj18nD+DBoYE3Y+fT7ePlDTLYj1mYny6ALukGRvt2E/YCywwBeknwzbG0Z1z/BxOjSZyzHHve2LFMkSO7jMniOBY49xfXbhY3CDNj2DbPuZz7qLb4o1aoqkk2tKgVjfOsqQcNxN0f45xgEdbWZjGSzH9ocebuUO84JGxink/x0SI9aQ24+w1aDBlpcucu772jFbIBkl3Kp18+qtA4rwCCGEEOLg0QuPEEIIIQ6eeyWtFonhstIlh2iUBvzzZ5BGDHJDgxhXDukib/3rX5x5GPzZCz9eHHvY7BJh9hfY2d7BdlLCWbStxuFn1hOqETrcrVwS2Gz8vOeQvQaEINMA5xjC70zsN0FCqwxh3BK2g4h6YB3kmhVrA+2RgBBkA+fcFmHEAZIFusoqOGF2CD/3l6iZhV37QwJpETaPgLA866/kuYdKj458F/70DQ+JUrYyMzs/R30cSHQp3CZ0CRyfeOgUeawshxNwhVpBiPiOnBSd+Tn7hG4e/3yNsPZxMU6ytw/CgOSBlCGRYDCFC6bM/JqXlLHQHwNcETkSQZ4gWVwG1wkTlA2YWzlkrAHJ01omy2vHc7O59JB1hzp0lDRGeSdpL8LxFDJ8DRmH7kgmRS2Q2HC3cRn2xXOsfah1NCp2tEfq7u7zUorpsNj2vbdrUeJ+UBdvApmso/wb4LhFu/So15QPPgdP537MOkk1XHpHs/Hfzlsk9iyZDJEJXCGPYOm0FJoTE2bSOXaBdZrydIt2DDwnnlkpXG3FrZpu+yBgreyDtymfLR2uh+tywLNic3bun4e8lTERJhcd3FfIfa0bjSx8V4Z1aXF0cnOct+OxGAMlUI4p/0zI8HzEsxXLqZ1jzUpYhw3Pvku4tI6O/R52OP8O68jpCR3Wr5abFeERQgghxMGjFx4hhBBCHDz3SlpphkRkCFmvEKZeIdlahvenHDvnWauJcUaec4vw2BZyS8J6QIl/1xoJ6dKFh1yfIKHe7dtrsW2/xW71F+ee1Gm9cdlsu+MucQ/zbeEimMBdlULGmsLBsoE01sOZ1UEeaBFmbh8gUZ2Z2RD9u5uRdIOd8XD8tAh/jq0NkDQhgXVI/JShHlZEqPVyxyRxuM/CQ8sRxzSs7bqxEyTOUAsGcswWQVyOyQskqBwlLsP9tHAbJObXQcXh6VOXX2Jx7L+LPg8pamzBObIvmvr85riuPQw8zT3EO4MU06Bfoc6Nk4vS4USHI5016IMZxkQOeTbLmJAM7rnOr7ODfGJmtkG4u4fzwjBfJktv6wbzt4ITpqvO/eeQTHaUtJA8MKVEhXpbE4bosYx03f770myctLShMw1TJCJR60jRQ2K4vqejzD9UQooYIBNFHCcY+5tzJClFUlTO8WrnA2kxG0tDy9Kl6ArO19XK19oE1z3DXMbybylqbFHe49/qTFrYoo3o0d1im8AArbrhnow9cf78mf8HZP4EySVHGRXhbt7SyYV6fAnaYYriWxOsoVNsr+ixps2XSC66dJloMkM9STw3q533kZlZwLyYo47XFE3H9aJb+eeZDJC1IyNqidWsz0bnFwZ5jmdIA2mwh4OseAeOO0V4hBBCCHHw6IVHCCGEEAfPvZJWgORUIuR6vvbQJ5NETeF+YJ0kQ7iLtU5mcI7MH791c3wMqScJSJCHMHaNXd50PjWoCdLbOMTVB78+mHosS/z6lgibTlGzpEfoM7YedqTsMWf9Ge5mx673iDAdXScD3j0TOOL2SY0d/WskRrTgfZghkVWN8HA+81DoHKHT7coddRXOP4PM2CMkOiBsHjBeIpJdVYjjz+BAmc3HiQdrJJ/cPvUw57b17wsYYxFh7Rpx8xpSX42xkOJ+8gS1otb+8y2kkvLEk+dNkcjrlhK3F6ZwZgU43Gbovwgt5uzM26SGLBMh/zFRXYtEm09fPL05znDONx75/Ra43yL14x1C0RWcWVUzlrQKSMOsi5aWC3zIx9QlHKFrJA98+5nLCWu4ABdPHvs91EhUV/u4aVoP5ZdLHzdJi+RmrAW3RyasbYfEgJTQWIeKtceWc7hUse5QGA+Um9G3NaRFOqW2a2+7T7HWHGTMFfqgRmJLM7OA50U7YN7SnYOaSyFxGa+FVDZA0ty1/pkNHLgVXJZUYgPGIedIj+0JacEtEPvhsxiDk6W7ho+PXZKlvLU+9zH16U9+yn+O9SrDdeZ4VgxI5tfhvtj3T+BOPT1Ckj+sb3T0Jad+nWZmwwYDA28MW0ivA2reNZh3dNzlcMzmE9RORFLXtvPvPsIzJMe4Wb3t65HhfWK7xTPtJSjCI4QQQoiDRy88QgghhDh47pW0WNMlRciasgzrYzQIfeYIlyUpd/DfnfhoeYQQMmSyDWqmbLcexmTdrhzJ3yrsBM8n4zBrgd9ZTiFdpXQC+OdDijAdJJDVmScoS1DrJi2QoG3mxwEhZOv9fjpkv2OiuiF59W7zd0OOMGqNMHCCxFIBIUiGLM8RpmWyxRLOHkp3RQGnSYSkhdB6P+B6EAbtURMlnSJE34wdFeutt18Dt8kMSbQa2Lyer12KrOAWq3p0evSQb4taLrGn0xDXhDmyRn92Jftw/39XJHBdITpsdQU5gG6/yMSZSH7Z4/MpalJBkobiZ4tjuMAeUcJDuBraCJ08LRO+FaijZ2YDkg02XYXP+bi43Hjov4W7atf45ylnVphrcUsnm/dxhAbSYrwncDi9cerXWrevTm72bkigsbMeGnYVWLX1+z85QqJN9NUFXFA5ZOLTI7+HFPJADtl6gu/tcZ8tHJCUNA0JSC+ej+vFxRLthPV5gkR/PeRgzrVqi1pauNY16i9VWF7agdsE/PNMkjlQ6sLzq63377pb73xuMldqg2dZgrn2HC7eHRzQbB/WJrtATToav5Bb0iISv6aQko4w3ks4qQ0J/4btWG5OkdCPblXWz4u41gTPryme8RMkMJ3AxZtjLUD3WYJnRQ55ssTnuw7bFDavTtirCI8QQgghDh698AghhBDi4LlX0hoQKqwa7rxGnRSEqXrUuGAyqPmjJ/6FiNEyAZZlHupkSfktapGUlKQW+PyRfz6WDJWPpaHHp34dJwu/7vNnb98c56wHhXBqz/o+SPzU4udW+q7yiBB1FujwgiOq8N+NSOC3qx+mXs8ENccG1IMKGAateXvXO7++sxe+A56uq+MFnHxo7nyKhFh0nSG2XMFRlyf+mdUOboPCP7+4JVFmrAmVuEyRUqaAPPbss89vjjv0Yd+x1g+OEV+tkTywHxCWhzxEF1yDzIZ5Ok7Ktg8mkB53iO9XSMhZb5FQEi7DDZLqPQqoVwMpaQf31skpEpTBEbSDZEShJ4MctEF/b1EDqIxjx13X3X2tkU4Q3DPr1nVIspbDhZLD1dTA7ZQhYn8MKT3POU7hZBvVhXqYvxEjxmOEJDB2HPpntmijFH3V0QWF9u4gP3GrAg2ErFGUIfnjBnJ7ibW/YG2+W9LQpoP7zSAfV5TNsM5h7RjgMaL8OuCZ0tcYI6gHl818/S+wJSFgPnaQlppmLMXtgw5jdrf2dZNJ9QxSzBROtA8t4EqEfLhGotgFnjmsR1jgeTrFWh8n3g4lnHEnR349GyRlHeBoMxuXj1tjW8Aa2wIabG3JURsrYCtABVm5azGO8MzJcyTgZHJSSJI1kgMndAAn4+u+C0V4hBBCCHHw6IVHCCGEEAfPvZLWJXY9LxBazBBaTVETIynhdpp4WGsOZ0eLsP/Z8/Ob44CEWZOjU//eBMn8EE6doE5QhiRXEfW/mDjRzGw696RGA6KL3JO+uvBQ7KTwe5hAPilnqOmDkGhEEsIGUkoGF1iCMOt0wXoqqKtVj2uZ7IstQtZMvNcjLFoj9H228pZZQx7ZrP36Li88jHgMOSGdM8yMYyZGQ3h1SCEtwHqQQmLs0nF/FnPKoHcn8rIe4XGMkwxS7PYcSQXPWRvOz1/DOXiJsC7dBpOJSz+zuYfWJ0yetydOH/u9JMGvpygh9cwoz8K1gWY8eoTzQFYeEsobqIeFsfli5d/LJGZTjPHd2tvt7a3/bgeniZlZymRqjQtk+czn4+zRyc3xFiH+qqcE6t/dQQ6iuypAMp3NMD4yOg79d8/PfB0sZg/zNyLl8wpJ9ehqLLAeFXS/QB3s0S4Rkl4HySFCZnyBJG4JZMIj1Fy6hARUwCmVN94uYRhLQwPqmOUTOvj8M2UBhyu+u2n95wnqsh2j7bvo311CvslKn9cpHm8DXETGBJv92Pm5D46RuG8K228C2Si0kNtwnSPZC8lO51gr51hD2T7LE3/OFnC3tlhDl0gQOJ/6z49O/ZlLx6uZ2eX5+c1x/RSycgr5vPZ7y+GU5nqfZdzCgKSTqN2VwBFYQOdsIX9Stu8g2w75q/tSER4hhBBCHDx64RFCCCHEwXOvpNX0cCnBsZLk3OXvP28ZNZzgM0i8R1eXwf1hqKtULj0094FjT242KemmQT0jhFzLOVwaCKWamQ3YPd72CJ1hZ38DtwiTWBVwLaRT//wAB9Ic9aa2cIVsO99VniF0m/CeId1NKlZC2R813QOoQ1UhHJnmlAQ8bMwaNVsk/LvokSgL4djJxtsoX/o9nyzogkJiKfSbIZlhgjA+k7OZjZOgLab++9zcj26zGYbbauP3v4Hc8XwF90QGWRKOxZRJsOBoyBFSnSFcnKb778/TUx93kxxJK0eyKsLgmHcN5JqjR5AFFx6KT+eoUQNp8+Kpt88F3BJ15d/VwTUWYJyog7fzCgkSzcxWLyDLMXFZ65LbEm3NhG7JhI5NP+7QT4aEbosZ5iz6yVJICIV/ASWj/IFcWh1cTgH3HzBH5pBwH8HN06Eu3gXqiuXB7yfB37axhfO1czlhfur1xp48hjx96Z9v4Vbc4jxFOR7jJxO6zlCr8Mgbk8kqezxrzlZYjzj/0fRN9LZoIfs1l34/zYAEmDg/3ccDnWJ7osUz5PTI51SGbSHVhbfjAFmGEluFObJB8r8wYFsEso7SKcekuXRThojPBF8QF6hll1S3kkhi20bAM7GE+7jCuNhhXBjqn7G+5huP/BnfXMJ1lfp99ljfKzyvDP23g4Nws3IX7stQhEcIIYQQB49eeIQQQghx8OiFRwghhBAHz/3FQ3NYwrkBAoXecujNA0TWGnbgCXS/I1j2WuyxCdA3lyj+mGETQMHsqrBPGzX/uWv+swWu2cwqCP8hsJAZsj7SN4m9QTmur4StsYM2nBX+3Tlso9sNbN81MpDCsktd+fxyvL9hX8SEGx+86wto6WWJPTwTv77yBJb+yvdb1MjYXOL+OxTkTBL/+QBdPcEegx0Kwxa4hgkKxS2X4/7keChQKHCC+1mhEN4EmbAv0CfMCl5B04/ItstsqI8euYWTxe4i9PEE+xYmGC/7YjqHdo8UCMxOCnfoaKbTcm0JCkOi4OPRmyiYib03F59yvf3Zxq3lGTInb9GeU+yXSbEHK2JfgZnZ9tLbazfgmpBKoMn8vC2y8S6RkuD4iX9+cYqUBpX//AiZeRfYoNMhX/QMluliisKI04f5G3GH/VD5KP0CCmYmKObbcZ8EMkpvzm6OQ0BG4S1s6dizl2FNmGKczrAHE9ufrINlvsB+pgn27JiN92HNsD/vBHM4RRqECpmTdzg27H87RmqQk6WvR8/Pfd/O07XvYepha85RAZeZ8pNyvC9wH3Duc99RhIW8ho2/Q1qFNbKd02TNjNgV+ntA6pUeaWR6tFs/x14ppJQZMB9r7NPpkB3azCx0THWAcYRtNaGGzXzr17HBnqoN2sVyv74O6VJS3ClTFZzjPEuMpwxjtn4xvu67UIRHCCGEEAePXniEEEIIcfDcb0tH1JmW7tRYuA7FIxEeTQIlDcTWIQ01kHGmCC0XCF0GWPyoH0wKZu/171pCejEbWyVTyGPHJy5L1I2H/OBGtRlCkIGZXVceOtsgxLuCXTsg+2eSeTh9d+GhuR1CfxH+6a5jAHN/DEgvzUKXbMvY310ocLlwiaM/Pbk5bvDKfHzsP2fWzwWkpB1ss6ePPby9hNW/gZR48Ryh/pFGY6OwaItihRHtR/t9jbF0uWLWaR8XyxNPg/AE0mq19vNXsG1OC2R2hRwaka6heQDr62Lp39s2tLXCKgxZogi0r3q7lUj1EDD2IzP8zpGR9THGinn/nT91mTNADlo88X49OvW2Xf/tj41v6BIZWbF2pJCrsoWH42vYdOcn/h2LY2+XAYULGxTqPYXsPYd8GlKkSUBBzpBjIZzuX540M9tsIFNAQphgQZqX3t5ZQgnRx3IHSXOGDCBTSOwN5keWY11Hht3zM2TFZuForMEdUnvMsf3BzOwR5k5Z+PraIttwNfj6l6Jo5BRViHsOShYVRQbfJbKLXxrWaWSXLiG5XeLn1W5swd4Hswm2TiC78IsLv98zFNLcQHZPkJV+jqzRBZ5dGbZgMHN/huMc/ZrjOUYbPuXJen1+c9zXY7m5jv59O2zhSLAet0xbM1pHnCy7W/besgjpxtfZJcZBU3vbbc37bJb63J/dklXvQhEeIYQQQhw8euERQgghxMFzr6S1QajpElv1TxFOp6tnhaKSCUKUCRSaAeHUEmHWOdwYpygeyh3s3C1eBBbD88/QENPRQmPjgqaPH5/cHF+cu2zSzfxcR5DWOhQs26It2p4hXv9dhpw7ZKFlQcpgHr4LKRwFEw/T7RMWCR1l92zgKELByeUJdsMjS6Yh9L9GNtApQpZzSD05QqKG4q7WIZMmIsubFbb/Q1qYpWNHRYqszc3WT0BXXAuJjmHUAq7D41OEheHmmMPBMcPA2p0hKzBcD/OTk5vjKc5f7/YvUbbMqooxyGtOMsjNcMQUcNM9eePNm+Mesup263NisvAY9Yd/4hs3x6fIlP4xyMqbjY+t2SOXM6YnyHD8JqVns/nOQ9Y5pec3fS144y2XxBr05ZO3PEPwCTJQb+DYydmXmNcJMrbnVEw7uE7g0MvLV4fN3w10CvZwoDYopHp8DFdT6xe72vh45IKeUCZHwVAYbWwDSeds+5mb48nc22i28PEygftnAZft8WRcIDdCNtpuvR+evXAXWYMsvEdHPpboMNrhPCnmWo815ezcz/nsuX9XBedggufOGhnIz5Dhe1+0uOYN3KcbrLPncFRxLM8wTrFcW4aeLZABPsW6PIXEH7E+bDfnN8fryr/3aOHbDvjMWddjCb5CPz1Fe7UorVBBokuRuZ4ZtBPz/luv4CBcwzHLrN4YzAMsYZn52KwCtjK0YynuLhThEUIIIcTBoxceIYQQQhw890pabUe5xsNXAY4PFpvMjhAiw878JVwRlD2YWGmG4oZzOJzaHpIBdm1nkGR2Ozh5EHLOkfTMbOz+WkwR+kYYjTvdW4S7kcPKAhwCTCyVJXSa+D8MiCEz8R4imaOCcxbu7ZZ3zzC87B9ujqYlHDKQDRsUQ6UD4HHpMkM5coL4/SAqbznCsXSjVFuERyEBLdGHsRm77tbncKcgXFwjweQlZNkEbrGEBUARO00R+h44QimzQA7NU78+uhEjCuYOYf8urQH90TNZGe69h4vGCr+Xk8dPbo5LFB+8xDwqUAi1RAHHALl5e+T3WOPez8/8PLOFt8+jN1icdOzqSVF8skXYfXkM2eQtSOl0aWZ+D7MFEoRCxrt85u0V4eQJrG4KDSGHrBJSyFv9/vvSzGw687GZZS4vdA3dPH4PWAptChlkAvmtaz3cv27gAsN6tNpgDmG5K1dIIgvnUIm17whr9vPduF0SSPQ9EsvVI0ehtyt/fUB/ruEYytDnU7hGS7gjM6wvrGZdY2xTWplk+/+bn8+QaenXOZ+kd/48Q8LSAQ3RYKtGBWm+x3rSVC7hXeQ+bzKM2U0PhynapD328xyf+PnP1mPn2lMkdvz0mX9HMvUxW/CZhcSRbYUtLJEubiRLRZ5dFiG+xLo2QZLLBb4qIFFhkoyfD3ehCI8QQgghDh698AghhBDi4LlXO8nSu9+HaiQv6iHdlJCiZrP5nT9nzRUmXFqjlswWbq8JEtIVkM8idoLX2J3dsMbOyHZhliNh04DQ6hGur9n5Ne02fh2G2j/cJc9QW41Eikz2NEXot0UNnPnUnQ0vLv3+H0rRMvTVArVoFlO4GVB/p8PwqGmcgnwxXfg9FGjvgP7pID8OCGO3kIzoCknRvlT6KI2ZmbWwdrVMlgUnGJPsDZD0Bjj48gTurxTh3Hi3UyeBAy2jcwyuECaPjOFlUuK7p+u8MSoknqsr/66soKsF4XE4OM4uvR7Wxc7He4r5O4PjckCNvIzyxol/xnC/yDNqy1PIWPnYQXnyGDWBav/ccun38OiUFkzIMiuPiWdwGSbRr2MG6SJAKuiwdmwhpWUJHJe41lDfSn65J9aQfUqMf8rkFcb4JeoZLiA/5kh012N+dFhfT5aoT/fmyc0xHVs92iVhwldo+AOcdQOkYzOzKcZPj4lbIBVdgT6ZIMlpgbVpNmcCTyRMxPq6q/w8SzjHMsjzK8jhAeO5xPG+OEbCwIi6dRP067xArTIkoL2AnHQ+uJxZYltIi/W3h5S/hWyXmj9nOsjrCdbGGuOjWUNSbsfJNbcD1jW4uXrIlgFrH+tItrV/Znvh18TkxX0FCRwO4Bpr1mjdGeCkhbs7fQc1CxXhEUIIIcTBoxceIYQQQhw894onYVTbHlIEpJuIYzonihTOF0SvI1wOc8hbGZKkpchUGOCoiHCgjGq6QJ6qsSs8vVV7qSwYIkNyJCYsQhiceQsnkGs6SH10l3VbJEnE75b4Xco+0/IEX+s/X9f7r+9iZlYg1FjvkPQQikuG5IxMGsUwcAYHC1M9NahJVSJ0ukhQrwiJBDv0c11BDsTnrfLPr22saQ0IccO0ZDFF4q8NxhJrC039O1iXJyJ0Sj2tgOMnoiZZC/veAFdfD6dCUe5fBkGeP5tOIbdh4EXMQSbem8M5lcLuM8/dZdnC8cBaYD3ks4D5dYxkpE+QsLLHfFpA0losx0tP6Dyh4cW5y2yTmX/uzSd+3gb9WqNeGmX4GhJmhlB5RD8FJG5LIAelWLSSxEd5XjzM34iXazjT0J+P0K4DnLIX589vjs82nnhvgsk8CXSXeb9lkEOfwHFYDP6ZFOtxPpKecP8dnw+36haijRMM1hoJXBOsIyX6xxpvixpbDC6xBWLA1oW6Y3/6vC4hLVWdn3+DukxMqLsvAr5rhaR/FdaEHveSo4bZBO7Rhi5LNE/Je8cYr5jkDw7YSySmnD3GthBIwdsL/8y2GCe+XTWQVfGs7BrvVybIZPLTDpLsCn1ZwgFcYqvFHE7vARJdi8SGZ5BS0xrvEL1cWkIIIYQQeuERQgghxOFzr6RVoVR7DnmLya1K1DfJAhPyIVQO2ScJTGjmoa/F0mupsO5L31NuggsIYbAcrpn1Bu6tbuwEqWA1YlK5Ca7j6MRDakyelyDB0QTfN2CXfN/CtYCwZo4d+WzHDmHfFKF41qHZJwXC1B2uI0K7Sw277fE+HBCWpkOI9dNyJHZskVXxCCHOyFtDksgafVMgKVUBR8Lqciz1sU+mqA8VkfjrjUfen5cIx+4qSKUQ5lLIACm+m26sDGOe/d+Owr0Ya3H//Tkp/JxV4t8VIRP0aMfjI7/m0xP0B511kFIDdc6Uyfwm+AyTDfpaUcItwZpydOXQyWFm9oE3PIElE1520a8J6pidnpzi85BbmXgPsm0BaYxSTIA80EESoLuG9YqS5GESDw4Yy3VLB5rLTFPIbAMlKjicspFTFBIz64ch8n959uzmeL1GTS7IoVO6/U58nc65bSEZP0ouL879PzB+eG9cdy633lcbyF4bOs2QtDXFNQUkua2YlA8JTHeovVVBGtzhe/dFgbXyCZ4n56jZ+Dbknc3a5+8a7XNeQ/5Dfav1mR+/YDsjme7RlAluMXHwON1Anh4g7dWJP8fMzM753MUzhBLrlrX9FnRs4nkC11zksxnPog3vHzJvCndfhgG8wbvCk7mPzZehCI8QQgghDh698AghhBDi4LlX0ho6uk48ZNVgh3nJ0BndTpCcGNBPSzqzUG+FydNQN4RJ+xbYwb1B+HWLHekXG68tEsNY0koQ7nyzOLk5LnK4BeBSqrZISIjvY82dAfJOB6tQB8fD0QTfBcdOdeEhzsjwZf8wLq0WbTxD0kO2S4taK4Zd73TdJQjxp8iSGCEJDJBTqtr7c33pIeQnH/CaTj1Cmfi4Zej/NBsnlkpZowruBsoUEyQou0Ryt4uVS3ElasIkkK5mmYeImfiK7gmGzRuE4jP8LZHE/WeSTAIkwBS1oejMmtJFRacca5jR+UKHFxyRCCHPIP+yLlyADFXDZdOiblMPl12SjR0Vm7XPhc3K51qFQjsF1oX0GEnckHwswT00KWpjITkfa+TtMCZSOIUaSFptBamnHK8p+6JBiH85Qc0ojPkj1GJK6czDeFyy3yClsl5TOpJw4QSqvK13WONCoISLNa6D5BDGc3NAckfKIJuW58W8gFTYY/4OkKrTHHX14JrsmGi0QeJFOLZyJMxbYK2I2Vi+2QfHkFspk27gIM5Rn2wH2fIMa8jzM3+WFRM/zxbP1lUFORNSPucX65ftnvs5S2w7MTigh1thEDot+exj7cTsyPtpDgdsj60KG/zu6sydhTs6ph/B4Tn4zxcLb69HT05ujo+X/r1vfPCxvQpFeIQQQghx8OiFRwghhBAHz/2xdiRYK5B4jsdGVQGh72kJ1wxCl5c7D6+1qNuUINTJXdvzme+8rrBj/wy1p1iLhFJXV4/ru4TWw5qICNsE7gfWfUoRUsMGc9s0fh0VpKi+RbgX4fcG9XdK1vTBtdE5kT/Qa+iAaHyPnfsB19QzcR0imTlC5csjr3VT5B5qrNAuA36Zjq0t9KoXZ95XbU+3G+u6QLqYjRP4dXBabQcmxINbcI3EXKgVFGvUB5p7qHx+4vdGxWyA9NHjWjtIgHQkMZklXWf7Yrvy5HwJ5mkOSW46YVJQJqHD+IW0FyZ006ENMT8CHE45HBhvPfZQdFUjoVm8ez7F5pZzDTXmQvTPBUhUdDIWcIIlkGsSyDg5JK1jOEfeXqN+2LnfP+t2dXBo9jSUvGLJfLecnLq8u4QkcDzx75vlfk0pnDMtahEhh92ohluELEW53SDbGto6gexnTK6KtbnF2tf2Y6kvmo+lkbOHDt+XJAbsIB+neNZMFz43O7iuejhLoVZZAklry0S1+N3OxmvKPqCsfAl59gW2MAxYjEvUNsuxjSQP/rspXHmGmpJHWIsDpPmuh+ttAwfW83P/XdQse3zsclAKV63Z2HFd41lL+Xi79TXlAp+Z4WE2L7DNAe7LArXq3njiz/skw72hrtYH8ZljuOCOj+XSEkIIIYTQC48QQgghDp9747NHcw8pPVp6YrDlzENwBcJaeXa3pEUJIMBRVaKWVsAO8wEhtPUlkvn1Xj+mZ3l5yAoFwt5JMpYSWO6lgcxCqcMgsw2Vf/eoxhLipudIIMVaIQytry9RiwcS0KhWGRO0IdS7T4rCv2N74de6OPa+Wp56iLBDbazYws3B2i+lh8R3a2/HS0iO2y3kJrRdDXmrgRzYwzEwkt5uKUMdZKkp2qynewjtGiGbziibwvGRQcajrJGj7fKIej0JawBBssH5k/0rWtbskHgupfPR7yWBrNhs75aJOH9Z5yhHvx7h3ssSyd/gFtnu4FyEWFvMmYTMP79DglAzsyM4rdqZ31vewCGGUHuBUPnQ0nXmEtXxsa9TdJfRHWpwl9G0Mp35/WepHwfWfNoja7ifUjg5WbqrgywzxcUGtN0G7jKUiLMBDpkesokhcVsD2WsyhYyBE9F9teN4b1hVb1yLi/IY6/NtMbdbSEsDHkvsKjo5WcMxQpLHkmUbrOuXcAvtIKfuduPr3gfzpcu753AfUt56/sJdSmcbv4Y1rrOcw3UF+biAxBghb3GBbOH6CxgrPda0OZKOPnnDJa3klht2i2u6YH0vzHPWyMzR9wu4nlPsf5m85RJuCdn2rcf+znF05O0Y8b1vvXlyc8zn1YwJD1+CIjxCCCGEOHj0wiOEEEKIg+deSYv1KyKkqBRy0hRhZoZQw+jzzhx1VXK4aXq8ehU4Z8+QK1wBlGeYVKtFSLsJ49tbop7MdoeEgS3Oi037TIaYQjaIaJcKn6Fjq8C9MdzXI0lcjzBlB4ltGB5AA7Gxs6WD7Yw1zQzXnY46FE4LuG0auJc2a0oLdGn5aaZL74MpXEGTqcsPA8KxKerS9LdqUvWQjdjXOcK8GZKVTSDFddH7ar31sHOPJHN0McwWfp4UodwWMh5lVkNYd7sZuwX3wQJ1c5ZHnuisqRnWRk2qii42P88oESbkkKHDcYv5CAnI4KbanLsrpGGCQIytHK6ZzdmtumhwAUbMBYOM0aNu2eWl91O9xfHG5//iwx++OWYdrxIJ/I5PvR2nmBM5k9PhPh9EnzSzC7hL+4H2QJ8vR3Ap7jAXJiPJCfWtsO50lI9QlynCldpCNpgP3m9lZFv4d+3g5KOL08yswdhLcB0t1raWf2+nTELIBHqoz4c5vkGdJajW1uGcz1c+Jpl0lPWd+BzYF2XpbqEs9349fvLmzfE5khBa6/IWE/txD0ZGOXdGd5t/nOtpHzmGvO/nSMD45lsf8HNins4gpZmZzRf+fRNsQ+E6vcP2jwQLzBEcsCgpaeljSFGQvQo4uU5OXdKaTf36jo/95+Xcr+0EDtuXoQiPEEIIIQ4evfAIIYQQ4uC5v5YWQvQ7JLd6hAQ/A90opYejcsgnox31fMdCVK9HGHy38e9iSfkJ6zMhmZJBkprwjm69znW1h90uesojkN9Q04UJAFPcQ4U44hQh7h21AuyGZ00v1pvqKHUhdN90+3cOmJk1CHOyVhKCnyNZo0LYtYOjKqDjGFqmU2cy8TESUw8nU4oscjirMNayGcKmcBpVGw9Rm5lVCKmuEFKdlv47p0f+HQnqagUmRkQ/ZJDDAj5f1d4nM4SOM9TEaSH1tRXcT8M4Kds+CAx3QzTuMNZaOGcS9EHAmO3QhkycmUDSYRLFLeSmFmPiOZOqYU1IR7WqvK0uMW7Mxu6fdvAxSKmrgguy2vnvNzvWufNrffrZt2+Ou4WPnRb1uUrU2Eog17Ut1wQ4BZP996WZWQ1ZlY6UrvafryBdlTieU2ZggjrW9sMa2aBmFp1paYZ7Q7JJrhs9age2cEneUrRG6xyXRa7/Hf6BSewGyG9Ni7psnbcFEx2mJeean/OyolzlDwa6lh6kN1OXjZLc18Hp0q/nCGWf2sQlmgJJ+xq44ApI+/QJBvT3DGtRinVgm979/K1xXOCZc5SOXcJ0ZZdYv1u073wCpxzkzOkc0igdW9zCggSZOcb1Ds9E1rBrsDUhR9JVG+59nTEzRXiEEEII8SWAXniEEEIIcfDcGwOiC6ZC8sAa4a+khpsDDp8S4coc4bUcu+5rJIaqGH6llMS6XUiMtUNIu0Poa44kXF1/SxqC8yRhTRjUlikQHk6Q6IuOrS12//e4/xSOh4quGMRNM9Q7meM4yRCKfoBkWGbjUGOPMGKPwG6A1Mf2Y0KvDufZ0FE19dBsWno7TtFvNtydZGuUrA7yBhMSbqtxu1CVHLlCEj/vFnLHbA4pFnHhGWoXzeBIGNhekKVaJLqMmBctQq3MU5lNHiKRJKQBjH8mi2wREk9H9kP0Pfq46+gm83Nesm4R2pxjfI3w9ijRWU/JyJcb1l0zG4e4B9T6iliiGkjSEZJxgGOJeSAvz8/9nDsmVaQbC0kuRzXfkMAPDq8kfxiXVsAi0UE22nHetUj0iM9vKoT4Ibnt1u5Y22EeRCQCjKh1NkFSyRbzroRDiNsNKHVyPb3CO2Iy93WhgxRVsY1LOkL9EKbekaxc0wlIjQcOrzXGP+XKHjUZ6+3+HZQZ1r4JElgeYStIg3lBWe0EifRSyMoZkvrWuK9LJDZkF8yxt+PkxCWpiMYtsaVgjjXqEVxQZmaRTukdvg81sDJIbh1kxekMteCwztJRluHCB3wXHdAT1NQsJ359E7jO8tyPX4YiPEIIIYQ4ePTCI4QQQoiD515Ji7V1mJHvxbknSlpClil7uG4QukwgB6Spn3OL0GKLMGs2ZSIi1NU6Q6IzJB5jIrgB37Xm9ds4wR6T0NGpYAjfD3SO7ehl8u9rEGaNkEAyxOgZRqR8VKZ3S0l5/jBOkLzw/qE7ZUGnESSOBi6fBlLJqDYawv2sUdRDW0jwXp0Gv4YC3wvT1MilEejGuGUFGVgTCq6lAh2aQmZdXZz7zxFqTnO6XBDyZY0t6pI0+OHHE4yvASHe+ACJJFu4E1ZbSrVMGgaZBHOhhnRFV0SO2jpVRbeIj4MMY4iJNikHsY6SQf5LMa7LME74ViIB4jAgwV7t1xohnxZ03KEtKNuyn7Ybv58U9bmYm3TAWGtQd61DVtTkHThB3g0BWwAipHS6aiLcdUzCWUNa6nDTNaSxCp9pkQm067wPzy98HG0vfSwvj1xOoHst4DpvF7pLsRasO2/7HZJHdqh5N0WiwzTj3+GU+uDkw3HC9Qhy9g5JBaPdvSb8OCVuDwSsU0yie3pycnPMhKgR19PhOZAwGSOcpI+QYO/DH3jLPzKqWebHlIlajOsEte1qyIvrDVyPNs61OUACpRObstQMbswlEg8uUVdvPr9bZquxjpxgDT06dqnvaAFXG9asEttZXoYiPEIIIYQ4ePTCI4QQQoiD5xWSloeTmXCKG+ozhB8Dwph0UfQ4T82fcyf/zMNRLWpjdbWH1zKELhsk/6PTooZjpWq4fd9sWni4v0GIP8E99HAOGUK2rHUVeyYlwz0jRBjRsjnqcDE+WMEJ0UBmSB8mam4DkzQhjFhBBqgrhr4RfkZYn3n0mpHbhonIIDPgnMslHQCQD9nWrLdDeaOlrGg2QGpJmYyqhOyygSsMSb2muL4CMmYKyWIyRagZ302ZYYCUVsLhx8SAlCj2xRTJwFi7ZkA/lRPKqn4NdHBQ0ipyhKIhydKhl6BtZ3B2sH0MTqEaySsT9vdk/LcWnUMVaj0dLfw+pwiJM/8fHXcV6pa1kMMKfh1+mXJ2hN0noC0yFAEqUcNsn2w3d7ttaiT9K3AdySjBJKR7fJ5zk20xSgQIGY/S63pHhyIcseingVKSjdfaFFJD07mrlXO7GDlq/f7ZAAOdmJDTIuYXk5NyS0MCSTqg4FaKdZpbD/YF61IdYb1b4HsXx37NJeR13uMOWz646E5Lb7fArRNorLrhs5uuPPQTxkGF5yET4pqN14hgj26Ox1KXHxdwl00wZo+PfC4/eeOJnzNB3UG4PdkW04m3UQkXXIaHJZ+/L0MRHiGEEEIcPHrhEUIIIcTBEx4i3C6EEEII8cWEIjxCCCGEOHj0wiOEEEKIg0cvPEIIIYQ4ePTCI4QQQoiDRy88QgghhDh49MIjhBBCiIPn/w+ePK1ZZry0ugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x576 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    \n",
    "    # Rescale the weights to be between 0 and 255\n",
    "    wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "    plt.imshow(wimg.astype('uint8'))\n",
    "    plt.axis('off')\n",
    "    plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5d17f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
